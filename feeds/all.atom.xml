<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Quang Nguyen</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2018-11-28T22:39:23+00:00</updated><entry><title>Training Models in Parallel with Keras</title><link href="/blog/tutorial/2018/training-models-in-parallel-with-keras/" rel="alternate"></link><published>2018-11-28T22:39:23+00:00</published><updated>2018-11-28T22:39:23+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-11-28:/blog/tutorial/2018/training-models-in-parallel-with-keras/</id><summary type="html">&lt;p&gt;If there is some problems with BatchNormalization, please refer to this &lt;a href="https://www.bountysource.com/issues/33177171-does-keras-support-using-multiple-gpus"&gt;Tensorflow pull requests&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="sample-code-to-check-the-speed"&gt;Sample code to check the speed&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.applications&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ResNet50&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;multi_gpu_model&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;224&lt;/span&gt;
&lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;224&lt;/span&gt;
&lt;span class="n"&gt;num_classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'/cpu:0'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ResNet50&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                    &lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;parallel_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;multi_gpu_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;parallel_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rmsprop'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;parallel_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;If there is some problems with BatchNormalization, please refer to this &lt;a href="https://www.bountysource.com/issues/33177171-does-keras-support-using-multiple-gpus"&gt;Tensorflow pull requests&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="sample-code-to-check-the-speed"&gt;Sample code to check the speed&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.applications&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ResNet50&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;multi_gpu_model&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;num_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;224&lt;/span&gt;
&lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;224&lt;/span&gt;
&lt;span class="n"&gt;num_classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'/cpu:0'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ResNet50&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                    &lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;parallel_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;multi_gpu_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;parallel_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'rmsprop'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;parallel_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="TUTORIAL"></category></entry><entry><title>Something you need to know about Aobayama Dormitory</title><link href="/blog/other/2018/something-you-need-to-know-about-aobayama-dormitory/" rel="alternate"></link><published>2018-09-04T12:20:31+00:00</published><updated>2018-09-04T12:20:31+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-09-04:/blog/other/2018/something-you-need-to-know-about-aobayama-dormitory/</id><summary type="html">&lt;h1 id="residence-registration"&gt;Residence Registration&lt;/h1&gt;
&lt;h2 id="the-uh-aobayama-office-is-open-for-residence-registration-during-the-following-hours"&gt;The UH Aobayama office is open for residence registration during the following hours.&lt;/h2&gt;
&lt;p&gt;September 27(Thurs.) ~ 30(Sun.) 9:00~16:00
From October 1 (Mon.) 9:00~16:00 (excluding holidays and weekends)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Please note that the registration must be made by the prospective resident himself/herself.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At a residence meeting, new residents need to apply for rental services such as  bedding, plates, bathmat. It is mandatory and cannot be canceled unless you have some special reason.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="please-be-sure-to-bring-your-residence-permit-and-your-id-with-you-when-you-move-in"&gt;Please be sure to bring your residence permit and your ID with you when you move in.&lt;/h2&gt;
&lt;p&gt;If you can&amp;rsquo;t bring an original residence permit, please show the dorm staff its copy or its PDF version.&lt;/p&gt;
&lt;h2 id="after-attending-the-registration-meeting-you-can-receive-your-room-key"&gt;After attending the registration meeting, you can receive your room key.&lt;/h2&gt;
&lt;h1 id="facilities-in-your-residence_1"&gt;Facilities in your Residence&lt;/h1&gt;
&lt;h2 id="furnishings"&gt;Furnishings:&lt;/h2&gt;
&lt;p&gt;Study desk, rollaway wagon, chair, bookshelf, bed, closet, air conditioner&lt;/p&gt;
&lt;h2 id="shared-facilities"&gt;Shared facilities&lt;/h2&gt;
&lt;p&gt;IH stove, refrigerator, microwave, rice …&lt;/p&gt;</summary><content type="html">&lt;h1 id="residence-registration"&gt;Residence Registration&lt;/h1&gt;
&lt;h2 id="the-uh-aobayama-office-is-open-for-residence-registration-during-the-following-hours"&gt;The UH Aobayama office is open for residence registration during the following hours.&lt;/h2&gt;
&lt;p&gt;September 27(Thurs.) ~ 30(Sun.) 9:00~16:00
From October 1 (Mon.) 9:00~16:00 (excluding holidays and weekends)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Please note that the registration must be made by the prospective resident himself/herself.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At a residence meeting, new residents need to apply for rental services such as  bedding, plates, bathmat. It is mandatory and cannot be canceled unless you have some special reason.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="please-be-sure-to-bring-your-residence-permit-and-your-id-with-you-when-you-move-in"&gt;Please be sure to bring your residence permit and your ID with you when you move in.&lt;/h2&gt;
&lt;p&gt;If you can&amp;rsquo;t bring an original residence permit, please show the dorm staff its copy or its PDF version.&lt;/p&gt;
&lt;h2 id="after-attending-the-registration-meeting-you-can-receive-your-room-key"&gt;After attending the registration meeting, you can receive your room key.&lt;/h2&gt;
&lt;h1 id="facilities-in-your-residence_1"&gt;Facilities in your Residence&lt;/h1&gt;
&lt;h2 id="furnishings"&gt;Furnishings:&lt;/h2&gt;
&lt;p&gt;Study desk, rollaway wagon, chair, bookshelf, bed, closet, air conditioner&lt;/p&gt;
&lt;h2 id="shared-facilities"&gt;Shared facilities&lt;/h2&gt;
&lt;p&gt;IH stove, refrigerator, microwave, rice cooker, dining table, chairs, washing machine, vacuum cleaner, iron, air conditioner, TV, shower room, toilets&lt;/p&gt;
&lt;h1 id="regulations-and-restrictions_1"&gt;Regulations and Restrictions&lt;/h1&gt;
&lt;h2 id="cases-of-residence-permit-cancellation-or-eviction"&gt;Cases of residence permit cancellation or eviction&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;If you fail move in within seven days of the start of the move-in period without providing a reason.&lt;/li&gt;
&lt;li&gt;If you allow unauthorized persons to stay overnight in your room.&lt;/li&gt;
&lt;li&gt;If you damage the dorm facilities badly.&lt;/li&gt;
&lt;li&gt;If your behavior is substantially disruptive to other residents.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="smoking-rules"&gt;Smoking Rules&lt;/h2&gt;
&lt;p&gt;Smoking cigarettes and burning incense are strictly prohibited in University House Aobayama and Aobayama campus. If your room becomes smelly or dirty due to smoking or incense, you must pay a cleaning fine &lt;/p&gt;
&lt;h2 id="restriction-of-baggage"&gt;Restriction of baggage&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Bringing electric home appliances or furniture is restricted. (Except for PCs.)&lt;/li&gt;
&lt;li&gt;You can rent appliances if you wish to use them in your room.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="uh-regulations_1"&gt;UH Regulations&lt;/h1&gt;
&lt;p&gt;Article 2: The objective of the UH dormitories is to assist with fostering international leaders by offering housing that provides opportunities for advanced intellectual exchange and high quality living environments for use by this university's students (here and afterward, this includes foreign students) and foreign researchers.&lt;/p&gt;
&lt;p&gt;Article 7: Those granted residency (hereafter "the residents") must complete the prescribed procedures and move in by the prescribed date.
2. The resident's residency permission will be revoked if s/he, without valid reason, fails to move in during the designated period, or is found to have falsified his/her application.
Residency Period&lt;/p&gt;
&lt;p&gt;Article 9: Residents shall pay housing fees, maintenance fees, and deposit money (hereafter, "housing fees etc.") by the designated date in accordance with the type of UH dormitory room, as shown in the residence permit.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The monthly rent will be paid in full for the month in which residents moves in/out of the dormitory, regardless of the date on which they move in/out.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Housing and maintenance fees will not be refunded.
Housing Fee Waiver&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Article 11: In addition to the housing fees etc. described in Article 9, residents must bear the cost for any utility fees etc. incurred personally (hereafter "other fees"). 
2. Payment of the other fees mentioned in the previous article will be described elsewhere.&lt;/p&gt;
&lt;p&gt;Facility Maintenance
Article 12: In order to maintain the facility and furnishings and preserve a pleasant environment, residents must comply with the following.
I. Residents must not use their rooms for any purpose other than habitation.
II. Residents must not allow guests to stay overnight in their rooms.
III. Residents must keep their rooms and the common facilities in good condition, and not perform any construction/alterations without permission. 
IV. Residents will work to prevent fires and other accidents, and maintain a hygienic environment. 
V. Residents must pay for the replacement/restoration of any facilities, furnishings etc. that are lost, damaged, or defaced, whether intentionally or due to negligence.
VI. Residents must refrain from behavior that is disruptive to other residents or the surrounding neighborhood.&lt;/p&gt;
&lt;p&gt;Moving-out Procedures
Article 13: Residents wishing to vacate their dormitory must submit an official moving-out form to the administrator in advance.
2. When vacating the dormitory, residents will undergo an inspection of the room and its furnishings etc. conducted by a person designated by the administrator, and follow any instructions from them.
Vacating the Dormitory&lt;/p&gt;
&lt;p&gt;Article 14: Residents must immediately vacate the dormitory under any of the following conditions.
I. Loss of status as a student of this university, or upon ceasing research or education activities at this university.&lt;/p&gt;
&lt;p&gt;II. Loss of residence eligibility.&lt;/p&gt;
&lt;p&gt;III. Completion of residency period.&lt;/p&gt;
&lt;p&gt;IV. Failure to pay housing, maintenance, or other fees for three months or longer.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Residents who fail to comply with the previous items will be ordered to vacate the dormitory by the administrator.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition to the above item, the administrator may, via the Tohoku University Student Support Committee, order residents to vacate the dormitory for any of the following reasons.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I. The UH dormitory is deemed unsuitable for the resident due to illness or other health reasons.&lt;/p&gt;
&lt;p&gt;II. The resident is suspended/expelled, or his/her employment is terminated.&lt;/p&gt;
&lt;p&gt;III. Academic leave of absence, or study abroad, lasting three months or longer.&lt;/p&gt;
&lt;p&gt;IV. Any behavior that is substantially disruptive to other UH residents&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Item 2 of the previous article applies mutatis mutandis when vacating as per the two items above,&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="contact-address"&gt;Contact address&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Life Support Section, Student Service Division,Tohoku University&lt;/li&gt;
&lt;li&gt;TEL: 022-795-3774 &lt;/li&gt;
&lt;li&gt;Email: ihome@grp.tohoku.ac.jp&lt;/li&gt;
&lt;/ul&gt;</content><category term="OTHER"></category></entry><entry><title>Automation and Make</title><link href="/blog/tutorial/2018/automation-and-make/" rel="alternate"></link><published>2018-08-27T16:15:24+00:00</published><updated>2018-08-27T16:15:24+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-27:/blog/tutorial/2018/automation-and-make/</id><summary type="html">&lt;h1 id="automation-and-make-running-make"&gt;Automation and Make: Running Make&lt;/h1&gt;
&lt;h2 id="key-points"&gt;Key Points&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Lesson&lt;/th&gt;
&lt;th&gt;Summary&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://swcarpentry.github.io/make-novice/reference.html"&gt;Introduction&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&amp;bull; Make allows us to specify what depends on what and how to update things that are out of date.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[Makefiles][]&lt;/td&gt;
&lt;td&gt;&amp;bull; Use &lt;code&gt;#&lt;/code&gt; for comments in Makefiles.&lt;br/&gt; &amp;bull; Write rules as &lt;code&gt;target&lt;/code&gt;: &lt;code&gt;dependencies&lt;/code&gt;. &lt;br/&gt; &amp;bull; Specify update actions in a tab-indented block under the rule. &lt;br/&gt; &amp;bull; Use &lt;code&gt;.PHONY&lt;/code&gt; to mark targets that don&amp;rsquo;t correspond to files.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Automatic Variables&lt;/td&gt;
&lt;td&gt;&amp;bull; Use $@ to refer to the target of the current rule. &lt;br/&gt; &amp;bull; Use $^ to refer to the dependencies of the current rule. &lt;br/&gt; &amp;bull; Use $&amp;lt; to refer to the first dependency of the current rule.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dependencies on Data and Code&lt;/td&gt;
&lt;td&gt;&amp;bull; Make results depend on processing scripts as well as data files. &lt;br/&gt;  &amp;bull; Dependencies are transitive: if A depends on B and B depends on C, a change to C will indirectly trigger an update&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pattern Rules&lt;/td&gt;
&lt;td&gt;&amp;bull; Use the wildcard % as a placeholder in targets and dependencies. &lt;br/&gt; &amp;bull; Use the special variable $* to refer …&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</summary><content type="html">&lt;h1 id="automation-and-make-running-make"&gt;Automation and Make: Running Make&lt;/h1&gt;
&lt;h2 id="key-points"&gt;Key Points&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Lesson&lt;/th&gt;
&lt;th&gt;Summary&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://swcarpentry.github.io/make-novice/reference.html"&gt;Introduction&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&amp;bull; Make allows us to specify what depends on what and how to update things that are out of date.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[Makefiles][]&lt;/td&gt;
&lt;td&gt;&amp;bull; Use &lt;code&gt;#&lt;/code&gt; for comments in Makefiles.&lt;br/&gt; &amp;bull; Write rules as &lt;code&gt;target&lt;/code&gt;: &lt;code&gt;dependencies&lt;/code&gt;. &lt;br/&gt; &amp;bull; Specify update actions in a tab-indented block under the rule. &lt;br/&gt; &amp;bull; Use &lt;code&gt;.PHONY&lt;/code&gt; to mark targets that don&amp;rsquo;t correspond to files.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Automatic Variables&lt;/td&gt;
&lt;td&gt;&amp;bull; Use $@ to refer to the target of the current rule. &lt;br/&gt; &amp;bull; Use $^ to refer to the dependencies of the current rule. &lt;br/&gt; &amp;bull; Use $&amp;lt; to refer to the first dependency of the current rule.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dependencies on Data and Code&lt;/td&gt;
&lt;td&gt;&amp;bull; Make results depend on processing scripts as well as data files. &lt;br/&gt;  &amp;bull; Dependencies are transitive: if A depends on B and B depends on C, a change to C will indirectly trigger an update&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pattern Rules&lt;/td&gt;
&lt;td&gt;&amp;bull; Use the wildcard % as a placeholder in targets and dependencies. &lt;br/&gt; &amp;bull; Use the special variable $* to refer to matching sets of files in actions.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Variables&lt;/td&gt;
&lt;td&gt;&amp;bull; Define variables by assigning values to names. &lt;br/&gt; &amp;bull; Reference variables using $(...).&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;w&lt;/p&gt;</content><category term="Make"></category><category term="Automation"></category><category term="TUTORIAL"></category></entry><entry><title>Use Jupyter notebook remotely</title><link href="/blog/tutorial/2018/use-jupyter-notebook-remotely/" rel="alternate"></link><published>2018-08-24T18:38:42+00:00</published><updated>2018-08-24T18:38:42+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-24:/blog/tutorial/2018/use-jupyter-notebook-remotely/</id><summary type="html">&lt;h1 id="motivation"&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Situation: Your data (may be TB) is in your working cluster. You want to access and interactively play with your datayour home computer. You can use xwin to open your Jupyter notebook on remote host. However, this kind of connection is quite slow.&lt;/p&gt;
&lt;p&gt;To make the connection faster, you can follow below instructions:&lt;/p&gt;
&lt;p&gt;First, make sure you install Jupyter notebook in both remote (working station in your offcie) and local (your home computer)&lt;/p&gt;
&lt;h1 id="in-remote-host"&gt;In remote host&lt;/h1&gt;
&lt;p&gt;In remote host, open the terminal, change directory to where you have your notebooks and type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;jupyter notebook --no-browser --port&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8889&lt;/span&gt;

&lt;span class="c1"&gt;# you should leave the this open&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="in-your-local-computer"&gt;In your local computer&lt;/h1&gt;
&lt;p&gt;In your local computer, open MS-DOS cmd (if using Windows) or Unix terminal, then type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh -i path/to/public-key -N -f -L localhost:8888:localhost:8889 username@your_remote_host_name
&lt;span class="c1"&gt;# make sure to change `username` to your real username in remote host&lt;/span&gt;
&lt;span class="c1"&gt;# change …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;h1 id="motivation"&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Situation: Your data (may be TB) is in your working cluster. You want to access and interactively play with your datayour home computer. You can use xwin to open your Jupyter notebook on remote host. However, this kind of connection is quite slow.&lt;/p&gt;
&lt;p&gt;To make the connection faster, you can follow below instructions:&lt;/p&gt;
&lt;p&gt;First, make sure you install Jupyter notebook in both remote (working station in your offcie) and local (your home computer)&lt;/p&gt;
&lt;h1 id="in-remote-host"&gt;In remote host&lt;/h1&gt;
&lt;p&gt;In remote host, open the terminal, change directory to where you have your notebooks and type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;jupyter notebook --no-browser --port&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8889&lt;/span&gt;

&lt;span class="c1"&gt;# you should leave the this open&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="in-your-local-computer"&gt;In your local computer&lt;/h1&gt;
&lt;p&gt;In your local computer, open MS-DOS cmd (if using Windows) or Unix terminal, then type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh -i path/to/public-key -N -f -L localhost:8888:localhost:8889 username@your_remote_host_name
&lt;span class="c1"&gt;# make sure to change `username` to your real username in remote host&lt;/span&gt;
&lt;span class="c1"&gt;# change `your_remote_host_name` to your address of your working station&lt;/span&gt;
&lt;span class="c1"&gt;# Example: &lt;/span&gt;
ssh -i ~/.ssh/.quang&lt;span class="se"&gt;\@&lt;/span&gt; -N -f -L localhost:8888:localhost:8889 ubuntu@10.0.6.236

&lt;span class="c1"&gt;# I made a function in bash-profile&lt;/span&gt;
fw remote_port local_port

&lt;span class="c1"&gt;# Example&lt;/span&gt;
fw &lt;span class="m"&gt;8889&lt;/span&gt; &lt;span class="m"&gt;8888&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now open web browser (google chrome, firefox, ...) and type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;localhost&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;8888&lt;/span&gt;
&lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;will&lt;/span&gt; &lt;span class="n"&gt;see&lt;/span&gt; &lt;span class="n"&gt;your&lt;/span&gt; &lt;span class="n"&gt;notebooks&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;your&lt;/span&gt; &lt;span class="n"&gt;given&lt;/span&gt; &lt;span class="n"&gt;directory&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Jupyter Notebook"></category><category term="TUTORIAL"></category></entry><entry><title>Result on Face Detection</title><link href="/blog/tutorial/2018/result-on-face-detection/" rel="alternate"></link><published>2018-08-24T12:29:05+00:00</published><updated>2018-08-24T12:29:05+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-24:/blog/tutorial/2018/result-on-face-detection/</id><summary type="html">&lt;h1 id="benchmarks"&gt;Benchmarks&lt;/h1&gt;
&lt;h2 id="validation"&gt;Validation&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Scale Face Val&lt;/td&gt;
&lt;td&gt;86.8&lt;/td&gt;
&lt;td&gt;86.7&lt;/td&gt;
&lt;td&gt;77.2&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSH (Pyramid) Val&lt;/td&gt;
&lt;td&gt;93.1&lt;/td&gt;
&lt;td&gt;92.1&lt;/td&gt;
&lt;td&gt;84.5&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;98.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HR Val&lt;/td&gt;
&lt;td&gt;91.9&lt;/td&gt;
&lt;td&gt;90.8&lt;/td&gt;
&lt;td&gt;82.3&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-CNN Val&lt;/td&gt;
&lt;td&gt;93.8&lt;/td&gt;
&lt;td&gt;92.2&lt;/td&gt;
&lt;td&gt;82.9&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-FCN Val&lt;/td&gt;
&lt;td&gt;94.7&lt;/td&gt;
&lt;td&gt;93.5&lt;/td&gt;
&lt;td&gt;87.4&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FAN Val&lt;/td&gt;
&lt;td&gt;95.3&lt;/td&gt;
&lt;td&gt;94.2&lt;/td&gt;
&lt;td&gt;88.8&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Seeing Small Faces Val&lt;/td&gt;
&lt;td&gt;94.9&lt;/td&gt;
&lt;td&gt;93.3&lt;/td&gt;
&lt;td&gt;86.1&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;99.23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SFD Val&lt;/td&gt;
&lt;td&gt;93.7&lt;/td&gt;
&lt;td&gt;92.4&lt;/td&gt;
&lt;td&gt;85.2&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;98.49&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="test"&gt;Test&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Scale Face Test&lt;/td&gt;
&lt;td&gt;86.7&lt;/td&gt;
&lt;td&gt;86.6&lt;/td&gt;
&lt;td&gt;76.4&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HR Test&lt;/td&gt;
&lt;td&gt;91.5&lt;/td&gt;
&lt;td&gt;90.2&lt;/td&gt;
&lt;td&gt;81.3&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-CNN Test&lt;/td&gt;
&lt;td&gt;93.2&lt;/td&gt;
&lt;td&gt;91.6&lt;/td&gt;
&lt;td&gt;82.7&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSH (Pyramid) Test&lt;/td&gt;
&lt;td&gt;92.7&lt;/td&gt;
&lt;td&gt;91.5&lt;/td&gt;
&lt;td&gt;84.4&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SFD Test …&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</summary><content type="html">&lt;h1 id="benchmarks"&gt;Benchmarks&lt;/h1&gt;
&lt;h2 id="validation"&gt;Validation&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Scale Face Val&lt;/td&gt;
&lt;td&gt;86.8&lt;/td&gt;
&lt;td&gt;86.7&lt;/td&gt;
&lt;td&gt;77.2&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSH (Pyramid) Val&lt;/td&gt;
&lt;td&gt;93.1&lt;/td&gt;
&lt;td&gt;92.1&lt;/td&gt;
&lt;td&gt;84.5&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;98.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HR Val&lt;/td&gt;
&lt;td&gt;91.9&lt;/td&gt;
&lt;td&gt;90.8&lt;/td&gt;
&lt;td&gt;82.3&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-CNN Val&lt;/td&gt;
&lt;td&gt;93.8&lt;/td&gt;
&lt;td&gt;92.2&lt;/td&gt;
&lt;td&gt;82.9&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-FCN Val&lt;/td&gt;
&lt;td&gt;94.7&lt;/td&gt;
&lt;td&gt;93.5&lt;/td&gt;
&lt;td&gt;87.4&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FAN Val&lt;/td&gt;
&lt;td&gt;95.3&lt;/td&gt;
&lt;td&gt;94.2&lt;/td&gt;
&lt;td&gt;88.8&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Seeing Small Faces Val&lt;/td&gt;
&lt;td&gt;94.9&lt;/td&gt;
&lt;td&gt;93.3&lt;/td&gt;
&lt;td&gt;86.1&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;99.23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SFD Val&lt;/td&gt;
&lt;td&gt;93.7&lt;/td&gt;
&lt;td&gt;92.4&lt;/td&gt;
&lt;td&gt;85.2&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;98.49&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="test"&gt;Test&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Scale Face Test&lt;/td&gt;
&lt;td&gt;86.7&lt;/td&gt;
&lt;td&gt;86.6&lt;/td&gt;
&lt;td&gt;76.4&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HR Test&lt;/td&gt;
&lt;td&gt;91.5&lt;/td&gt;
&lt;td&gt;90.2&lt;/td&gt;
&lt;td&gt;81.3&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-CNN Test&lt;/td&gt;
&lt;td&gt;93.2&lt;/td&gt;
&lt;td&gt;91.6&lt;/td&gt;
&lt;td&gt;82.7&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSH (Pyramid) Test&lt;/td&gt;
&lt;td&gt;92.7&lt;/td&gt;
&lt;td&gt;91.5&lt;/td&gt;
&lt;td&gt;84.4&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SFD Test&lt;/td&gt;
&lt;td&gt;92.8&lt;/td&gt;
&lt;td&gt;91.3&lt;/td&gt;
&lt;td&gt;84.0&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-FCN Test&lt;/td&gt;
&lt;td&gt;94.3&lt;/td&gt;
&lt;td&gt;93.1&lt;/td&gt;
&lt;td&gt;87.6&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Seeing Small Faces Test&lt;/td&gt;
&lt;td&gt;94.9&lt;/td&gt;
&lt;td&gt;93.5&lt;/td&gt;
&lt;td&gt;86.5&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;99.23&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="methods_1"&gt;Methods&lt;/h1&gt;
&lt;h2 id="hr-finding-tiny-faces"&gt;HR - Finding Tiny Faces&lt;/h2&gt;
&lt;p&gt;Finding Tiny Faces. IEEE Conference on Computer Vision and 
Pattern Recognition (CVPR), 2017. &lt;/p&gt;
&lt;p&gt;Peiyun Hu, Deva Ramanan
Robotics Institute
Carnegie Mellon University&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1612.04402v1.pdf&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;HR Val&lt;/td&gt;
&lt;td&gt;91.9&lt;/td&gt;
&lt;td&gt;90.8&lt;/td&gt;
&lt;td&gt;82.3&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HR Test&lt;/td&gt;
&lt;td&gt;91.5&lt;/td&gt;
&lt;td&gt;90.2&lt;/td&gt;
&lt;td&gt;81.3&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="HR.png"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the shared CNNs, we experimented with three different architectures: ResNet101, ResNet50, and VGG16&lt;/li&gt;
&lt;li&gt;Positive examples IoU &amp;gt; 0.7&lt;/li&gt;
&lt;li&gt;Negative examples &amp;lt; 0.3 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contribution&lt;/strong&gt;
1. We provide an in-depth analysis of image resolution, object scale, and spatial context for the purposes of finding small faces. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We demonstrate stateof-the-art results on massively-benchmarked face datasets (FDDB and WIDER FACE). In particular, when compared to prior art on WIDER FACE, our results reduce error by a factor of 2 (our models produce an AP of 81% while prior art ranges from 29-64%).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="face-r-cnn"&gt;Face R-CNN&lt;/h2&gt;
&lt;p&gt;Hao Wang Zhifeng Li&amp;lowast; Xing Ji Yitong Wang
Tencent AI Lab, China&lt;/p&gt;
&lt;p&gt;Face R-CNN. arXiv preprint arXiv:1706.01061, 2017. &lt;/p&gt;
&lt;p&gt;Paper:  https://arxiv.org/pdf/1706.01061.pdf&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Face R-CNN Val&lt;/td&gt;
&lt;td&gt;93.8&lt;/td&gt;
&lt;td&gt;92.2&lt;/td&gt;
&lt;td&gt;82.9&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-CNN Test&lt;/td&gt;
&lt;td&gt;93.2&lt;/td&gt;
&lt;td&gt;91.6&lt;/td&gt;
&lt;td&gt;82.7&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Implementations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="Face_R_CNN.png"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VGG19 with ImageNet pretrained model&lt;/li&gt;
&lt;li&gt;Positive examples IoU &amp;gt; 0.5&lt;/li&gt;
&lt;li&gt;Negative examples 0.1 &amp;lt; IoU &amp;lt; 0.5 &lt;/li&gt;
&lt;li&gt;IoU of NMS = 0.7&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contribution&lt;/strong&gt;
The major contributions of this work are summarized as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Considering the specific property of face detection, we propose a Faster R-CNN based approach
called Face R-CNN for face detection by integrating several newly developed techniques including
center loss, online hard example mining, and multi-scale training.&lt;/li&gt;
&lt;li&gt;The proposed approach differs from the available Faster R-CNN based face detection methods.
First, this is the first attempt to use the center loss to reduce the large intra-class variations in face
detection. Second, the use of online hard example mining in our approach differs from the others.
By appropriately setting the ratio between positive hard samples and negative hard samples, the
combination of OHEM and center loss can lead to better performance.&lt;/li&gt;
&lt;li&gt;The proposed approach consistently obtains superior performance over the state-of-the-arts on
two public-domain face detection benchmarks (WIDER FACE dataset [25] and FDDB dataset [28]).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="scaleface"&gt;ScaleFace&lt;/h2&gt;
&lt;p&gt;Face Detection through Scale-Friendly Deep Convolutional Networks. arXiv preprint arXiv:1706.02863, 2017. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Scale Face Val&lt;/td&gt;
&lt;td&gt;86.8&lt;/td&gt;
&lt;td&gt;86.7&lt;/td&gt;
&lt;td&gt;77.2&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scale Face Test&lt;/td&gt;
&lt;td&gt;86.7&lt;/td&gt;
&lt;td&gt;86.6&lt;/td&gt;
&lt;td&gt;76.4&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt="" src="Scale_Face_result.png"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;
&lt;img alt="" src="Scale_Face.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contribution&lt;/strong&gt;
* Runtime NVIDIA Titan X GPU
900 x 1300 (7.1 fps)&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="ScaleFace_Runtime.png"/&gt;&lt;/p&gt;
&lt;h2 id="ssh"&gt;SSH&lt;/h2&gt;
&lt;p&gt;SSH: Single Stage Headless Face Detector. IEEE International 
Conference on Computer Vision (ICCV), 2017.
Mahyar Najibi&lt;em&gt; Pouya Samangouei&lt;/em&gt; Rama Chellappa Larry S. Davis&lt;/p&gt;
&lt;p&gt;University of Maryland&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1708.03979.pdf &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SSH (Pyramid) Val&lt;/td&gt;
&lt;td&gt;93.1&lt;/td&gt;
&lt;td&gt;92.1&lt;/td&gt;
&lt;td&gt;84.5&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;98.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSH (Pyramid) Test&lt;/td&gt;
&lt;td&gt;92.7&lt;/td&gt;
&lt;td&gt;91.5&lt;/td&gt;
&lt;td&gt;84.4&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;
&lt;img alt="" src="SSH.png"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Positive examples IoU &amp;gt; 0.5, This is in contrast to the methods based on Faster RCNN which assign to each ground-truth at least one anchor
with the highest IoU.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Negative examples IoU &amp;lt; 0.3&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contribution&lt;/strong&gt;
* That is, it is able to achieve state-of-the-art results while removing the &amp;ldquo;head&amp;rdquo; of its underlying classification network &amp;ndash; i.e. all fully connected layers in the VGG-16 which contains a large number of parameters. Additionally, instead of relying on an image pyramid to detect faces with various scales, SSH is scale-invariant by design.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SSH beats the ResNet-101- based state-of-the-art on the WIDER dataset. Even though, unlike the current state-of-the-art, SSH does not use an image
pyramid and is 5X faster. Moreover, if an image pyramid is deployed, our light-weight network achieves stateof-the-art on all subsets of the WIDER dataset, improving the AP by 2.5%. &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="sfd"&gt;SFD&lt;/h2&gt;
&lt;p&gt;S&amp;sup3;FD: Single Shot Scale-invariant Face Detector. IEEE International Conference on Computer Vision (ICCV), 2017. 
&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SFD Val&lt;/td&gt;
&lt;td&gt;93.7&lt;/td&gt;
&lt;td&gt;92.4&lt;/td&gt;
&lt;td&gt;85.2&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;98.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SFD Test&lt;/td&gt;
&lt;td&gt;92.8&lt;/td&gt;
&lt;td&gt;91.3&lt;/td&gt;
&lt;td&gt;84.0&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Runtime 
* The speed using Titan X 36 FPS  VGA-resolution (640 x 480)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;
&lt;img alt="" src="sfd_architecture.png"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Loss = Cls_Loss * 4 + Loc_Loss&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Negatives : Positives = 3 : 1 
&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Proposing a scale-equitable face detection framework
with a wide range of anchor-associated layers and a
series of reasonable anchor scales so as to handle different
scales of faces well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Presenting a scale compensation anchor matching
strategy to improve the recall rate of small faces.&lt;/li&gt;
&lt;li&gt;Introducing a max-out background label to reduce the
high false positive rate of small faces.&lt;/li&gt;
&lt;li&gt;Achieving state-of-the-art results on AFW, PASCAL
face, FDDB and WIDER FACE with real-time speed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="face-r-fcn"&gt;Face R-FCN&lt;/h2&gt;
&lt;p&gt;Detecting Faces Using Region-based Fully
Convolutional Networks
&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Face R-FCN Val&lt;/td&gt;
&lt;td&gt;94.7&lt;/td&gt;
&lt;td&gt;93.5&lt;/td&gt;
&lt;td&gt;87.4&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-FCN Test&lt;/td&gt;
&lt;td&gt;94.3&lt;/td&gt;
&lt;td&gt;93.1&lt;/td&gt;
&lt;td&gt;87.6&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;
&lt;img alt="" src="Face_RFCN.png"/&gt;
* Resnet101 as base network&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;These anchors then map to the original image to calculate the IoU scores
with the ground truth for further picking up with following rules: First, the anchors with highest IoU score are strictly kept as positive; Second, the anchors with IoU score above 0.7 are assigned as positive; Third, If the anchors have IoU score that is lower than 0.3, they are marked as negative.
The R-FCN is then trained on the processed anchors (proposals) where the positive samples and negative samples are defined as IoU greater than 0.5 and between 0.1 and 0.5 respectively.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the testing stage, multi-scale testing is performed by scale image into
an image pyramid for better detecting on both tiny and general faces.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="mscnn"&gt;MSCNN&lt;/h2&gt;
&lt;p&gt;A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection, European Conference on Computer Vision (ECCV), 2016. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;
&lt;img alt="" src="fan.png"/&gt;
&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="fan"&gt;FAN&lt;/h2&gt;
&lt;p&gt;Face Attention Network: An Effective Face Detector for the Occluded Faces. arXiv preprint arXiv:1711.07246, 2017. &lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1711.07246.pdf&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;FAN Val&lt;/td&gt;
&lt;td&gt;95.3&lt;/td&gt;
&lt;td&gt;94.2&lt;/td&gt;
&lt;td&gt;88.8&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FAN Test&lt;/td&gt;
&lt;td&gt;94.6&lt;/td&gt;
&lt;td&gt;93.6&lt;/td&gt;
&lt;td&gt;88.5&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contribution&lt;/strong&gt;
In summary, there are three contributions in our paper.
&lt;em&gt; We propose a anchor-level attention, which can well address the occlusion issue in the face detection task. One illustrative example for our detection results in the crowd case can be found in Figure 1.
&lt;/em&gt; A practical baseline setting is introduced based on the one-shot RetinaNet detector, which obtains comparable performance with fast computation speed.
* Our FAN which integrates our reproduced one-shot RetinaNet and anchor-level attention significantly outperforms state-of-art detectors on the popular face detection benchmarks including WiderFace [34] and
MAFA [6], especially in the occluded cases like MAFA.&lt;/p&gt;
&lt;h2 id="seeing-small-face-cvpr-2018"&gt;Seeing Small Face CVPR 2018&lt;/h2&gt;
&lt;p&gt;Seeing Small Faces from Robust Anchor's Perspective. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Seeing Small Faces Val&lt;/td&gt;
&lt;td&gt;94.9&lt;/td&gt;
&lt;td&gt;93.3&lt;/td&gt;
&lt;td&gt;86.1&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;99.23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Seeing Small Faces Test&lt;/td&gt;
&lt;td&gt;94.9&lt;/td&gt;
&lt;td&gt;93.5&lt;/td&gt;
&lt;td&gt;86.5&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;99.23&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Runtime speed on single NVIDIA Titan X GPU with batch size 1:&lt;ul&gt;
&lt;li&gt;85% on size 1400x1800 = 1s&lt;/li&gt;
&lt;li&gt;75.7% on size 600x1000 = 0.2s
&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="pyramidbox"&gt;PyramidBox&lt;/h2&gt;
&lt;p&gt;PyramidBox: A Context-assisted Single Shot Face Detector. arXiv preprint arXiv:1803.07737, 2018. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/p&gt;</content><category term="Face Detection"></category><category term="TUTORIAL"></category></entry><entry><title>Keras model for Cdiscount Kaggle competition</title><link href="/blog/tutorial/2018/keras-model-for-cdiscount-kaggle-competition/" rel="alternate"></link><published>2018-08-23T16:34:17+00:00</published><updated>2018-08-23T16:34:17+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-23:/blog/tutorial/2018/keras-model-for-cdiscount-kaggle-competition/</id><summary type="html">&lt;h1 id="deepsenseai"&gt;Deepsense.ai&lt;/h1&gt;
&lt;p&gt;https://github.com/neptune-ml/open-solution-cdiscount-starter&lt;/p&gt;
&lt;p&gt;https://deepsense.ai/image-classification-sample-solution-kaggle/&lt;/p&gt;</summary><content type="html">&lt;h1 id="deepsenseai"&gt;Deepsense.ai&lt;/h1&gt;
&lt;p&gt;https://github.com/neptune-ml/open-solution-cdiscount-starter&lt;/p&gt;
&lt;p&gt;https://deepsense.ai/image-classification-sample-solution-kaggle/&lt;/p&gt;</content><category term="Cdiscount"></category><category term="Kaggle"></category><category term="TUTORIAL"></category></entry><entry><title>Read the AFW mat file</title><link href="/blog/tutorial/2018/read-the-afw-mat-file/" rel="alternate"></link><published>2018-08-23T14:33:34+00:00</published><updated>2018-08-23T14:33:34+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-23:/blog/tutorial/2018/read-the-afw-mat-file/</id><summary type="html">&lt;p&gt;| This can be applied to read &lt;code&gt;.mat&lt;/code&gt; file of Pascal Face.&lt;/p&gt;
&lt;style type="text/css"&gt;/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The …&lt;/style&gt;</summary><content type="html">&lt;p&gt;| This can be applied to read &lt;code&gt;.mat&lt;/code&gt; file of Pascal Face.&lt;/p&gt;
&lt;style type="text/css"&gt;/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell &gt; div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area &gt; div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area &gt; div.highlight &gt; pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the &lt;head&gt; if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev &lt;Maniac@SoftwareManiacs.Org&gt;
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph &gt; img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell &gt; div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
&lt;/style&gt;
&lt;style type="text/css"&gt;.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */&lt;/style&gt;
&lt;style type="text/css"&gt;
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
&lt;/style&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Load-annotations-of-AFW-dataset-from-matlab-file"&gt;Load annotations of AFW dataset from matlab file&lt;a class="anchor-link" href="#Load-annotations-of-AFW-dataset-from-matlab-file"&gt;&amp;para;&lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Import-matlab-utils-from-scipy"&gt;Import matlab utils from scipy&lt;a class="anchor-link" href="#Import-matlab-utils-from-scipy"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[23]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.io.matlab&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="n"&gt;loadmat&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;PIL&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="The-matlab-file"&gt;The matlab file&lt;a class="anchor-link" href="#The-matlab-file"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The file is found on &lt;code&gt;annotations&lt;/code&gt; folder of &lt;code&gt;Accuracy tool&lt;/code&gt; for Face Detection  developed by M. Mathias, R. Benenson, M. Pedersoli, L. Van Gool, ECCV 2014 at &lt;a href="http://markusmathias.bitbucket.org/2014_eccv_face_detection/"&gt;bitbutket&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[24]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;file_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Annotations_Face_PASCALLayout_large_fixed.mat"&lt;/span&gt;
&lt;span class="c1"&gt;# Load the file&lt;/span&gt;
&lt;span class="n"&gt;ann_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loadmat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file_path&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s1"&gt;'Annotations'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ann_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[24]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;851&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Information-about-file"&gt;Information about file&lt;a class="anchor-link" href="#Information-about-file"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ann_data&lt;/code&gt; contains &lt;code&gt;imgname&lt;/code&gt;, &lt;code&gt;objects&lt;/code&gt; which is a set of bounding boxes. &lt;/li&gt;
&lt;li&gt;It contains the information of 205 images inside.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[25]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ann_data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;ann_data&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;851
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[25]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;array([[(array(['2008_003880.jpg'], dtype='&amp;lt;U15'), array([[360],
       [480],
       [  3]]), array([[135, 130, 179, 182,   0,   0]]), array([[0]]))]],
      dtype=[('imgname', 'O'), ('imgsize', 'O'), ('objects', 'O'), ('flags', 'O')])&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Loops-to-get-image-paths-and-bounding-boxes"&gt;Loops to get image paths and bounding boxes&lt;a class="anchor-link" href="#Loops-to-get-image-paths-and-bounding-boxes"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[16]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_img_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;all_img_boxes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ann_data&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="n"&gt;img_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ann_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'imgname'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;all_img_names&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    
    &lt;span class="c1"&gt;# a set of bounding boxes for this image&lt;/span&gt;
    &lt;span class="n"&gt;img_bboxes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;items&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ann_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;'objects'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;xmin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;ymin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;xmax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;ymax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;img_bboxes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xmax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymax&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    
    &lt;span class="n"&gt;all_img_boxes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_bboxes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;all_img_boxes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_img_boxes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[17]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_img_names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[17]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;'2007_000272.jpg'&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[18]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_img_boxes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[18]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;[[166, 118, 246, 235]]&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Check-the-result-by-visualization"&gt;Check the result by visualization&lt;a class="anchor-link" href="#Check-the-result-by-visualization"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[19]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;draw_bboxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img_bboxes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;current_axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    
    &lt;span class="c1"&gt;# Draw the ground truth boxes in green&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;img_bboxes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;xmin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;ymin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;xmax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;ymax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;current_axis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_patch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Rectangle&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymin&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; 
                                         &lt;span class="n"&gt;xmax&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymax&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;ymin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                                         &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'green'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                          
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[46]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Read the image&lt;/span&gt;
&lt;span class="n"&gt;img_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'/Users/quanguet/Datasets/PascalFace/'&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;all_img_names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Get the bounding boxes&lt;/span&gt;
&lt;span class="n"&gt;img_bboxes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;all_img_boxes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[47]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;draw_bboxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img_bboxes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_png output_subarea "&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Build-the-dataset-file-for-Pytorch-Model"&gt;Build the dataset file for Pytorch Model&lt;a class="anchor-link" href="#Build-the-dataset-file-for-Pytorch-Model"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The file contains 205 lines in which each line represents for 1 image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each line has the format:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;img_name.jpg box1 box2 ... boxk&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Each box has the format:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;xmin ymin xmax ymax label&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For face detection, the &lt;code&gt;label&lt;/code&gt; = &lt;code&gt;0&lt;/code&gt; because we have only one label.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[54]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;afw_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'pascal_file.txt'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'w'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[55]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img_bboxes&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_img_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_img_boxes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;afw_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;' '&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;img_bboxes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# The bbox&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;afw_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;afw_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;' '&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# The label&lt;/span&gt;
        &lt;span class="n"&gt;afw_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'0 '&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        
    &lt;span class="n"&gt;afw_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;afw_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[56]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;head -2 new_afw_file.txt
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;2007_000272.jpg 166 118 246 235 0 
2007_000664.jpg 94 91 274 252 0 
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="Check-whether-Pascal-Face-Image-exists-or-not_1"&gt;Check whether Pascal Face Image exists or not&lt;a class="anchor-link" href="#Check-whether-Pascal-Face-Image-exists-or-not"&gt;&amp;para;&lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[10]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'pascal_file.txt'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'r'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;readlines&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[13]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;img_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;' '&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;img_names&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[13]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;['2007_000272.jpg', '2007_000664.jpg']&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[15]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="n"&gt;img_paths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'/Users/quanguet/Datasets/PascalFace/'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
             &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;img_name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;img_names&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;img_paths&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[15]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;'/Users/quanguet/Datasets/PascalFace/2007_000272.jpg'&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[21]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;img_path&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;img_paths&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'No'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;No /Users/quanguet/Datasets/PascalFace/2008_000210.jpg
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[22]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'/Users/quanguet/Datasets/PascalFace/2011_000360.jpg'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[22]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;True&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[&amp;nbsp;]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AFW"></category><category term="Matlab"></category><category term="Face Detection"></category><category term="TUTORIAL"></category></entry><entry><title>AI nào cho Việt Nam</title><link href="/blog/other/2018/ai-nao-cho-viet-nam/" rel="alternate"></link><published>2018-08-23T08:26:00+00:00</published><updated>2018-08-23T08:26:00+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-23:/blog/other/2018/ai-nao-cho-viet-nam/</id><summary type="html">&lt;h1 id="ai-nao-cho-viet-nam"&gt;AI n&amp;agrave;o cho Việt Nam?&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;T&amp;aacute;c giả: Gi&amp;aacute;o sư Hồ T&amp;uacute; Bảo
Viện John von Neumann, Đại học Quốc gia th&amp;agrave;nh phố Hồ Ch&amp;iacute; Minh
Viện Nghi&amp;ecirc;n cứu Cao cấp về To&amp;aacute;n.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sau hơn một năm cả nước đ&amp;atilde; n&amp;oacute;i rất nhiều về c&amp;aacute;ch mạng c&amp;ocirc;ng nghiệp lần thứ tư l&amp;agrave;
g&amp;igrave;, đ&amp;atilde; đến l&amp;uacute;c ch&amp;uacute;ng ta phải h&amp;agrave;nh động nhiều hơn, trong đ&amp;oacute; c&amp;oacute; việc cần l&amp;agrave;m g&amp;igrave; v&amp;agrave;
l&amp;agrave;m thế n&amp;agrave;o?&lt;/p&gt;
&lt;p&gt;Sự cộng hưởng trong những năm vừa qua của c&amp;aacute;c c&amp;ocirc;ng nghệ số c&amp;oacute; nhiều đột ph&amp;aacute;
(như điện to&amp;aacute;n đ&amp;aacute;m m&amp;acirc;y, internet vạn vật, dữ liệu lớn, tr&amp;iacute; tuệ nh&amp;acirc;n tạo&amp;hellip;) đ&amp;atilde; b&amp;aacute;o
hiệu những thay đổi lớn lao đang bắt đầu xảy ra, được gọi ở nhiều nơi l&amp;agrave; cuộc c&amp;aacute;ch
mạng c&amp;ocirc;ng nghiệp lần thứ tư với đặc trưng cơ bản l&amp;agrave; sản xuất th&amp;ocirc;ng minh. Việc sản
xuất th&amp;ocirc;ng minh n&amp;agrave;y được xem …&lt;/p&gt;</summary><content type="html">&lt;h1 id="ai-nao-cho-viet-nam"&gt;AI n&amp;agrave;o cho Việt Nam?&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;T&amp;aacute;c giả: Gi&amp;aacute;o sư Hồ T&amp;uacute; Bảo
Viện John von Neumann, Đại học Quốc gia th&amp;agrave;nh phố Hồ Ch&amp;iacute; Minh
Viện Nghi&amp;ecirc;n cứu Cao cấp về To&amp;aacute;n.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sau hơn một năm cả nước đ&amp;atilde; n&amp;oacute;i rất nhiều về c&amp;aacute;ch mạng c&amp;ocirc;ng nghiệp lần thứ tư l&amp;agrave;
g&amp;igrave;, đ&amp;atilde; đến l&amp;uacute;c ch&amp;uacute;ng ta phải h&amp;agrave;nh động nhiều hơn, trong đ&amp;oacute; c&amp;oacute; việc cần l&amp;agrave;m g&amp;igrave; v&amp;agrave;
l&amp;agrave;m thế n&amp;agrave;o?&lt;/p&gt;
&lt;p&gt;Sự cộng hưởng trong những năm vừa qua của c&amp;aacute;c c&amp;ocirc;ng nghệ số c&amp;oacute; nhiều đột ph&amp;aacute;
(như điện to&amp;aacute;n đ&amp;aacute;m m&amp;acirc;y, internet vạn vật, dữ liệu lớn, tr&amp;iacute; tuệ nh&amp;acirc;n tạo&amp;hellip;) đ&amp;atilde; b&amp;aacute;o
hiệu những thay đổi lớn lao đang bắt đầu xảy ra, được gọi ở nhiều nơi l&amp;agrave; cuộc c&amp;aacute;ch
mạng c&amp;ocirc;ng nghiệp lần thứ tư với đặc trưng cơ bản l&amp;agrave; sản xuất th&amp;ocirc;ng minh. Việc sản
xuất th&amp;ocirc;ng minh n&amp;agrave;y được xem v&amp;agrave; tin rằng sẽ chủ yếu dựa v&amp;agrave;o lĩnh vực tr&amp;iacute; tuệ nh&amp;acirc;n
tạo, thường gọi l&amp;agrave; AI theo t&amp;ecirc;n tiếng Anh. AI c&amp;ograve;n được hiểu một c&amp;aacute;ch d&amp;acirc;n d&amp;atilde; hơn l&amp;agrave;
&amp;ldquo;th&amp;ocirc;ng minh nh&amp;acirc;n tạo&amp;rdquo;, tức l&amp;agrave; l&amp;agrave;m sao cho m&amp;aacute;y m&amp;oacute;c, chủ yếu l&amp;agrave; m&amp;aacute;y t&amp;iacute;nh, c&amp;oacute; những
khả năng của tr&amp;iacute; th&amp;ocirc;ng minh con người.&lt;/p&gt;
&lt;p&gt;Nếu n&amp;oacute;i một c&amp;aacute;ch dễ hiểu th&amp;igrave; AI l&amp;agrave; g&amp;igrave;? Tại sao cần v&amp;agrave; c&amp;oacute; thể d&amp;ugrave;ng AI để ph&amp;aacute;t triển
đất nước khi nhiều lĩnh vực sản xuất của ta đang ở tr&amp;igrave;nh độ của c&amp;aacute;ch mạng c&amp;ocirc;ng
nghiệp những lần trước? Quan trọng hơn, ta cần AI g&amp;igrave;, ph&amp;aacute;t triển v&amp;agrave; d&amp;ugrave;ng AI đ&amp;oacute; thế
n&amp;agrave;o?
AI đ&amp;atilde; c&amp;oacute; một lịch sử hơn 60 năm, ngay sau khi những chiếc m&amp;aacute;y t&amp;iacute;nh điện tử đầu
ti&amp;ecirc;n với chức năng cộng trừ nh&amp;acirc;n chia ra đời v&amp;agrave;o những năm 1940 của thế kỷ trước.
Nhưng chỉ &amp;iacute;t năm sau đ&amp;oacute;, con người đ&amp;atilde; c&amp;oacute; mục ti&amp;ecirc;u l&amp;agrave;m những chiếc m&amp;aacute;y t&amp;iacute;nh c&amp;oacute; AI,
ti&amp;ecirc;u biểu l&amp;agrave; c&amp;aacute;c khả năng suy luận, giải quyết vấn đề, hiểu ng&amp;ocirc;n ngữ, nhận thức v&amp;agrave;
học tập.
Trước hết l&amp;agrave; l&amp;agrave;m cho m&amp;aacute;y biết suy luận. Để l&amp;agrave;m vậy cần đưa được c&amp;aacute;c nguy&amp;ecirc;n l&amp;yacute;,
c&amp;aacute;c c&amp;aacute;ch lập luận logic của con người v&amp;agrave;o m&amp;aacute;y dưới dạng c&amp;aacute;c chương tr&amp;igrave;nh gọi l&amp;agrave;
động cơ suy diễn. Ngo&amp;agrave;i ra, phải đưa v&amp;agrave;o v&amp;agrave; diễn đạt được tri thức của con người
trong m&amp;aacute;y theo những c&amp;aacute;ch n&amp;agrave;o đ&amp;oacute;, gọi l&amp;agrave; cơ sở tri thức, để động cơ suy diễn d&amp;ugrave;ng
đuợc c&amp;aacute;c tri thức n&amp;agrave;y khi lập luận. C&amp;oacute; thể h&amp;igrave;nh dung động cơ suy diễn như c&amp;aacute;i m&amp;aacute;y
c&amp;ograve;n tri thức như nhi&amp;ecirc;n liệu đưa v&amp;agrave;o trong m&amp;aacute;y đ&amp;oacute; để tạo ra c&amp;aacute;c kết quả suy diễn. Đ&amp;atilde;
từ l&amp;acirc;u, c&amp;ocirc;ng thức cơ bản của AI l&amp;agrave; &amp;ldquo;AI = suy diễn + tri thức&amp;rdquo;.
Những   mốc ch&amp;iacute;nh   trong   lịch    sử ph&amp;aacute;t triển   ng&amp;agrave;nh   AI
Thực hiện c&amp;ocirc;ng thức tr&amp;ocirc;ng đơn giản n&amp;agrave;y kh&amp;ocirc;ng hề đơn giản. Trong những năm 70
của thế kỷ trước một mục ti&amp;ecirc;u ch&amp;iacute;nh của AI li&amp;ecirc;n quan việc m&amp;aacute;y suy luận l&amp;agrave; c&amp;aacute;c hệ
chuy&amp;ecirc;n gia. Mỗi hệ chuy&amp;ecirc;n gia l&amp;agrave; một chương tr&amp;igrave;nh m&amp;aacute;y t&amp;iacute;nh biết suy luận với tri
thức chuy&amp;ecirc;n gia trong một lĩnh vực hẹp, c&amp;oacute; thể tương t&amp;aacute;c với người d&amp;ugrave;ng để đưa ra
c&amp;aacute;c quyết định, c&amp;aacute;c lời khuy&amp;ecirc;n&amp;hellip; như của c&amp;aacute;c chuy&amp;ecirc;n gia. Hơn ba mươi năm trước
những &amp;ldquo;d&amp;acirc;n AI&amp;rdquo; ở ta đ&amp;atilde; nỗ lực l&amp;agrave;m một hệ chuy&amp;ecirc;n gia về chẩn đo&amp;aacute;n bệnh với mơ
ước sẽ copy th&amp;agrave;nh h&amp;agrave;ng ngh&amp;igrave;n bản để gi&amp;uacute;p b&amp;aacute;c sĩ ở những bệnh viện hay trạm y tế
xa x&amp;ocirc;i cũng c&amp;oacute; thể kh&amp;aacute;m chữa bệnh gần như c&amp;aacute;c thầy thuốc bậc thầy. Tuy nhi&amp;ecirc;n
việc n&amp;agrave;y kh&amp;ocirc;ng th&amp;agrave;nh. Kh&amp;ocirc;ng chỉ c&amp;oacute; thất bại ở Việt Nam, c&amp;aacute;c hệ chuy&amp;ecirc;n gia cũng
thất bại khắp nơi. Nguy&amp;ecirc;n nh&amp;acirc;n ch&amp;iacute;nh l&amp;agrave; việc thu thập tri thức chuy&amp;ecirc;n gia dưới c&amp;aacute;c
dạng tường minh để c&amp;oacute; thể đưa v&amp;agrave;o m&amp;aacute;y t&amp;iacute;nh thực ra rất kh&amp;oacute; khăn. Vượt qua kh&amp;oacute;
khăn n&amp;agrave;y ch&amp;iacute;nh l&amp;agrave; một động lực của lĩnh vực học m&amp;aacute;y (machine learning), hiện nay
đang đ&amp;oacute;ng vai tr&amp;ograve; trung t&amp;acirc;m của AI.
Khả năng giải quyết vấn đề của m&amp;aacute;y cũng đ&amp;atilde; chủ yếu dựa v&amp;agrave;o khả năng suy diễn
của m&amp;aacute;y nhằm từng bước t&amp;igrave;m ra &amp;ldquo;đường đi nước bước&amp;rdquo; để dần đến đ&amp;iacute;ch. Khả năng
n&amp;agrave;y thường được &amp;ldquo;đo&amp;rdquo; bằng c&amp;aacute;c trận đấu cờ của m&amp;aacute;y với người. V&amp;agrave;o năm 1997 m&amp;aacute;y
t&amp;iacute;nh Deep Blue của IBM với AI đ&amp;atilde; đ&amp;aacute;nh bại nh&amp;agrave; v&amp;ocirc; địch cờ vua thế giới Garry
Kasparov, v&amp;agrave; cuối năm 2006 m&amp;aacute;y t&amp;iacute;nh Deep Fritz lại đ&amp;aacute;nh bại nh&amp;agrave; v&amp;ocirc; địch Kramnik.
Hai chục năm sau, cuối năm 2016 v&amp;agrave; 2017, chương tr&amp;igrave;nh AlphaGo v&amp;agrave; AlphaGo Zero
của Google đ&amp;atilde; thắng Lee Sedol, nh&amp;agrave; v&amp;ocirc; địch của m&amp;ocirc;n cờ Go, vốn phức tạp hơn cờ
vua rất nhiều. AlphaGo dựa tr&amp;ecirc;n kết hợp của kỹ thuật học m&amp;aacute;y v&amp;agrave; t&amp;igrave;m kiếm nhanh
của m&amp;aacute;y t&amp;iacute;nh trong một kh&amp;ocirc;ng gian rất nhiều c&amp;aacute;c nước cờ khi m&amp;aacute;y biết học theo rất
nhiều v&amp;aacute;n cờ nổi tiếng trong qu&amp;aacute; khứ.
L&amp;agrave;m cho m&amp;aacute;y c&amp;oacute; khả năng nhận thức li&amp;ecirc;n quan đến c&amp;aacute;c gi&amp;aacute;c quan của con người.
Trước hết l&amp;agrave; hiểu những thứ nghe v&amp;agrave; nh&amp;igrave;n được. Đ&amp;oacute; l&amp;agrave; việc m&amp;aacute;y &amp;ldquo;nghe&amp;rdquo; được tiếng
người v&amp;agrave; chuyển th&amp;agrave;nh c&amp;aacute;c c&amp;acirc;u chữ ở dạng số (nhận dạng tiếng n&amp;oacute;i). Đ&amp;oacute; l&amp;agrave; việc
m&amp;aacute;y &amp;ldquo;đọc&amp;rdquo; được chữ viết của con người trong s&amp;aacute;ch b&amp;aacute;o v&amp;agrave; chuyển ch&amp;uacute;ng th&amp;agrave;nh c&amp;aacute;c
c&amp;acirc;u chữ tr&amp;ecirc;n m&amp;aacute;y (nhận dạng chữ viết). L&amp;agrave;m m&amp;aacute;y &amp;ldquo;hiểu&amp;rdquo; được tiếng n&amp;oacute;i v&amp;agrave; chữ viết
của con người dựa tr&amp;ecirc;n việc ph&amp;acirc;n t&amp;iacute;ch c&amp;aacute;c c&amp;acirc;u chữ m&amp;aacute;y thu được khi nghe v&amp;agrave; nh&amp;igrave;n
(hiểu ng&amp;ocirc;n ngữ tự nhi&amp;ecirc;n). Một kết quả đặc sắc của việc hiểu n&amp;agrave;y l&amp;agrave; dịch c&amp;aacute;c văn bản
từ một ng&amp;ocirc;n ngữ n&amp;agrave;y sang ng&amp;ocirc;n
ngữ kh&amp;aacute;c (dịch m&amp;aacute;y). Rất nhiều
trong ch&amp;uacute;ng ta đang h&amp;agrave;ng ng&amp;agrave;y
d&amp;ugrave;ng hệ dịch miễn ph&amp;iacute; của Google
để dịch nhanh nhiều t&amp;agrave;i liệu tiếng
Anh qua tiếng Việt. Gần đ&amp;acirc;y hơn,
c&amp;aacute;c ứng dụng nhận dạng tiếng n&amp;oacute;i
bắt đầu trở n&amp;ecirc;n phổ biến.
B&amp;ecirc;n cạnh việc nh&amp;igrave;n v&amp;agrave; đọc được
chữ viết l&amp;agrave; việc l&amp;agrave;m cho m&amp;aacute;y
&amp;ldquo;nh&amp;igrave;n&amp;rdquo; được nhiều thứ kh&amp;aacute;c qua Người m&amp;aacute;y ASIMO đưa đồ uống cho kh&amp;aacute;ch theo y&amp;ecirc;u cầu
(http://world.honda.com/ASIMO/)
camera v&amp;agrave; &amp;ldquo;hiểu&amp;rdquo; c&amp;aacute;c h&amp;igrave;nh ảnh thu được (xử l&amp;yacute; ảnh). Chẳng hạn đ&amp;oacute; l&amp;agrave; l&amp;agrave;m sao m&amp;aacute;y
ph&amp;aacute;t hiện được những bất thường từ những bức ảnh phức tạp trong y học, hoặc
việc m&amp;aacute;y nhận biết được ảnh của những vật thể đang chuyển động (như biển số xe
chạy tr&amp;ecirc;n cao tốc khi trời mưa hay nhận dạng mặt người ở s&amp;acirc;n bay).
Khi internet ra đời, những kỹ thuật AI kể tr&amp;ecirc;n được nh&amp;uacute;ng v&amp;agrave;o m&amp;ocirc;i trường mạng,
như m&amp;aacute;y dịch tự động qua mạng, nhận dạng tiếng n&amp;oacute;i qua mạng, v&amp;agrave; ti&amp;ecirc;u biểu l&amp;agrave; c&amp;aacute;c
t&amp;aacute;c tử th&amp;ocirc;ng minh. Mỗi t&amp;aacute;c tử th&amp;ocirc;ng minh tr&amp;ecirc;n mạng l&amp;agrave; một phần mềm tự quản, tti&amp;ecirc;u
biểu l&amp;agrave; c&amp;aacute;c t&amp;aacute;c tử hoạt động tr&amp;ecirc;n internet v&amp;agrave; c&amp;oacute; thể thực hiện c&amp;aacute;c nhiệm vụ được
giao ph&amp;oacute;. C&amp;ocirc;ng thức cơ bản của AI cho c&amp;aacute;c b&amp;agrave;i to&amp;aacute;n suy diễn, giải quyết vấn đề, lập
kế hoạch, trợ gi&amp;uacute;p quyết định th&amp;ocirc;ng minh&amp;hellip; do vậy trở th&amp;agrave;nh
AI = Suy diễn + Tri thức + M&amp;ocirc;i trường
Ở đ&amp;acirc;y &amp;ldquo;m&amp;ocirc;i trường&amp;rdquo; ch&amp;iacute;nh l&amp;agrave; Internet, l&amp;agrave; c&amp;aacute;c thiết bị li&amp;ecirc;n lạc kh&amp;ocirc;ng d&amp;acirc;y, c&amp;aacute;c thiết bị
t&amp;iacute;nh to&amp;aacute;n v&amp;agrave; lưu trữ th&amp;ocirc;ng tin, l&amp;agrave; điện to&amp;aacute;n đ&amp;aacute;m m&amp;acirc;y...
AI cũng gắn kết với lĩnh vực robot, l&amp;agrave;m cho c&amp;aacute;c robot xưa vốn chỉ biết hoạt động
theo một chương tr&amp;igrave;nh cố định th&amp;igrave; nay đang c&amp;oacute; th&amp;ecirc;m c&amp;aacute;c chức năng của AI v&amp;agrave; trở
th&amp;agrave;nh c&amp;aacute;c robot th&amp;ocirc;ng minh.
Tuy hầu hết mọi người đều c&amp;oacute; những h&amp;igrave;nh dung về AI qua c&amp;aacute;c chức năng AI kể tr&amp;ecirc;n,
&amp;iacute;t người ngo&amp;agrave;i ng&amp;agrave;nh biết c&amp;aacute;c rằng c&amp;aacute;c tiến bộ đột ph&amp;aacute; của AI lại chủ yếu dựa tr&amp;ecirc;n
lĩnh vực học m&amp;aacute;y (machine learning).
Học m&amp;aacute;y l&amp;agrave; một nh&amp;aacute;nh của AI với mục ti&amp;ecirc;u l&amp;agrave;m cho m&amp;aacute;y c&amp;oacute; những khả năng học tập
của con người. Đ&amp;iacute;ch của việc học của cả người v&amp;agrave; m&amp;aacute;y l&amp;agrave; để c&amp;oacute; th&amp;ecirc;m kiến thức v&amp;agrave;
hiểu biết mới. Điều đặc biệt l&amp;agrave; việc học của m&amp;aacute;y được thực hiện qua sự ph&amp;acirc;n t&amp;iacute;ch
c&amp;aacute;c nguồn dữ liệu để t&amp;igrave;m ra c&amp;aacute;c kiến thức v&amp;agrave; hiểu biết con người cần.
Học m&amp;aacute;y đ&amp;atilde; ph&amp;aacute;t triển chừng nửa thế kỷ. Khi nhận ra những kh&amp;oacute; khăn bản chất về
việc tạo tri thức cho c&amp;aacute;c hệ AI, cũng l&amp;agrave; l&amp;uacute;c con người nhận ra học m&amp;aacute;y ch&amp;iacute;nh l&amp;agrave; con
đường để c&amp;oacute; thể vượt qua kh&amp;oacute; khăn n&amp;agrave;y.
Sở dĩ vậy v&amp;igrave; tri thức đưa v&amp;agrave;o c&amp;aacute;c hệ AI đều
phải l&amp;agrave; tri thức tường minh, diễn giải r&amp;otilde;
r&amp;agrave;ng. Tuy nhi&amp;ecirc;n, chỉ một phần tri thức của
con người l&amp;agrave; tường minh c&amp;ograve;n phần lớn l&amp;agrave;
kinh nghiệm, l&amp;agrave; trực cảm, l&amp;agrave; những hiểu
biết tiềm ẩn chỉ ph&amp;aacute;t lộ v&amp;agrave; kết nối khi họ
suy nghĩ để đưa ra c&amp;aacute;c quyết định h&amp;agrave;nh
động. Ch&amp;iacute;nh c&amp;aacute;c tri thức ẩn n&amp;agrave;y được lưu
trong dữ liệu do đo đạc, quan s&amp;aacute;t từ h&amp;agrave;nh
động của của con người (như c&amp;aacute;c bệnh &amp;aacute;n
điện tử ẩn chứa rất nhiều tri thức v&amp;agrave; kinh
nghiệm của b&amp;aacute;c sĩ). Mục ti&amp;ecirc;u của học m&amp;aacute;y Ba th&amp;agrave;nh   phần    ch&amp;iacute;nh   của khoa    học dữ liệu
ch&amp;iacute;nh l&amp;agrave; l&amp;agrave;m sao
ph&amp;aacute;t hiện ra c&amp;aacute;c tri
thức tiềm ẩn n&amp;agrave;y
từ dữ liệu v&amp;agrave;
chuyển ch&amp;uacute;ng
th&amp;agrave;nh dạng tường
minh.
Thống k&amp;ecirc; to&amp;aacute;n học
đ&amp;atilde; ph&amp;aacute;t triển hơn
bốn trăm năm qua
v&amp;agrave; cũng l&amp;agrave; khoa
học nhằm tạo ra,
ph&amp;acirc;n t&amp;iacute;ch v&amp;agrave; đưa
ra kết luận từ c&amp;aacute;c
bảng dữ liệu, rất
l&amp;acirc;u trước khi c&amp;oacute; ph&amp;acirc;n t&amp;iacute;ch thống k&amp;ecirc; với m&amp;aacute;y t&amp;iacute;nh. Tuy phương ph&amp;aacute;p ban đầu kh&amp;aacute;c
nhau, những m&amp;ocirc; h&amp;igrave;nh to&amp;aacute;n học chặt chẽ của thống k&amp;ecirc; đ&amp;atilde; v&amp;agrave; đang trở th&amp;agrave;nh nền tảng
cho học m&amp;aacute;y. Thống k&amp;ecirc; v&amp;agrave; học m&amp;aacute;y x&amp;iacute;ch lại gần nhau trong khoảng hơn hai chục
năm qua, bổ sung cho nhau, tạo n&amp;ecirc;n thay đổi lớn cho cả hai lĩnh vực. Sự kết hợp
n&amp;agrave;y v&amp;agrave; sức mạnh của ch&amp;uacute;ng đ&amp;atilde; h&amp;igrave;nh th&amp;agrave;nh v&amp;agrave; khẳng định một khoa học thiết yếu
của thời chuyển đổi số: Khoa học dữ liệu. Đ&amp;acirc;y l&amp;agrave; khoa học về việc tạo ra, quản l&amp;yacute;,
ph&amp;acirc;n t&amp;iacute;ch v&amp;agrave; khai th&amp;aacute;c dữ liệu. Dữ liệu l&amp;agrave; t&amp;agrave;i nguy&amp;ecirc;n ch&amp;iacute;nh của ph&amp;aacute;t triển trong thời
hiện đại, v&amp;agrave; do vậy khoa học dữ liệu c&amp;oacute; một gi&amp;aacute; trị to lớn n&amp;oacute;i chung, v&amp;agrave; c&amp;oacute; thể xem
khoa học dữ liệu l&amp;agrave; một th&amp;agrave;nh phần của AI theo nghĩa rộng.
Khoa    học dữ liệu:    Từ dữ liệu  đến quyết   định    v&amp;agrave;  h&amp;agrave;nh    động
Chiến   lược    về AI   của một số quốc gia (https://medium.com/politics-ai/an-overview-of-national-aistrategies-2a70ec6edfd)
Hầu hết c&amp;aacute;c lĩnh vực của AI như nhận dạng ảnh, hiểu ng&amp;ocirc;n ngữ, dịch tự động, nhận
dạng tiếng n&amp;oacute;i&amp;hellip; trong nhiều năm qua đều ph&amp;aacute;t triển dựa tr&amp;ecirc;n khai th&amp;aacute;c dữ liệu về
ảnh, ng&amp;ocirc;n ngữ, tiếng n&amp;oacute;i với c&amp;aacute;c phương ph&amp;aacute;p của học m&amp;aacute;y. Lời giải học m&amp;aacute;y dẫn
dắt bởi dữ liệu (data-driven) hiện l&amp;agrave; phương ph&amp;aacute;p phổ biến của AI v&amp;agrave; nhờ vậy đang
tạo n&amp;ecirc;n c&amp;aacute;c đột ph&amp;aacute; của lĩnh vực n&amp;agrave;y.
Nhận thức về thời chuyển đổi số v&amp;agrave; sức mạnh của AI, gần đ&amp;acirc;y rất nhiều quốc gia, cả
c&amp;aacute;c nước ph&amp;aacute;t triển như Mỹ, Ph&amp;aacute;p, Đức, Anh, Canada, Nhật, H&amp;agrave;n quốc, Trung
Quốc&amp;hellip; v&amp;agrave; nhiều nước &amp;iacute;t ph&amp;aacute;t triển hơn như Kenya, Mexico, Tunisia, Malaysia&amp;hellip; đều
x&amp;acirc;y dựng chiến lược quốc gia về AI của m&amp;igrave;nh. Dưới c&amp;aacute;i &amp;ocirc; rất rộng của AI, mỗi quốc
gia kể tr&amp;ecirc;n đều định ra những nội dung m&amp;igrave;nh cần l&amp;agrave;m v&amp;agrave; con đường đạt c&amp;aacute;c đ&amp;iacute;ch đ&amp;oacute;.
Th&amp;iacute; dụ như nước Ph&amp;aacute;p đ&amp;atilde; d&amp;agrave;nh 1,5 tỷ Euro cho chiến lược n&amp;agrave;y trong 5 năm với một
kế hoạch được giao cho nh&amp;agrave; to&amp;aacute;n học C&amp;eacute;dric Villani chủ tr&amp;igrave;, người c&amp;ugrave;ng nhận giải
thưởng Fields với gi&amp;aacute;o sư Ng&amp;ocirc; Bảo Ch&amp;acirc;u. Kinh ph&amp;iacute; n&amp;agrave;y được sơ bộ d&amp;agrave;nh 700 triệu
Euro cho nghi&amp;ecirc;n cứu AI, 100 triệu cho c&amp;aacute;c c&amp;ocirc;ng ty v&amp;agrave; khởi nghiệp trong năm đầu, 70
triệu cho mỗi năm cho Ng&amp;acirc;n h&amp;agrave;ng đầu tư của Ph&amp;aacute;p, 400 triệu cho c&amp;aacute;c đề &amp;aacute;n AI của
c&amp;ocirc;ng nghiệp.
Việt Nam c&amp;oacute; cần chiến lược quốc gia về AI kh&amp;ocirc;ng? V&amp;agrave; nếu c&amp;oacute; th&amp;igrave; n&amp;ecirc;n l&amp;agrave; thế n&amp;agrave;o?
Đ&amp;acirc;y l&amp;agrave; những c&amp;acirc;u hỏi lớn, v&amp;agrave; b&amp;agrave;i b&amp;aacute;o n&amp;agrave;y chỉ đưa ra một số &amp;yacute; kiến ban đầu về một
c&amp;acirc;u hỏi li&amp;ecirc;n quan v&amp;agrave; đơn giản hơn: AI n&amp;agrave;o cho Việt Nam?
Tổng    thống   Ph&amp;aacute;p    Macron  tuy&amp;ecirc;n   bố tại  hội nghị AI phục    vụ cho  nh&amp;acirc;n    loại    (29.3.2018)
Ở đ&amp;acirc;y ta n&amp;oacute;i đến c&amp;aacute;c ứng dụng của AI, v&amp;agrave; c&amp;aacute;c ứng dụng n&amp;agrave;y cần dựa tr&amp;ecirc;n việc l&amp;agrave;m
chủ được c&amp;aacute;c kỹ thuật, c&amp;ocirc;ng nghệ AI đ&amp;atilde; được t&amp;igrave;m ra tr&amp;ecirc;n thế giới cũng như nghi&amp;ecirc;n
cứu c&amp;aacute;c kỹ thuật AI đặc th&amp;ugrave; cho Việt Nam (như hiểu tiếng Việt).
Truyền th&amp;ocirc;ng thường giới thiệu AI với c&amp;ocirc;ng ch&amp;uacute;ng qua c&amp;aacute;c robot. C&amp;aacute;c robot th&amp;ocirc;ng
minh t&amp;iacute;ch hợp c&amp;aacute;c kỹ thuật từ nhiều lĩnh vực của AI, từ nghe nh&amp;igrave;n đến hiểu tiếng n&amp;oacute;i,
biết trả lời c&amp;aacute;c c&amp;acirc;u hỏi, biết suy luận, thể hiện biểu cảm tr&amp;ecirc;n mặt&amp;hellip; Ta thường thấy
c&amp;aacute;c video về robot phục vụ người gi&amp;agrave;, bưng b&amp;ecirc; c&amp;agrave;-ph&amp;ecirc; ở qu&amp;aacute;n, nhặt r&amp;aacute;c tr&amp;ecirc;n phố&amp;hellip;
Ng&amp;agrave;y 13 th&amp;aacute;ng 7 vừa qua, nhiều người Việt đ&amp;atilde; tận mắt thấy robot Sophia biểu diễn
ở H&amp;agrave; Nội.
Một ứng dụng kh&amp;aacute;c của AI được n&amp;oacute;i nhiều gần đ&amp;acirc;y, l&amp;agrave; c&amp;aacute;c &amp;ocirc;t&amp;ocirc; tự l&amp;aacute;i. C&amp;aacute;c &amp;ocirc;t&amp;ocirc; tự l&amp;aacute;i
bắt đầu đi tr&amp;ecirc;n đường, kết quả của rất nhiều năm &amp;ldquo;học&amp;rdquo; tự l&amp;aacute;i tr&amp;ecirc;n sa mạc, khởi
nguồn từ c&amp;aacute;c cuộc thi l&amp;agrave;m robot đ&amp;aacute; b&amp;oacute;ng mấy chục năm qua (Robotcup), rồi c&amp;aacute;c
cuộc thi mang t&amp;ecirc;n &amp;ldquo;th&amp;aacute;ch thức lớn của DARPA&amp;rdquo; từ 2004 (DARPA l&amp;agrave; viết tắt của cơ
quan chỉ đạo c&amp;aacute;c dự &amp;aacute;n nghi&amp;ecirc;n cứu quốc ph&amp;ograve;ng ti&amp;ecirc;n tiến của Mỹ). Cuộc thi cũng
hướng đến s&amp;aacute;ng tạo c&amp;aacute;c c&amp;ocirc;ng nghệ t&amp;iacute;ch hợp từ thị gi&amp;aacute;c m&amp;aacute;y, robot, lập kế hoạch tự
động, học m&amp;aacute;y, lập luận&amp;hellip; để &amp;ocirc;t&amp;ocirc; c&amp;oacute; thể tự chạy an to&amp;agrave;n.
C&amp;aacute;c robot th&amp;ocirc;ng minh, &amp;ocirc;t&amp;ocirc; tự l&amp;aacute;i&amp;hellip; kể tr&amp;ecirc;n l&amp;agrave; đỉnh cao của ứng dụng kỹ thuật AI v&amp;agrave;o
c&amp;aacute;c hệ thống tự động, một trong c&amp;aacute;c đ&amp;iacute;ch h&amp;agrave;ng đầu của AI tại c&amp;aacute;c nước c&amp;ocirc;ng nghiệp
c&amp;oacute; nền kinh tế v&amp;agrave; khoa học ph&amp;aacute;t triển.
Trong chừng mực n&amp;agrave;o đấy AI trong mắt đa số người Việt phần lớn l&amp;agrave; c&amp;aacute;c hệ tự động
th&amp;ocirc;ng minh, l&amp;agrave; robot. Rất nhiều bạn trẻ băn khoăn về một ng&amp;agrave;y c&amp;aacute;c robot sẽ thống trị
con người v&amp;agrave; rất th&amp;iacute;ch th&amp;uacute; với robot Sophia. Tuy nhi&amp;ecirc;n, nhiều người l&amp;agrave;m AI nhận
định rằng trong sự kiện ồn &amp;agrave;o vừa qua về robot n&amp;agrave;y, Sophia kh&amp;aacute; về biểu cảm tr&amp;ecirc;n
mặt, nhưng c&amp;aacute;c ph&amp;aacute;t biểu ch&amp;agrave;o đ&amp;oacute;n, c&amp;aacute;c c&amp;acirc;u hỏi-đ&amp;aacute;p&amp;hellip; đều mới ứng dụng c&amp;aacute;c kỹ
thuật AI đơn giản, l&amp;agrave;m sẵn v&amp;agrave; ở mức quảng c&amp;aacute;o nhưng chưa phải &amp;ldquo;AI thật&amp;rdquo;.
Vậy AI n&amp;agrave;o cho Việt Nam?
C&amp;acirc;u trả lời chung l&amp;agrave; ta cần những AI gắn với c&amp;aacute;c mục ti&amp;ecirc;u ph&amp;aacute;t triển của đất nước.
Đấy l&amp;agrave; AI cho một ch&amp;iacute;nh phủ điện tử hiệu quả, l&amp;agrave; AI cho n&amp;ocirc;ng nghiệp th&amp;agrave;nh th&amp;ocirc;ng
minh, l&amp;agrave; AI cho du lịch th&amp;agrave;nh một ng&amp;agrave;nh dịch vụ th&amp;ocirc;ng minh, l&amp;agrave; AI để giao th&amp;ocirc;ng an
to&amp;agrave;n v&amp;agrave; nhanh hơn, l&amp;agrave; AI cho chăm s&amp;oacute;c sức khoẻ người d&amp;acirc;n tốt hơn&amp;hellip; Những AI n&amp;agrave;y
đang cần được nh&amp;igrave;n nhận v&amp;agrave; ưu ti&amp;ecirc;n ph&amp;aacute;t triển hơn c&amp;aacute;c hệ tự động th&amp;ocirc;ng minh, vốn
rất quan trọng với c&amp;aacute;c nước c&amp;oacute; nền c&amp;ocirc;ng nghiệp ph&amp;aacute;t triển nhưng chưa khẩn thiết
cho một quốc gia chưa lấy c&amp;ocirc;ng nghiệp l&amp;agrave;m trọng t&amp;acirc;m ph&amp;aacute;t triển như Việt Nam.
Ở tr&amp;ecirc;n đ&amp;atilde; b&amp;agrave;n về vai tr&amp;ograve; cơ bản của học m&amp;aacute;y, v&amp;agrave; ta c&amp;oacute; thể n&amp;oacute;i AI sẽ ứng dụng được
v&amp;agrave;o mọi lĩnh vực vừa kể tr&amp;ecirc;n nếu ở đ&amp;oacute; cần d&amp;ugrave;ng dữ liệu. Những lĩnh vực ưu ti&amp;ecirc;n
ph&amp;aacute;t triển của đất nước như n&amp;ocirc;ng nghiệp, du lịch, giao th&amp;ocirc;ng, m&amp;ocirc;i trường, t&amp;agrave;i ch&amp;iacute;nh,
y tế&amp;hellip; đều c&amp;oacute; rất nhiều dữ liệu v&amp;agrave; đều l&amp;agrave; những mảnh đất quan trọng để tr&amp;ecirc;n đ&amp;oacute; đưa
v&amp;agrave;o giải ph&amp;aacute;p AI. Khi t&amp;igrave;m giải ph&amp;aacute;p AI cho c&amp;aacute;c lĩnh vực n&amp;agrave;y, c&amp;aacute;c kỹ thuật AI cơ bản
sẽ được sử dụng, như hiểu ng&amp;ocirc;n ngữ d&amp;ugrave;ng cho xử l&amp;yacute; tự động c&amp;aacute;c văn bản, nhận
dạng tiếng n&amp;oacute;i d&amp;ugrave;ng nhập liệu v&amp;agrave; giao tiếp người m&amp;aacute;y&amp;hellip;
Từ đ&amp;acirc;y c&amp;oacute; thể nhận định rằng trong ho&amp;agrave;n cảnh cụ thể của Việt Nam, l&amp;uacute;c n&amp;agrave;y c&amp;aacute;i AI ta
cần nhiều hơn l&amp;agrave; AI dựa tr&amp;ecirc;n khai th&amp;aacute;c dữ liệu trong c&amp;aacute;c lĩnh vực kinh tế v&amp;agrave; x&amp;atilde; hội.
Chẳng hạn đ&amp;oacute; l&amp;agrave; AI để tổ chức c&amp;aacute;c kho văn bản của ch&amp;iacute;nh phủ, c&amp;aacute;c tỉnh th&amp;agrave;nh, c&amp;aacute;c
bộ ng&amp;agrave;nh&amp;hellip; để c&amp;oacute; thể như t&amp;igrave;m ra rất nhanh c&amp;aacute;c văn bản li&amp;ecirc;n quan đến một quyết
định quan trọng, hoặc tr&amp;iacute;ch r&amp;uacute;t ra c&amp;aacute;c điều khoản từ c&amp;aacute;c văn bản đ&amp;oacute; cho một nhu cầu
n&amp;agrave;o đ&amp;oacute;&amp;hellip;
Chẳng hạn đ&amp;oacute; l&amp;agrave; AI cho n&amp;ocirc;ng nghiệp th&amp;ocirc;ng minh khi đ&amp;aacute;nh gi&amp;aacute; được thị trường, đ&amp;aacute;nh
gi&amp;aacute; được cung cầu trong v&amp;agrave; ngo&amp;agrave;i nước về c&amp;aacute;c loại sản phẩm để c&amp;oacute; thể quy hoạch
sản xuất, để tr&amp;aacute;nh được m&amp;ugrave;a mất gi&amp;aacute; được gi&amp;aacute; mất m&amp;ugrave;a&amp;hellip; b&amp;ecirc;n cạnh c&amp;aacute;c trang trại
th&amp;ocirc;ng minh như số đ&amp;ocirc;ng vẫn đang hướng đến.
Chẳng hạn đ&amp;oacute; l&amp;agrave; AI để c&amp;oacute; thể ph&amp;aacute;t hiện c&amp;aacute;c bất thường trong chi ti&amp;ecirc;u tiền bạc v&amp;agrave; sử
dụng t&amp;agrave;i sản nh&amp;agrave; nước ngay khi những điều n&amp;agrave;y đang xảy ra.
Chẳng hạn đ&amp;oacute; l&amp;agrave; AI để c&amp;aacute;c doanh nghiệp vừa v&amp;agrave; nhỏ c&amp;oacute; thể l&amp;agrave;m ph&amp;acirc;n t&amp;iacute;ch kinh
doanh khi x&amp;acirc;y dựng c&amp;aacute;c kế hoạch v&amp;agrave; đưa ra c&amp;aacute;c quyết định trong sản xuất.
Trong c&amp;aacute;c th&amp;iacute; dụ tr&amp;ecirc;n, điều quan trọng l&amp;agrave; ch&amp;uacute;ng ta phải x&amp;acirc;y dựng được c&amp;aacute;c cơ sở dữ
liệu của quốc gia, địa phương, doanh nghiệp v&amp;agrave; khai th&amp;aacute;c được ch&amp;uacute;ng. Đ&amp;acirc;y ch&amp;iacute;nh l&amp;agrave;
hạ tầng số thiết yếu của AI cho Việt Nam.
Tất nhi&amp;ecirc;n ta cần đ&amp;agrave;o tạo ra lực lượng tinh nhuệ l&amp;agrave;m AI, cần nối kết được lực lượng
AI trong v&amp;agrave; ngo&amp;agrave;i nước, v&amp;agrave; cần một chiến lược AI của Việt Nam.
Trước khi đến một chiến lược AI của đất nước, nh&amp;igrave;n r&amp;otilde; những AI n&amp;agrave;o ta đang cần v&amp;agrave;
ưu ti&amp;ecirc;n l&amp;agrave; một điều cần thiết.
Hồ T&amp;uacute; Bảo
Viện John von Neumann, Đại học Quốc gia th&amp;agrave;nh phố Hồ Ch&amp;iacute; Minh
Viện Nghi&amp;ecirc;n cứu Cao cấp về To&amp;aacute;n&lt;/p&gt;</content><category term="OTHER"></category></entry><entry><title>Face Detector Report</title><link href="/blog/paper/2018/face-detector-report/" rel="alternate"></link><published>2018-08-23T08:26:00+00:00</published><updated>2018-08-23T08:26:00+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-23:/blog/paper/2018/face-detector-report/</id><summary type="html">&lt;h1 id="dataset"&gt;Dataset&lt;/h1&gt;
&lt;p&gt;http://www.face-rec.org/databases/&lt;/p&gt;
&lt;h1 id="timer-and-speed"&gt;Timer and speed&lt;/h1&gt;
&lt;p&gt;https://github.com/ShuangXieIrene/ssds.pytorch/blob/master/lib/utils/timer.py
https://github.com/ShuangXieIrene/ssds.pytorch/blob/master/lib/ssds.py&lt;/p&gt;
&lt;h1 id="write-clean-code-like"&gt;Write clean code like&lt;/h1&gt;
&lt;p&gt;https://github.com/DavexPro/pytorch-pose-estimation/blob/master/pose_estimation.py&lt;/p&gt;
&lt;h1 id="the-incredible-pytorch"&gt;The incredible pytorch&lt;/h1&gt;
&lt;p&gt;https://www.ritchieng.com/the-incredible-pytorch/
Face problems
https://www.adrianbulat.com/
FERA 2017 - Addressing Head Pose in the Third Facial Expression
Recognition and Analysis Challenge - 2017 Dataset
https://arxiv.org/pdf/1702.04174.pdf&lt;/p&gt;
&lt;p&gt;consecutively &lt;/p&gt;
&lt;p&gt;competitive results over the compared baseline methods.&lt;/p&gt;
&lt;p&gt;earlier works/ efforts
the subsequent layer
these models usually heavily relied on
laborious feature engineering&lt;/p&gt;
&lt;p&gt;Recent advances in deep neural networks and
representation learning have substantially improved
the performance of text classification tasks.
The dominant approaches are recurrent neural net&lt;/p&gt;
&lt;h1 id="how-to-train"&gt;How to train&lt;/h1&gt;
&lt;p&gt;https://www.youtube.com/watch?v=Rgpfk6eYxJA&lt;/p&gt;
&lt;h2 id="combined-result"&gt;Combined Result&lt;/h2&gt;
&lt;h3 id="methods"&gt;Methods&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;SSH&lt;/strong&gt;: Like HR, a four level image …&lt;/p&gt;</summary><content type="html">&lt;h1 id="dataset"&gt;Dataset&lt;/h1&gt;
&lt;p&gt;http://www.face-rec.org/databases/&lt;/p&gt;
&lt;h1 id="timer-and-speed"&gt;Timer and speed&lt;/h1&gt;
&lt;p&gt;https://github.com/ShuangXieIrene/ssds.pytorch/blob/master/lib/utils/timer.py
https://github.com/ShuangXieIrene/ssds.pytorch/blob/master/lib/ssds.py&lt;/p&gt;
&lt;h1 id="write-clean-code-like"&gt;Write clean code like&lt;/h1&gt;
&lt;p&gt;https://github.com/DavexPro/pytorch-pose-estimation/blob/master/pose_estimation.py&lt;/p&gt;
&lt;h1 id="the-incredible-pytorch"&gt;The incredible pytorch&lt;/h1&gt;
&lt;p&gt;https://www.ritchieng.com/the-incredible-pytorch/
Face problems
https://www.adrianbulat.com/
FERA 2017 - Addressing Head Pose in the Third Facial Expression
Recognition and Analysis Challenge - 2017 Dataset
https://arxiv.org/pdf/1702.04174.pdf&lt;/p&gt;
&lt;p&gt;consecutively &lt;/p&gt;
&lt;p&gt;competitive results over the compared baseline methods.&lt;/p&gt;
&lt;p&gt;earlier works/ efforts
the subsequent layer
these models usually heavily relied on
laborious feature engineering&lt;/p&gt;
&lt;p&gt;Recent advances in deep neural networks and
representation learning have substantially improved
the performance of text classification tasks.
The dominant approaches are recurrent neural net&lt;/p&gt;
&lt;h1 id="how-to-train"&gt;How to train&lt;/h1&gt;
&lt;p&gt;https://www.youtube.com/watch?v=Rgpfk6eYxJA&lt;/p&gt;
&lt;h2 id="combined-result"&gt;Combined Result&lt;/h2&gt;
&lt;h3 id="methods"&gt;Methods&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;SSH&lt;/strong&gt;: Like HR, a four level image pyramid is deployed. To form the pyramid, the image is first scaled
to have a shortest side of up to 800 pixels and the longest side less than 1200 pixels. Then, we scale the image to have min sizes of 500, 800, 1200, and 1600 pixels in the pyramid. All modules detect faces on all pyramid levels, except M3
which is not applied to the largest level.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;th&gt;FDDB&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Scale Face Val&lt;/td&gt;
&lt;td&gt;86.8&lt;/td&gt;
&lt;td&gt;86.7&lt;/td&gt;
&lt;td&gt;77.2&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;96.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSH (Pyramid) Val&lt;/td&gt;
&lt;td&gt;93.1&lt;/td&gt;
&lt;td&gt;92.1&lt;/td&gt;
&lt;td&gt;84.5&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;98.27&lt;/td&gt;
&lt;td&gt;98.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HR Val(Tiny Face Detector)&lt;/td&gt;
&lt;td&gt;91.9&lt;/td&gt;
&lt;td&gt;90.8&lt;/td&gt;
&lt;td&gt;82.3&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-CNN Val&lt;/td&gt;
&lt;td&gt;93.8&lt;/td&gt;
&lt;td&gt;92.2&lt;/td&gt;
&lt;td&gt;82.9&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-FCN Val&lt;/td&gt;
&lt;td&gt;94.7&lt;/td&gt;
&lt;td&gt;93.5&lt;/td&gt;
&lt;td&gt;87.4&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Seeing Small Faces Val&lt;/td&gt;
&lt;td&gt;94.9&lt;/td&gt;
&lt;td&gt;93.3&lt;/td&gt;
&lt;td&gt;86.1&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;99.23&lt;/td&gt;
&lt;td&gt;97.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SFD Val&lt;/td&gt;
&lt;td&gt;93.7&lt;/td&gt;
&lt;td&gt;92.4&lt;/td&gt;
&lt;td&gt;85.2&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;98.49&lt;/td&gt;
&lt;td&gt;98.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale1 (Remove tiny boxes min &amp;lt; 18)&lt;/td&gt;
&lt;td&gt;94.2&lt;/td&gt;
&lt;td&gt;92.8&lt;/td&gt;
&lt;td&gt;86.1&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OURS Val without  FPC&lt;/td&gt;
&lt;td&gt;93.7&lt;/td&gt;
&lt;td&gt;92.2&lt;/td&gt;
&lt;td&gt;84.6&lt;/td&gt;
&lt;td&gt;99.91&lt;/td&gt;
&lt;td&gt;99.28&lt;/td&gt;
&lt;td&gt;98.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Our Val with FPC&lt;/td&gt;
&lt;td&gt;94.3&lt;/td&gt;
&lt;td&gt;93.2&lt;/td&gt;
&lt;td&gt;84.8&lt;/td&gt;
&lt;td&gt;99.89&lt;/td&gt;
&lt;td&gt;99.28&lt;/td&gt;
&lt;td&gt;98.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Retinanet with Correction&lt;/td&gt;
&lt;td&gt;95.2&lt;/td&gt;
&lt;td&gt;93.6&lt;/td&gt;
&lt;td&gt;85.9&lt;/td&gt;
&lt;td&gt;99.91&lt;/td&gt;
&lt;td&gt;99.37&lt;/td&gt;
&lt;td&gt;98.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Retinanet with Correction CNN&lt;/p&gt;
&lt;h2 id="post-processing_1"&gt;Post Processing&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;easy&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;hard&lt;/th&gt;
&lt;th&gt;AFW&lt;/th&gt;
&lt;th&gt;Pascal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Scale Face Val&lt;/td&gt;
&lt;td&gt;86.8&lt;/td&gt;
&lt;td&gt;86.7&lt;/td&gt;
&lt;td&gt;77.2&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSH (Pyramid) Val&lt;/td&gt;
&lt;td&gt;93.1&lt;/td&gt;
&lt;td&gt;92.1&lt;/td&gt;
&lt;td&gt;84.5&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;98.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HR Val&lt;/td&gt;
&lt;td&gt;91.9&lt;/td&gt;
&lt;td&gt;90.8&lt;/td&gt;
&lt;td&gt;82.3&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-CNN Val&lt;/td&gt;
&lt;td&gt;93.8&lt;/td&gt;
&lt;td&gt;92.2&lt;/td&gt;
&lt;td&gt;82.9&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face R-FCN Val&lt;/td&gt;
&lt;td&gt;94.7&lt;/td&gt;
&lt;td&gt;93.5&lt;/td&gt;
&lt;td&gt;87.4&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Seeing Small Faces Val&lt;/td&gt;
&lt;td&gt;94.9&lt;/td&gt;
&lt;td&gt;93.3&lt;/td&gt;
&lt;td&gt;86.1&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;99.23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SFD Val&lt;/td&gt;
&lt;td&gt;93.7&lt;/td&gt;
&lt;td&gt;92.4&lt;/td&gt;
&lt;td&gt;85.2&lt;/td&gt;
&lt;td&gt;99.85&lt;/td&gt;
&lt;td&gt;98.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OURS Val&lt;/td&gt;
&lt;td&gt;93.7&lt;/td&gt;
&lt;td&gt;92.2&lt;/td&gt;
&lt;td&gt;84.6&lt;/td&gt;
&lt;td&gt;99.89&lt;/td&gt;
&lt;td&gt;99.37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OURS Val&lt;/td&gt;
&lt;td&gt;93.7&lt;/td&gt;
&lt;td&gt;92.2&lt;/td&gt;
&lt;td&gt;84.6&lt;/td&gt;
&lt;td&gt;99.91&lt;/td&gt;
&lt;td&gt;99.37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale1 (Remove tiny boxes min &amp;lt; 18)&lt;/td&gt;
&lt;td&gt;94.2%&lt;/td&gt;
&lt;td&gt;92.8%&lt;/td&gt;
&lt;td&gt;53.4%&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;td&gt;_&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;remove_low_iou &amp;lt; 0.5 (a)&lt;/td&gt;
&lt;td&gt;81.9%&lt;/td&gt;
&lt;td&gt;86.1%&lt;/td&gt;
&lt;td&gt;82.3%&lt;/td&gt;
&lt;td&gt;99.89&lt;/td&gt;
&lt;td&gt;99.37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;remove_low_iou &amp;lt; 0.5 (b)&lt;/td&gt;
&lt;td&gt;81.8%&lt;/td&gt;
&lt;td&gt;86.0%&lt;/td&gt;
&lt;td&gt;82.1%&lt;/td&gt;
&lt;td&gt;99.89&lt;/td&gt;
&lt;td&gt;99.37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;remove_low_iou &amp;lt; 0.5 (c)&lt;/td&gt;
&lt;td&gt;81.7%&lt;/td&gt;
&lt;td&gt;86.0%&lt;/td&gt;
&lt;td&gt;82.3%&lt;/td&gt;
&lt;td&gt;99.89&lt;/td&gt;
&lt;td&gt;99.37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;remove_low_iou &amp;lt; 0.5 (d)&lt;/td&gt;
&lt;td&gt;_%&lt;/td&gt;
&lt;td&gt;_%&lt;/td&gt;
&lt;td&gt;_%&lt;/td&gt;
&lt;td&gt;99.89&lt;/td&gt;
&lt;td&gt;99.37&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;(a) set iou &amp;gt; 0.5 = 1, iou &amp;lt; 0.5 = np.sqrt(01.*score)
(b) set iou &amp;gt; 0.5 = 1, iou &amp;lt; 0.5 = 0.0
(c) set iou &amp;gt; 0.5 = 1, iou &amp;lt; 0.5 = score
(d) set iou &amp;gt; 0.5 = score, iou &amp;lt; 0.5 = 0.1&lt;/p&gt;
&lt;p&gt;(b) Set iou &amp;gt; 0.5 -&amp;gt; np.sqrt(0.99&lt;em&gt;score), iou &amp;lt; 0.5 -&amp;gt; np.sqrt(0.01&lt;/em&gt;score)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Num_layers&lt;/th&gt;
&lt;th&gt;Notes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Resnet101&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Loss ratio (4:1), IoU Thresholds: 0.3, 0.1, topN = 5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resnet101_New&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Loss ratio (4:1), IoU Thresholds: 0.5, 0.3, topN = 4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resnet101_Newer&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Loss ratio (4:1), IoU Thresholds: 0.5, 0.3, topN = 4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resnet101_Neweest&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Loss ratio (4:1), IoU Thresholds: 0.5, 0.1, topN = 4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="current-work_1"&gt;Current work&lt;/h1&gt;
&lt;p&gt;ssh amazon_server; chạy predict_Resnet101_New, with 2 layers, &lt;/p&gt;
&lt;h3 id="table-of-deepresnet"&gt;Table of deepresnet&lt;/h3&gt;
&lt;p&gt;Ket qua quen khong ghi chep? Maybe model &lt;code&gt;Resnet101/best_val_model.pkl&lt;/code&gt;
Predict on VAL with Resnet101/best_val_model.pkl (num_layers=4); 
File     | Easy  | Medium | Hard
---------|-------|--------|-------
min_500  | 89.9% | 87.4%  | 64.2%
min_640  | 91.1% | 88.1%  | 75.0%
min_800  | 91.0% | 89.1%  | 79.6%
min_1200 | 89.9% | 88.8%  | 82.4%
min_1600 | 89.9% | 87.4%  | 81.7%
original | 92.1% | 89.6%  | 78.6%&lt;/p&gt;
&lt;h1 id="what-im-doing-now_1"&gt;What I'm doing now?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Prediction on Wider_val, test with old method: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The result saved at &lt;code&gt;Dropbox/Resnet101&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The screen session: &lt;code&gt;wf_prediction_final&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;On &lt;code&gt;dais&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Used file &lt;code&gt;widerface_pred/wider_benchmark.sh&lt;/code&gt;, &lt;code&gt;widerface_pred/benchmark_eval.py&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prediction on Wider_val, test with &lt;code&gt;new&lt;/code&gt; method: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The result saved at &lt;code&gt;Dropbox/WIDER_test&lt;/code&gt;, &lt;code&gt;Dropbox/WIDER_val&lt;/code&gt;, &lt;/li&gt;
&lt;li&gt;The screen session: &lt;code&gt;wf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use file &lt;code&gt;prediction/wf_prediction_multi_scale.sh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_scale = (1980 * 1980/ (w * h)) ** (0.5)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;On &lt;code&gt;dais&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prediction on Wider_val with :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On &lt;code&gt;amazon_server&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use file &lt;code&gt;prediction/wf_prediction_multi_scale.sh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_scale = (2400 * 2400/ (w * h)) ** (0.5)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="table-of-results-resnet101-layers"&gt;Table of results Resnet101 layers&lt;/h3&gt;
&lt;p&gt;Update on 25-Sept
Predict on VAL with Resnet101/best_val_loss_model.pkl (num_layers=4); 
score_thresh=0.1;
nms_thresh=0.3;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;File&lt;/th&gt;
&lt;th&gt;Easy&lt;/th&gt;
&lt;th&gt;Medium&lt;/th&gt;
&lt;th&gt;Hard&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;min_500&lt;/td&gt;
&lt;td&gt;87.6%&lt;/td&gt;
&lt;td&gt;82.6%&lt;/td&gt;
&lt;td&gt;65.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_640&lt;/td&gt;
&lt;td&gt;91.5%&lt;/td&gt;
&lt;td&gt;88.6%&lt;/td&gt;
&lt;td&gt;76.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_800&lt;/td&gt;
&lt;td&gt;91.5%&lt;/td&gt;
&lt;td&gt;89.7%&lt;/td&gt;
&lt;td&gt;80.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_1200&lt;/td&gt;
&lt;td&gt;89.9%&lt;/td&gt;
&lt;td&gt;88.8%&lt;/td&gt;
&lt;td&gt;82.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_1600&lt;/td&gt;
&lt;td&gt;89.9%&lt;/td&gt;
&lt;td&gt;87.9%&lt;/td&gt;
&lt;td&gt;82.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;original&lt;/td&gt;
&lt;td&gt;92.5%&lt;/td&gt;
&lt;td&gt;90.1%&lt;/td&gt;
&lt;td&gt;79.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale0&lt;/td&gt;
&lt;td&gt;92.7%&lt;/td&gt;
&lt;td&gt;91.3%&lt;/td&gt;
&lt;td&gt;83.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale1&lt;/td&gt;
&lt;td&gt;92.9%&lt;/td&gt;
&lt;td&gt;91.7%&lt;/td&gt;
&lt;td&gt;86.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale1 (Remove tiny boxes max &amp;lt; 8)&lt;/td&gt;
&lt;td&gt;92.9%&lt;/td&gt;
&lt;td&gt;91.7%&lt;/td&gt;
&lt;td&gt;86.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale1 (Remove tiny boxes max &amp;lt; 10)&lt;/td&gt;
&lt;td&gt;93.1%&lt;/td&gt;
&lt;td&gt;91.9%&lt;/td&gt;
&lt;td&gt;86.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale1 (Remove tiny boxes min &amp;lt; 10)&lt;/td&gt;
&lt;td&gt;93.3%&lt;/td&gt;
&lt;td&gt;92.2%&lt;/td&gt;
&lt;td&gt;81.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale1 (Remove tiny boxes min &amp;lt; 12)&lt;/td&gt;
&lt;td&gt;93.6%&lt;/td&gt;
&lt;td&gt;92.5%&lt;/td&gt;
&lt;td&gt;72.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale1 (Remove tiny boxes min &amp;lt; 15)&lt;/td&gt;
&lt;td&gt;93.9%&lt;/td&gt;
&lt;td&gt;92.8%&lt;/td&gt;
&lt;td&gt;62.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale1 (Remove tiny boxes min &amp;lt; 18)&lt;/td&gt;
&lt;td&gt;94.2%&lt;/td&gt;
&lt;td&gt;92.8%&lt;/td&gt;
&lt;td&gt;53.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale2&lt;/td&gt;
&lt;td&gt;92.2%&lt;/td&gt;
&lt;td&gt;91.7%&lt;/td&gt;
&lt;td&gt;86.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale0.45 (nms_thresh=0.45)&lt;/td&gt;
&lt;td&gt;92.7%&lt;/td&gt;
&lt;td&gt;91.6%&lt;/td&gt;
&lt;td&gt;85.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale3_2400x2400_0.3&lt;/td&gt;
&lt;td&gt;92.8%&lt;/td&gt;
&lt;td&gt;91.7%&lt;/td&gt;
&lt;td&gt;86.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wf_scale3_2400x2400_0.45&lt;/td&gt;
&lt;td&gt;92.7%&lt;/td&gt;
&lt;td&gt;91.6%&lt;/td&gt;
&lt;td&gt;85.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WIDER_Val_0.3 (nms_thresh=0.3)&lt;/td&gt;
&lt;td&gt;92.3%&lt;/td&gt;
&lt;td&gt;91.1%&lt;/td&gt;
&lt;td&gt;83.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WIDER_Val_0.45 (nms_thresh=0.45)&lt;/td&gt;
&lt;td&gt;92.1%&lt;/td&gt;
&lt;td&gt;90,7%&lt;/td&gt;
&lt;td&gt;83.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(1) merge500_640&lt;/td&gt;
&lt;td&gt;91.5%&lt;/td&gt;
&lt;td&gt;88.2%&lt;/td&gt;
&lt;td&gt;71.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(2) merge500_640_800_1200&lt;/td&gt;
&lt;td&gt;91.8%&lt;/td&gt;
&lt;td&gt;90.7%&lt;/td&gt;
&lt;td&gt;81.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(3) merge500_640_800_1200_1600&lt;/td&gt;
&lt;td&gt;91.8%&lt;/td&gt;
&lt;td&gt;90.4%&lt;/td&gt;
&lt;td&gt;83.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(1) mix_640_800 (filter)&lt;/td&gt;
&lt;td&gt;91.5%&lt;/td&gt;
&lt;td&gt;89.3%&lt;/td&gt;
&lt;td&gt;75.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(2) mix_640_800_...1600_original(filter)&lt;/td&gt;
&lt;td&gt;91.8%&lt;/td&gt;
&lt;td&gt;91.0%&lt;/td&gt;
&lt;td&gt;84.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(3) mix_640_800_...1600_original(filter)&lt;/td&gt;
&lt;td&gt;91.7%&lt;/td&gt;
&lt;td&gt;90.9%&lt;/td&gt;
&lt;td&gt;83.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(3) mix_640_800_...1600_original(filter)&lt;/td&gt;
&lt;td&gt;91.7%&lt;/td&gt;
&lt;td&gt;90.9%&lt;/td&gt;
&lt;td&gt;83.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(4) mix_640_800_...1600_original(filter)&lt;/td&gt;
&lt;td&gt;90.6%&lt;/td&gt;
&lt;td&gt;90.4%&lt;/td&gt;
&lt;td&gt;83.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(5) mix_640_800_...1600_original(filter)&lt;/td&gt;
&lt;td&gt;91.8%&lt;/td&gt;
&lt;td&gt;91.0%&lt;/td&gt;
&lt;td&gt;83.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(6) mix_640_800_...1600_original(filter)&lt;/td&gt;
&lt;td&gt;91.8%&lt;/td&gt;
&lt;td&gt;91.0%&lt;/td&gt;
&lt;td&gt;84.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(7) mix_640_800_...1600_original(filter)&lt;/td&gt;
&lt;td&gt;92.0%&lt;/td&gt;
&lt;td&gt;91.1%&lt;/td&gt;
&lt;td&gt;84.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;(7) mix_640_800_...1920_original(filter)&lt;/td&gt;
&lt;td&gt;91.6%&lt;/td&gt;
&lt;td&gt;90.9%&lt;/td&gt;
&lt;td&gt;84.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;h2 id="the-best-is-wf_scale1_1"&gt;The best is wf_scale1:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;wf_scale1: &lt;/li&gt;
&lt;li&gt;score_thresh=0.1, &lt;/li&gt;
&lt;li&gt;nms_thresh=0.3; &lt;/li&gt;
&lt;li&gt;scale: 0.5 -&amp;gt; 0.5 -&amp;gt; 1 -&amp;gt; 2. -&amp;gt; max_scale (including max_scale)&lt;/li&gt;
&lt;li&gt;remove tiny_faces min(w, h) &amp;lt; 15: for detection big faces&lt;/li&gt;
&lt;li&gt;keep the same for small faces.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Update on 26-Sept
&lt;em&gt; wf_scale0: score_thresh=0.1, nms_thresh=0.3; scale: 0.5 -&amp;gt; 0.5 -&amp;gt; 1 -&amp;gt; 2. -&amp;gt; max_scale (except max_scale)
&lt;/em&gt; wf_scale1: score_thresh=0.1, nms_thresh=0.3; scale: 0.5 -&amp;gt; 0.5 -&amp;gt; 1 -&amp;gt; 2. -&amp;gt; max_scale (including max_scale)&lt;br/&gt;
The code from predict_wf_multi_scale.py&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img_path&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_paths&lt;/span&gt;&lt;span class="p"&gt;[:]):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'predict image index'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;img_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"/"&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;event_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"/"&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;
    &lt;span class="n"&gt;max_scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1980&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1980&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.25&lt;/span&gt;
    &lt;span class="n"&gt;img_boxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;flip_test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;score_thresh&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                  &lt;span class="n"&gt;nms_thresh&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img_boxes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img_scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;max_scale&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_scale&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;max_scale&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;
        &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scale_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                  &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;score_thresh&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                  &lt;span class="n"&gt;nms_thresh&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;img_boxes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;img_scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The code from scale_predict function &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# shrink only to detect big faces&lt;/span&gt;
        &lt;span class="n"&gt;keep_big&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                              &lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;
        &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;keep_big&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# enlarge only to detect small faces&lt;/span&gt;
        &lt;span class="n"&gt;keep_small&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;

        &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;keep_small&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;wf_scale2: score_thresh=0.1, nms_thresh=0.3; scale: 0.25 -&amp;gt; 0.5 -&amp;gt; 1 -&amp;gt; 2. -&amp;gt; max_scale (including max_scale)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;merge (1), (2), (3) simply just concat all and then perform box_vote&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;(1) mix_640_800(filter):&lt;/li&gt;
&lt;li&gt;(2) mix_640_800_..._original(filter):  &lt;/li&gt;
&lt;li&gt;mix (1) and (2) use the script below:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# shrink only to detect big faces&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(3) mix_640_800_..._original(filter): Change the ratio &amp;gt; 2.0 and the result reduce&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# shrink only to detect big faces&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(4) mix_640_800_..._original(filter): Change the upper ratio &amp;gt; 1.0 and the result reduce.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(5) mix_640_800_..._original(filter): keep upper ratio = 1.5, np.minimum &amp;gt; 100&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# shrink only to detect big faces&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(6) mix_640_800_..._original(filter): keep upper ratio = 1.5, np.minimum &amp;gt; 100, remove all boxes np.maximum(w, h) &amp;lt; 6&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# shrink only to detect big faces&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;continue&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(6) mix_640_800_..._original(filter): The same as mix (1), mix(2) but add iou_thresh=0.45 for box_vote()&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;img_name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;img_names&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;box_vote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;merged_boxes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="n"&gt;merged_scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="n"&gt;iou_thresh&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.45&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="table-of-results-resnet101_new-2-layers"&gt;Table of results Resnet101_New 2 layers&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;File&lt;/th&gt;
&lt;th&gt;Easy&lt;/th&gt;
&lt;th&gt;Medium&lt;/th&gt;
&lt;th&gt;Hard&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;min_500&lt;/td&gt;
&lt;td&gt;89.9%&lt;/td&gt;
&lt;td&gt;87.4%&lt;/td&gt;
&lt;td&gt;64.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_640&lt;/td&gt;
&lt;td&gt;91.1%&lt;/td&gt;
&lt;td&gt;88.1%&lt;/td&gt;
&lt;td&gt;75.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_800&lt;/td&gt;
&lt;td&gt;91.0%&lt;/td&gt;
&lt;td&gt;89.1%&lt;/td&gt;
&lt;td&gt;79.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_1200&lt;/td&gt;
&lt;td&gt;89.6%&lt;/td&gt;
&lt;td&gt;89.0%&lt;/td&gt;
&lt;td&gt;83.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_1600&lt;/td&gt;
&lt;td&gt;89.9%&lt;/td&gt;
&lt;td&gt;87.4%&lt;/td&gt;
&lt;td&gt;81.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;original&lt;/td&gt;
&lt;td&gt;92.0%&lt;/td&gt;
&lt;td&gt;89.5%&lt;/td&gt;
&lt;td&gt;78.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="resnet101_newer-3-layers_1"&gt;Resnet101_newer 3 layers&lt;/h2&gt;
&lt;p&gt;Ket qua khong ghi chep - nhung xac nhan la dung
Update 26-Sept
File     | Easy  | Medium | Hard
---------|-------|--------|-------
min_400  | 87.9% | 80.7%  | 52.9%
min_640  | 90.3% | 87.0%  | 72.0%
min_1280 | 89.9% | 87.5%  | 81.2%
min_1600 | 86.4% | 86.0%  | 80.3%
min_1920 | 82.0% | 83.0%  | 77.6%
original | 82.6% | 83.0%  | 77.6%&lt;/p&gt;
&lt;h1 id="to-do-list_1"&gt;To-do list&lt;/h1&gt;
&lt;h2 id="data-analysis"&gt;Data analysis&lt;/h2&gt;
&lt;p&gt;Done
* Data analysis on WiderFace Validation, Test
    * Make a ipython notebook file for Validation and Test:
    * Height, width of dataset
    * Size of bounding boxes
    * Number of bboxes
    * Number per images&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data analysis on AFW Face&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data analysis on Pascal Face&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="test-with-different-scales"&gt;Test with different scales&lt;/h2&gt;
&lt;p&gt;Done&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make a python script to run on AFW Face.&lt;/li&gt;
&lt;li&gt;Go to school to run it? May be.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="train-with-lower-anchors"&gt;Train with lower anchors&lt;/h2&gt;
&lt;p&gt;Not yet&lt;/p&gt;
&lt;h2 id="train-with-lower-connection"&gt;Train with lower connection&lt;/h2&gt;
&lt;p&gt;Not yet &lt;/p&gt;
&lt;h2 id="train-with-focal_loss"&gt;Train with focal_loss&lt;/h2&gt;
&lt;p&gt;Not yet&lt;/p&gt;
&lt;h2 id="train-with-lower-topn"&gt;Train with lower topN&lt;/h2&gt;
&lt;p&gt;Not yet&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;topN = 4&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="build-tensorflow"&gt;Build Tensorflow&lt;/h2&gt;
&lt;h2 id="notes"&gt;NOTES&lt;/h2&gt;
&lt;p&gt;In order to use previous DeepResnet101, just change the number of CNN layers in prediction block to &lt;code&gt;4&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id="result-on-face-dataset_1"&gt;Result on Face Dataset&lt;/h1&gt;
&lt;h2 id="widerface-dataset"&gt;WiderFace Dataset&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Việc cần l&amp;agrave;m:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Retrain lại model with better accuracy!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IOU Threshold &amp;lt;0.3, 0.1&amp;gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Build CNN &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write the paper.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="table-of-results"&gt;Table of results&lt;/h3&gt;
&lt;h2 id="resnet_newer_1"&gt;Resnet_newer&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;File&lt;/th&gt;
&lt;th&gt;Easy&lt;/th&gt;
&lt;th&gt;Medium&lt;/th&gt;
&lt;th&gt;Hard&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;min_400&lt;/td&gt;
&lt;td&gt;87.9%&lt;/td&gt;
&lt;td&gt;80.7%&lt;/td&gt;
&lt;td&gt;52.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_640&lt;/td&gt;
&lt;td&gt;90.3%&lt;/td&gt;
&lt;td&gt;87.0%&lt;/td&gt;
&lt;td&gt;72.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_1280&lt;/td&gt;
&lt;td&gt;89.9%&lt;/td&gt;
&lt;td&gt;87.5%&lt;/td&gt;
&lt;td&gt;81.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_1600&lt;/td&gt;
&lt;td&gt;86.4%&lt;/td&gt;
&lt;td&gt;86.0%&lt;/td&gt;
&lt;td&gt;80.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_1920&lt;/td&gt;
&lt;td&gt;82.0%&lt;/td&gt;
&lt;td&gt;83.0%&lt;/td&gt;
&lt;td&gt;77.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;original&lt;/td&gt;
&lt;td&gt;82.6%&lt;/td&gt;
&lt;td&gt;83.0%&lt;/td&gt;
&lt;td&gt;77.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mix1&lt;/td&gt;
&lt;td&gt;93.4%&lt;/td&gt;
&lt;td&gt;91.8%&lt;/td&gt;
&lt;td&gt;84.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mix2&lt;/td&gt;
&lt;td&gt;93.7%&lt;/td&gt;
&lt;td&gt;92.2%&lt;/td&gt;
&lt;td&gt;80.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;File&lt;/th&gt;
&lt;th&gt;Easy&lt;/th&gt;
&lt;th&gt;Medium&lt;/th&gt;
&lt;th&gt;Hard&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;remove_low_iou &amp;lt; 0.3&lt;/td&gt;
&lt;td&gt;93.8%&lt;/td&gt;
&lt;td&gt;92.4%&lt;/td&gt;
&lt;td&gt;83.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;remove_low_iou &amp;lt; 0.5&lt;/td&gt;
&lt;td&gt;95.0%&lt;/td&gt;
&lt;td&gt;93.3%&lt;/td&gt;
&lt;td&gt;84.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol&gt;
&lt;li&gt;min_640
    just rescale the smaller size to 640 and keep the ratio unchanged&lt;/li&gt;
&lt;li&gt;original
    just keep the same size and pass it to the network&lt;/li&gt;
&lt;li&gt;mix1
    all the sizes with&lt;/li&gt;
&lt;li&gt;mix2
    the same as mix1, but remove the tiny boxes &amp;lt; 8&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="resnet_maxout"&gt;Resnet_maxout&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;File&lt;/th&gt;
&lt;th&gt;Easy&lt;/th&gt;
&lt;th&gt;Medium&lt;/th&gt;
&lt;th&gt;Hard&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;min_640&lt;/td&gt;
&lt;td&gt;90.2%&lt;/td&gt;
&lt;td&gt;86.5%&lt;/td&gt;
&lt;td&gt;69.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_1280&lt;/td&gt;
&lt;td&gt;89.1%&lt;/td&gt;
&lt;td&gt;87.5%&lt;/td&gt;
&lt;td&gt;80.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_1600&lt;/td&gt;
&lt;td&gt;87.8%&lt;/td&gt;
&lt;td&gt;86.4%&lt;/td&gt;
&lt;td&gt;79.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_1920&lt;/td&gt;
&lt;td&gt;82.0%&lt;/td&gt;
&lt;td&gt;83.0%&lt;/td&gt;
&lt;td&gt;77.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;original&lt;/td&gt;
&lt;td&gt;83.4%&lt;/td&gt;
&lt;td&gt;82.6%&lt;/td&gt;
&lt;td&gt;76.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="fddb-dataset"&gt;FDDB Dataset&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;File&lt;/th&gt;
&lt;th&gt;Easy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;min_640&lt;/td&gt;
&lt;td&gt;96.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;original&lt;/td&gt;
&lt;td&gt;93.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SFD&lt;/td&gt;
&lt;td&gt;98.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CVPR2018&lt;/td&gt;
&lt;td&gt;97.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;max_640&lt;/td&gt;
&lt;td&gt;97.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Deepresnet max_640&lt;/td&gt;
&lt;td&gt;97.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="afw-dataset"&gt;AFW Dataset&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;File&lt;/th&gt;
&lt;th&gt;DeepResnet&lt;/th&gt;
&lt;th&gt;Resnet101&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;afw_deepresnet_1 (0.5, 1, -1)&lt;/td&gt;
&lt;td&gt;99.32%&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;afw_deepresnet_2 (0.25)&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;afw_deepresnet_3 (0.5)&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;afw_deepresnet_4 (1)&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;afw_deepresnet_5 (2)&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;afw_deepresnet_6 (max)&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;afw_deepresnet_7 (min_size=640)&lt;/td&gt;
&lt;td&gt;99.91%&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;afw_deepresnet_8 (0.5, 1)&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;afw_deepresnet_9 (0.5, 1, 2)&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;afw_deepresnet_10 (0.5, 1, 2, max)&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;afw_deepresnet_1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pyramid prediction: &lt;ul&gt;
&lt;li&gt;0.5, &lt;/li&gt;
&lt;li&gt;1, &lt;/li&gt;
&lt;li&gt;1 - flip&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;afw_deepresnet_2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pyramid prediction:&lt;ul&gt;
&lt;li&gt;0.5&lt;/li&gt;
&lt;li&gt;1&lt;/li&gt;
&lt;li&gt;1 - flip&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;afw_deepresnet_2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pyramid prediction:&lt;ul&gt;
&lt;li&gt;0.5&lt;/li&gt;
&lt;li&gt;1&lt;/li&gt;
&lt;li&gt;1 - flip&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torchvision&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;transform&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Compose&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
        &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ToTensor&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normalize&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mf"&gt;0.485&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.456&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.406&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.229&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.224&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.225&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;no_grad&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fm_sizes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unsqueeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;loc_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;cls_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;functional&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;box_coder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_box_coder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fm_sizes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;box_coder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                 &lt;span class="n"&gt;score_thresh&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                 &lt;span class="n"&gt;nms_thresh&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;detach&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;detach&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Some comments here:&lt;/strong&gt;
- Use pyramid prediction &lt;code&gt;afw_deepresnet_1&lt;/code&gt; gives a lot of false positives, predicting the areas including b&amp;agrave;n tay, đ&amp;ugrave;i, ngực, bụng, tường as &lt;code&gt;face&lt;/code&gt;. Thường th&amp;igrave; những trường hợp False positives n&amp;agrave;y c&amp;oacute; sizes lớn (&amp;gt; 300).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Conduct post-processing to filter the false positives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set &lt;code&gt;confidence score&lt;/code&gt; of all boxes of IoU with &lt;code&gt;IoUwith(gt_boxes) &amp;lt; 0.5&lt;/code&gt; to &lt;code&gt;0.1&lt;/code&gt; then: &lt;code&gt;mAP = 99.92&lt;/code&gt;.   &lt;/li&gt;
&lt;li&gt;Set &lt;code&gt;confidence score&lt;/code&gt; of all boxes of IoU with &lt;code&gt;IoUwith(gt_boxes) &amp;lt; 0.3&lt;/code&gt; to &lt;code&gt;0.1&lt;/code&gt; then: &lt;code&gt;mAP = 99.99&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Set &lt;code&gt;confidence score&lt;/code&gt; of all boxes of IoU with &lt;code&gt;IoUwith(gt_boxes) &amp;lt; 0.5&lt;/code&gt; to &lt;code&gt;0.0&lt;/code&gt; then: &lt;code&gt;mAP = 95.58&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Set &lt;code&gt;confidence score&lt;/code&gt; of all boxes of IoU with &lt;code&gt;IoUwith(gt_boxes) &amp;lt; 0.3&lt;/code&gt; to &lt;code&gt;0.0&lt;/code&gt; then: &lt;code&gt;mAP = 99.99&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set &lt;code&gt;confidence score&lt;/code&gt; of all boxes of IoU with &lt;code&gt;IoUwith(gt_boxes) &amp;lt; 0.2&lt;/code&gt; to &lt;code&gt;0.0&lt;/code&gt; then: &lt;code&gt;mAP = 99.99&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remove the &lt;code&gt;false positive&lt;/code&gt; boxes then: &lt;code&gt;mAP = 99.33&lt;/code&gt;?.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="pascal-dataset"&gt;Pascal Dataset&lt;/h2&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;File&lt;/th&gt;
&lt;th&gt;DeepResnet&lt;/th&gt;
&lt;th&gt;Resnet101&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;max_640_deepresnet101&lt;/td&gt;
&lt;td&gt;99.16%&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min_640_deepresnet101&lt;/td&gt;
&lt;td&gt;98.43%&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;original_deepresnet101&lt;/td&gt;
&lt;td&gt;98.25&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;max_640_deepresnet101: resize the larger size to 640, and keep the ratios w:h unchanged.&lt;/li&gt;
&lt;li&gt;min_640_deepresnet101: resize the smaller size to 640, and keep the ratios w:h unchanged.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;original_deepresnet101: keep the original size.&lt;/p&gt;
&lt;p&gt;File not in dataset 2008_000210.jpg 1.0 139 94 325 267&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="charts-and-graphs_1"&gt;Charts and Graphs&lt;/h1&gt;
&lt;h2 id="false-positive-in-different-ranges"&gt;False positive in different ranges.&lt;/h2&gt;
&lt;p&gt;Fig 1.a Number of false positives&lt;/p&gt;
&lt;p&gt;Fig 1.b Confidence of False positive samples&lt;/p&gt;
&lt;p&gt;Fig. 1: (a) Comparison of the number of false positives in different ranges. (b)
Comparison of the mAP gains by progressively removing false positives; from
right to left, the detector is performing better as false positives are removed
according to their confidence scores&lt;/p&gt;
&lt;h2 id="fpn-models"&gt;FPN models&lt;/h2&gt;
&lt;h2 id="afw-pascal-face-wider-face"&gt;AFW &amp;amp; Pascal Face, Wider Face&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;num_layers&lt;/th&gt;
&lt;th&gt;AFW Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Resnet101/best_val_loss_model.pkl&lt;/td&gt;
&lt;td&gt;DeepResnet&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;99.89%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resnet101/best_val_model.pkl&lt;/td&gt;
&lt;td&gt;DeepResnet&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;99.92%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="deep-resnet-with-maxout_1"&gt;Deep Resnet with Maxout&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torchcv.models.deep_fpnresnet.fpn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DeepLabFPN101&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn.functional&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;F&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;PredictBlock&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_anchors&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PredictBlock&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_anchors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_anchors&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc_head&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_make_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_anchors&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cls_head&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_make_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_anchors&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_make_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_planes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;layers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="sd"&gt;'''&lt;/span&gt;
&lt;span class="sd"&gt;        Change back to previous DeepResnet model, &lt;/span&gt;
&lt;span class="sd"&gt;        num_layers = 4&lt;/span&gt;
&lt;span class="sd"&gt;        '''&lt;/span&gt;
        &lt;span class="n"&gt;num_layers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_layers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_planes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;loc_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cls_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cls_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;loc_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loc_pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;permute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contiguous&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;cls_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cls_pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;permute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contiguous&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;loc_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loc_pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cls_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cls_pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cls_pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loc_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DeepFPNResnet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fpn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DeepFPNResnet&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_anchors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extractor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fpn&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dropout&lt;/span&gt;

        &lt;span class="c1"&gt;# Freezing those layers&lt;/span&gt;
        &lt;span class="n"&gt;frozen_layer_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'conv1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'bn1'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;frozen_layer_name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;frozen_layer_names&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;frozen_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;getattr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extractor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;frozen_layer_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;frozen_layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
                &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;requies_grad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pred_layers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModuleList&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_anchor&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_anchors&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pred_layers&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;PredictBlock&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_anchor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fm_sizes_return&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;loc_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;cls_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;h1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h6&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extractor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# print("h1.shape", h1.shape)&lt;/span&gt;
        &lt;span class="c1"&gt;# print("h2.shape", h2.shape)&lt;/span&gt;
        &lt;span class="c1"&gt;# print("h3.shape", h3.shape)&lt;/span&gt;
        &lt;span class="c1"&gt;# print("h4.shape", h4.shape)&lt;/span&gt;
        &lt;span class="c1"&gt;# print("h5.shape", h5.shape)&lt;/span&gt;
        &lt;span class="c1"&gt;# print("h6.shape", h6.shape)&lt;/span&gt;

        &lt;span class="n"&gt;loc_pred1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pred_layers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;h1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_pred1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cls_pred1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;loc_pred2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pred_layers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;h2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_pred2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cls_pred2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;loc_pred3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pred_layers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;h3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_pred3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cls_pred3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;loc_pred4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pred_layers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;h4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_pred4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cls_pred4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;loc_pred5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred5&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pred_layers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;h5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_pred5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cls_pred5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;loc_pred6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred6&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pred_layers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;h6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_pred6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cls_pred6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;loc_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# []&lt;/span&gt;
        &lt;span class="n"&gt;cls_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# []&lt;/span&gt;

        &lt;span class="c1"&gt;# print("loc_preds", loc_preds.size())&lt;/span&gt;
        &lt;span class="c1"&gt;# print("cls_preds", cls_preds.size())&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;fm_sizes_return&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;fm_sizes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;h1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="n"&gt;h2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="n"&gt;h3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:],&lt;/span&gt;
                        &lt;span class="n"&gt;h4&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="n"&gt;h5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="n"&gt;h6&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:]]&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fm_sizes&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_preds&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;WrapModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fpn&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;WrapModel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;module&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DeepFPNResnet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fpn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;fpn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fm_sizes_return&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fm_sizes_return&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;fm_sizes_return&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'cuda'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parallel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;fpn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DeepLabFPN101&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;parallel&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DeepFPNResnet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fpn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;fpn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataParallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;WrapModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fpn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;fpn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;model_path&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;model_path&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;start_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start_epoch&lt;/span&gt;

    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;state_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;map_location&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'cuda:0'&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
        &lt;span class="n"&gt;start_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;state_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'epoch'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;params_tensors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;state_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'net'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_state_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params_tensors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"[INFO] --load pretrained model: {model_path}"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;" | "&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"train_loss: {state_dict['loss']:.2f}"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;" | "&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"epoch: {start_epoch}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start_epoch&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Rectifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Rectifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="c1"&gt;# 28 -&amp;gt; 14 -&amp;gt; 7 -&amp;gt; 3 -&amp;gt; 1&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"conv3"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv4&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"conv4"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Face Detection"></category><category term="2018"></category><category term="PAPER"></category></entry><entry><title>Reduce false positive rate of Object Detector</title><link href="/blog/tutorial/2018/reduce-false-positive-rate-of-object-detector/" rel="alternate"></link><published>2018-08-23T00:52:57+00:00</published><updated>2018-08-23T00:52:57+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-23:/blog/tutorial/2018/reduce-false-positive-rate-of-object-detector/</id><summary type="html">&lt;h1 id="lecture-for-object-detection"&gt;Lecture for Object Detection&lt;/h1&gt;
&lt;p&gt;https://zsc.github.io/megvii-pku-dl-course/slides/Lecture6(Object%20Detection).pdf&lt;/p&gt;
&lt;h1 id="lessons-from-object-detection-models"&gt;Lessons from object detection models&lt;/h1&gt;
&lt;h2 id="region-based-models"&gt;Region-based models&lt;/h2&gt;
&lt;p&gt;https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9&lt;/p&gt;
&lt;h2 id="single-shot-object-detectors"&gt;Single shot object detectors&lt;/h2&gt;
&lt;p&gt;https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d&lt;/p&gt;
&lt;p&gt;Learn a lot from this guy:
https://jhui.github.io/&lt;/p&gt;
&lt;h1 id="train-tensorflow-object-detection-on-own-dataset_1"&gt;Train Tensorflow Object Detection on own dataset&lt;/h1&gt;
&lt;p&gt;After spending a couple days trying to achieve this task, I would like to share my experience of how I went about answering the question:&lt;/p&gt;
&lt;p&gt;How do I use TS Object Detection to train using my own dataset?
https://stackoverflow.com/questions/44973184/train-tensorflow-object-detection-on-own-dataset&lt;/p&gt;
&lt;h1 id="best-strategy-to-reduce-false-positives-googles-new-object-detection-api-on-satellite-imagery"&gt;Best strategy to reduce false positives: Google's new Object Detection API on Satellite Imagery&lt;/h1&gt;
&lt;p&gt;https://stackoverflow.com/questions/45666499/best-strategy-to-reduce-false-positives-googles-new-object-detection-api-on-sa&lt;/p&gt;
&lt;p&gt;# FPN 
https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c&lt;/p&gt;
&lt;h1 id="what-are-the-best-methods-for-reducing-false-positives-using-tensorflows-object-detection-framework"&gt;What are the best methods for reducing false positives using tensorflow's object detection framework?&lt;/h1&gt;
&lt;p&gt;I am training a single object detector with mask rcnn and I have tried …&lt;/p&gt;</summary><content type="html">&lt;h1 id="lecture-for-object-detection"&gt;Lecture for Object Detection&lt;/h1&gt;
&lt;p&gt;https://zsc.github.io/megvii-pku-dl-course/slides/Lecture6(Object%20Detection).pdf&lt;/p&gt;
&lt;h1 id="lessons-from-object-detection-models"&gt;Lessons from object detection models&lt;/h1&gt;
&lt;h2 id="region-based-models"&gt;Region-based models&lt;/h2&gt;
&lt;p&gt;https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9&lt;/p&gt;
&lt;h2 id="single-shot-object-detectors"&gt;Single shot object detectors&lt;/h2&gt;
&lt;p&gt;https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d&lt;/p&gt;
&lt;p&gt;Learn a lot from this guy:
https://jhui.github.io/&lt;/p&gt;
&lt;h1 id="train-tensorflow-object-detection-on-own-dataset_1"&gt;Train Tensorflow Object Detection on own dataset&lt;/h1&gt;
&lt;p&gt;After spending a couple days trying to achieve this task, I would like to share my experience of how I went about answering the question:&lt;/p&gt;
&lt;p&gt;How do I use TS Object Detection to train using my own dataset?
https://stackoverflow.com/questions/44973184/train-tensorflow-object-detection-on-own-dataset&lt;/p&gt;
&lt;h1 id="best-strategy-to-reduce-false-positives-googles-new-object-detection-api-on-satellite-imagery"&gt;Best strategy to reduce false positives: Google's new Object Detection API on Satellite Imagery&lt;/h1&gt;
&lt;p&gt;https://stackoverflow.com/questions/45666499/best-strategy-to-reduce-false-positives-googles-new-object-detection-api-on-sa&lt;/p&gt;
&lt;p&gt;# FPN 
https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c&lt;/p&gt;
&lt;h1 id="what-are-the-best-methods-for-reducing-false-positives-using-tensorflows-object-detection-framework"&gt;What are the best methods for reducing false positives using tensorflow's object detection framework?&lt;/h1&gt;
&lt;p&gt;I am training a single object detector with mask rcnn and I have tried several methods for reducing false positives. I started with a few thousand examples of images of the object with bounding boxes and trained that, got decent results, but when running on images that don't contain that object, would often get false matches with high confidence (sometimes .99).&lt;/p&gt;
&lt;p&gt;The first thing I tried was adding the hard example miner in the config file. I believe I did this correctly because I added a print statement to ensure the object gets created. However none of the configs for faster rcnn have hard example mining in them. So I am suspicious that the miner only works correctly for ssd. I would expect a noticeable improvement with a hard example miner but I did not see it&lt;/p&gt;
&lt;p&gt;The second thing I tried was to add "background" images. I set the minimum number of negatives to a non-zero value in the hard example miner config and added tons of background images that previously got false detections as part of the training. I even added these images into the tfrecords file so that it would be balanced evenly with images that do have the object. This approach actually made things worse - and gave me more false detections&lt;/p&gt;
&lt;p&gt;The last thing I tried was creating another category, called "object-background" and took all the false matches and assigned them to this new category. This approach worked pretty well, but I view it as a hack.&lt;/p&gt;
&lt;p&gt;I guess to summarize my main question is - what is the best method for reducing false positives within the current tensorflow object detection framework? Would SSD be a better approach since that seems to have a hard example miner built into it by default in the configs?&lt;/p&gt;
&lt;p&gt;thanks&lt;/p&gt;
&lt;h2 id="answer"&gt;Answer&lt;/h2&gt;
&lt;p&gt;in the tensorflow github, they advise to ask questions on stackoverflow. There are well known methods for reducing false positives (such as the mentioned hard example mining - ohem) but there doesn't seem to be a lot of documentation on using it&lt;/p&gt;
&lt;h2 id="his-finding"&gt;His finding&lt;/h2&gt;
&lt;p&gt;After some more investigation I actually was able to get the hard example miner with faster rcnn working. I had a bug where I wasn't actually inserting background images into the tf records file.&lt;/p&gt;
&lt;p&gt;I think when training a single object detector (category with one model) it's most crucial to add background images if you want to have good precision/recall. If you just have a few thousand examples of the object, that won't be nearly enough images for the model to learn all the various background noise you will be sending when actually using the model for your application&lt;/p&gt;
&lt;h1 id="object-detection-hard-negatives_1"&gt;Object detection : hard negatives.&lt;/h1&gt;
&lt;p&gt;https://github.com/tensorflow/models/issues/2544&lt;/p&gt;
&lt;p&gt;I am currently using object detection on my own dataset, for some of my classes, i have a lot of false positives with high scores (&amp;gt;0.99, so having a higher score threshold won't help).&lt;/p&gt;
&lt;p&gt;I know there is already some hard negative mining implemented ,but would it be possible to have a feature where one could add hard negatives examples to the training ?&lt;/p&gt;
&lt;p&gt;Let's say we want to detect object A, and we know that object A look a lot like object B, but we are not interested in detecting, object B , in that case we could add images of object B to training (without any bounding boxes) in order for the network to distinguish between A and B.&lt;/p&gt;
&lt;h2 id="answer_1"&gt;Answer&lt;/h2&gt;
&lt;p&gt;Depends on what network you are using, I guess.&lt;/p&gt;
&lt;p&gt;For example, Faster R-CNN considers as negative boxes those which have IoU under a certain threshold (0.3 by default, I think). And then it takes as many negative examples as positive ones (this is configurable, I think).
So basically, if you have your object B in images with A as well, and it is not labeled, it is some chance that it will be considered as a negative.&lt;/p&gt;
&lt;p&gt;To be more certain, I guess you could use object classification (detecting both A and B) and then only keep the objects A.
This should work a bit better, because it tries to regress the class difference to be as big as possible.&lt;/p&gt;
&lt;h2 id="answer_2"&gt;Answer&lt;/h2&gt;
&lt;p&gt;Hello, I guess I have a similar/same problem. Using the InceptionV2-SSD model, and the model gives me great precision and recall on test set that contains at least one of the classes that I am interested in. But as soon as I add some images which do not contain any of my classes, I start getting false positives. Tried training with these images and provided an empty array in place of the truth data boxes. This did not seem to help.&lt;/p&gt;
&lt;p&gt;It would be nice to get answers to two questions (1) Is this an existing problem in the code, or am I doing something wrong here. (2) Is there a recommended solution other than adding another classifier? Would something like modifying the loss functions to take these examples into account work?&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;h2 id="answer_3"&gt;Answer&lt;/h2&gt;
&lt;p&gt;@niyazpk Sorry, I should have been more specific. You need to set min_negatives_per_image to a non zero number in the config for the model to sample boxes from purely negative images: https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_inception_v2_coco.config#L118 . Few 10s might be a good number to choose.&lt;/p&gt;
&lt;p&gt;@OverFlow7 , You can have purely negative images and faster_rcnn models will sample from anchors from them. We use sampling in both stages with a certain ratio of positives to negatives. If the sampler can't achieve that ratio ( in purely negative images), it fills the batch with negatives. See https://github.com/tensorflow/models/blob/master/research/object_detection/core/balanced_positive_negative_sampler.py#L18&lt;/p&gt;
&lt;h2 id="answer_4"&gt;Answer&lt;/h2&gt;
&lt;p&gt;I feel this discussion might benefit others if do it on stackoverflow. Can you please move the discussion there if my response does not sufficiently answer your question alreadt. I will close this issue for now.&lt;/p&gt;
&lt;p&gt;But how do you create tfrecords with purely negative images? what do you put in the .xml?&lt;/p&gt;
&lt;h1 id="some-notes_1"&gt;Some notes&lt;/h1&gt;
&lt;p&gt;Hi !
For the scaling, the idea is try to scale such that all error terms (classification + position + size) have roughly the same scaling. Otherwise, the training would tend to over-optimise one component and not the others.&lt;/p&gt;
&lt;p&gt;Exactly, the negative values are used to mark the anchors with no annotations. The idea comes from the KITTI dataset where some part of the dataset images are signaled as being not labelled : there may be a car/person/... in these parts, but it has not been segmented. If you don't keep track of these parts, you may end up with the SSD model detecting objects not annotated, and the loss function thinking it is False positive, and pushing for not detecting it. Which is not really what we want !
So basically, I set up a mask such that the loss function ignores the anchors which overlap too much with parts of images no-annotated.
Hope it is a bit more clear! I guess I should add a bit of documentation about that!&lt;/p&gt;
&lt;h1 id="revisit-faster-r-cnn"&gt;Revisit Faster R-CNN&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/pdf/1803.06799.pdf&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this paper, we analyze failure cases of state-ofthe-art detectors and observe that &lt;code&gt;most hard false positives&lt;/code&gt; result from &lt;code&gt;classification&lt;/code&gt; instead of localization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3 problems&lt;/strong&gt; :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Shared feature representation is not optimal due to the mismatched goals of feature learning for classification and localization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multi-task learning helps, yet optimization of the multi-task loss may result in sub-optimal for individual tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Large receptive field for different scales leads to redundant context information for small objects.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: We demonstrate the potential of detector classification power by a simple, effective, and widely-applicable Decoupled Classification Refinement (DCR) network. DCR samples hard false positives from the base classifier in Faster RCNN and trains a RCNN-styled strong classifier.&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="{attach}fig1.png"/&gt;
&lt;em&gt;Fig.1 (a) Comparison of the number of false positives in different ranges. (b) Comparison of the mAP gains by progressively removing false positives; from right to left, the detector is performing better as false positives are removed according to their confidence scores.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To answer this question, in this paper, we begin with investigating the key
factors affecting the performance of Faster RCNN. &lt;/p&gt;</content><category term="Face Detection"></category><category term="Ojbect Detection"></category><category term="TUTORIAL"></category></entry><entry><title>CIFAR-10 with Resnet</title><link href="/blog/tutorial/2018/cifar-10-with-resnet/" rel="alternate"></link><published>2018-08-22T16:38:10+00:00</published><updated>2018-08-22T16:38:10+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-22:/blog/tutorial/2018/cifar-10-with-resnet/</id><summary type="html">&lt;h1 id="tensorflow"&gt;Tensorflow&lt;/h1&gt;
&lt;p&gt;The &lt;a href="https://github.com/tensorflow/models/tree/master/official/resnet"&gt;official tensorflow resnet cifar-10&lt;/a&gt;, with --resnet_size=110, hits 93.96%, (beating the reference value of 93.39%).&lt;/p&gt;
&lt;p&gt;Resnet for cifar10 and imagenet look a little different. You can see here that the convolution stride kernel is smaller. Maybe this is what you are doing wrong. Scaling CIFAR images to 224x224 is worse than using smaller kernel in conv1 with 32x32 images.&lt;/p&gt;
&lt;h1 id="pytorch-with-several-models"&gt;Pytorch with several models&lt;/h1&gt;
&lt;p&gt;P.S. I have trained models using &lt;a href="https://github.com/kuangliu/pytorch-cifar"&gt;this repo&lt;/a&gt; and got similar or better accuracy than written in the README with 95.16% on CIFAR10 with PyTorch.&lt;/p&gt;
&lt;p&gt;I tried that repo, running &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python3 main.py
&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; python3 main.py --lr &lt;span class="m"&gt;0&lt;/span&gt;.01 --resume
&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; python3 main.py --lr &lt;span class="m"&gt;0&lt;/span&gt;.001 --resume
&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; python3 main.py --lr &lt;span class="m"&gt;0&lt;/span&gt;.0001 --resume 
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and got only 94.680% rather than 95.16%. And frankly this script is overfitting on the test set, because it just picks whatever works best on …&lt;/p&gt;</summary><content type="html">&lt;h1 id="tensorflow"&gt;Tensorflow&lt;/h1&gt;
&lt;p&gt;The &lt;a href="https://github.com/tensorflow/models/tree/master/official/resnet"&gt;official tensorflow resnet cifar-10&lt;/a&gt;, with --resnet_size=110, hits 93.96%, (beating the reference value of 93.39%).&lt;/p&gt;
&lt;p&gt;Resnet for cifar10 and imagenet look a little different. You can see here that the convolution stride kernel is smaller. Maybe this is what you are doing wrong. Scaling CIFAR images to 224x224 is worse than using smaller kernel in conv1 with 32x32 images.&lt;/p&gt;
&lt;h1 id="pytorch-with-several-models"&gt;Pytorch with several models&lt;/h1&gt;
&lt;p&gt;P.S. I have trained models using &lt;a href="https://github.com/kuangliu/pytorch-cifar"&gt;this repo&lt;/a&gt; and got similar or better accuracy than written in the README with 95.16% on CIFAR10 with PyTorch.&lt;/p&gt;
&lt;p&gt;I tried that repo, running &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python3 main.py
&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; python3 main.py --lr &lt;span class="m"&gt;0&lt;/span&gt;.01 --resume
&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; python3 main.py --lr &lt;span class="m"&gt;0&lt;/span&gt;.001 --resume
&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; python3 main.py --lr &lt;span class="m"&gt;0&lt;/span&gt;.0001 --resume 
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and got only 94.680% rather than 95.16%. And frankly this script is overfitting on the test set, because it just picks whatever works best on the test set, rather than validation.&lt;/p&gt;
&lt;p&gt;Also, my main point was the difference in the conv1 layer for cifar and imagenet. Do check that.&lt;/p&gt;
&lt;p&gt;Some discussion about the implementation at &lt;a href="https://www.reddit.com/r/MachineLearning/comments/7dtrfl/d_how_do_you_get_high_performance_with_resnet/"&gt;reddit&lt;/a&gt;.&lt;/p&gt;</content><category term="Resnet"></category><category term="Cifar10"></category><category term="TUTORIAL"></category></entry><entry><title>Resnet in Tensorflow and Pytorch</title><link href="/blog/tutorial/2018/resnet-in-tensorflow-and-pytorch/" rel="alternate"></link><published>2018-08-22T15:47:37+00:00</published><updated>2018-08-22T15:47:37+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-22:/blog/tutorial/2018/resnet-in-tensorflow-and-pytorch/</id><summary type="html">&lt;p&gt;The Resnet paper can be found at &lt;a href="https://arxiv.org/pdf/1512.03385.pdf"&gt;arxiv&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id="resnet-in-different-frameworks"&gt;Resnet in different frameworks&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Framework&lt;/th&gt;
&lt;th&gt;Pytorch&lt;/th&gt;
&lt;th&gt;Pytorch Stride&lt;/th&gt;
&lt;th&gt;Tensorflow&lt;/th&gt;
&lt;th&gt;Tensorflow Stride&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Input shape&lt;/td&gt;
&lt;td&gt;[batch, channels, height_in, width_in]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[batch, height_in, width_in, channels]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Input&lt;/td&gt;
&lt;td&gt;(1, 3, 640, 640)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;(1, 640, 640, 3)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 1&lt;/td&gt;
&lt;td&gt;(1, 256, 160, 160)&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;(1, 80, 80, 256)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 2&lt;/td&gt;
&lt;td&gt;(1, 512, 80, 80)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;(1, 40, 40, 512)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 3&lt;/td&gt;
&lt;td&gt;(1, 1024, 40, 40)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;(1, 20, 20, 1024)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 4&lt;/td&gt;
&lt;td&gt;(1, 2048, 20, 20)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;(1, 20, 20, 2048)&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="resnet-architecture"&gt;Resnet Architecture&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Block&lt;/th&gt;
&lt;th&gt;Base depth&lt;/th&gt;
&lt;th&gt;Resnet50&lt;/th&gt;
&lt;th&gt;Resnet101&lt;/th&gt;
&lt;th&gt;Output Depth&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Block 1&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 2&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 3&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;1024&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 4&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2048&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="tensorflow-architecture"&gt;Tensorflow Architecture&lt;/h1&gt;
&lt;h2 id="prepare-the-image-inputs"&gt;Prepare the image inputs&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os.path&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;osp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;PIL&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;resnet&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;resnet_v1&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.contrib&lt;/span&gt; &lt;span class="kn"&gt;import …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;The Resnet paper can be found at &lt;a href="https://arxiv.org/pdf/1512.03385.pdf"&gt;arxiv&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id="resnet-in-different-frameworks"&gt;Resnet in different frameworks&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Framework&lt;/th&gt;
&lt;th&gt;Pytorch&lt;/th&gt;
&lt;th&gt;Pytorch Stride&lt;/th&gt;
&lt;th&gt;Tensorflow&lt;/th&gt;
&lt;th&gt;Tensorflow Stride&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Input shape&lt;/td&gt;
&lt;td&gt;[batch, channels, height_in, width_in]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;[batch, height_in, width_in, channels]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Input&lt;/td&gt;
&lt;td&gt;(1, 3, 640, 640)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;(1, 640, 640, 3)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 1&lt;/td&gt;
&lt;td&gt;(1, 256, 160, 160)&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;(1, 80, 80, 256)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 2&lt;/td&gt;
&lt;td&gt;(1, 512, 80, 80)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;(1, 40, 40, 512)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 3&lt;/td&gt;
&lt;td&gt;(1, 1024, 40, 40)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;(1, 20, 20, 1024)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 4&lt;/td&gt;
&lt;td&gt;(1, 2048, 20, 20)&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;(1, 20, 20, 2048)&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="resnet-architecture"&gt;Resnet Architecture&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Block&lt;/th&gt;
&lt;th&gt;Base depth&lt;/th&gt;
&lt;th&gt;Resnet50&lt;/th&gt;
&lt;th&gt;Resnet101&lt;/th&gt;
&lt;th&gt;Output Depth&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Block 1&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 2&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 3&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;1024&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block 4&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2048&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="tensorflow-architecture"&gt;Tensorflow Architecture&lt;/h1&gt;
&lt;h2 id="prepare-the-image-inputs"&gt;Prepare the image inputs&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os.path&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;osp&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;PIL&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;resnet&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;resnet_v1&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.contrib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.contrib.slim&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;slim&lt;/span&gt;


&lt;span class="n"&gt;home_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"~/"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;WIDTH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HEIGHT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="mi"&gt;640&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;640&lt;/span&gt;
&lt;span class="n"&gt;IMG_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;home_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Pictures/Hien.jpg'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;WIDTH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HEIGHT&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;
&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;IMG_PATH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="create-and-restore-the-graph"&gt;Create and Restore the Graph&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create resnet graph&lt;/span&gt;
&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HEIGHT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;WIDTH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resnet_v1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet_arg_scope&lt;/span&gt;&lt;span class="p"&gt;()):&lt;/span&gt;
    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end_points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;resnet_v1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet_v1_&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;is_training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Load the resnet graph to the session&lt;/span&gt;
&lt;span class="n"&gt;saver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Saver&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;RESNET_&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="n"&gt;_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'path/to/resnet/weights.ckpt'&lt;/span&gt;
&lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RESNET_&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="n"&gt;_PATH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="the-output-of-resnet_v1"&gt;The output of &lt;code&gt;resnet_v1&lt;/code&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end_points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;resnet_v1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet_v1_50&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;is_training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;net&lt;/code&gt; is the final &lt;code&gt;logits&lt;/code&gt; values.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;end_points&lt;/code&gt; is a dictionary of &lt;code&gt;key_name&lt;/code&gt; and &lt;code&gt;feature_maps&lt;/code&gt; at each layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we run the lines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Run the graph session&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;end_points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'Placeholder:0'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;keys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"{key:60}"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then we get the output like below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# end_points&lt;/span&gt;

resnet_v1_50/conv1                                           &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;320&lt;/span&gt;, &lt;span class="m"&gt;320&lt;/span&gt;, &lt;span class="m"&gt;64&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut            &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_1/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;64&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_1/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;64&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_1/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_1/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_2/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;64&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_2/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;64&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_2/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_2/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_3/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;160&lt;/span&gt;, &lt;span class="m"&gt;64&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_3/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;64&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_3/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1/unit_3/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block1                                          &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut            &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_1/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;128&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_1/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;128&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_1/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_1/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_2/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;128&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_2/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;128&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_2/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_2/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_3/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;128&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_3/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;128&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_3/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_3/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_4/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;80&lt;/span&gt;, &lt;span class="m"&gt;128&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_4/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;128&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_4/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2/unit_4/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block2                                          &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut            &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_1/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_1/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_1/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_1/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_2/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_2/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_2/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_2/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_3/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_3/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_3/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_3/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_4/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_4/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_4/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_4/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_5/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_5/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_5/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_5/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_6/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;40&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_6/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;256&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_6/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3/unit_6/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block3                                          &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut            &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;2048&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_1/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_1/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_1/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;2048&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_1/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;2048&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_2/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_2/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_2/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;2048&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_2/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;2048&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_3/bottleneck_v1/conv1               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_3/bottleneck_v1/conv2               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;512&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_3/bottleneck_v1/conv3               &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;2048&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4/unit_3/bottleneck_v1                     &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;2048&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/block4                                          &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;20&lt;/span&gt;, &lt;span class="m"&gt;2048&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
resnet_v1_50/logits                                          &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;1000&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
predictions                                                  &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="m"&gt;1000&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="resnet50-feature-maps"&gt;Resnet50 Feature Maps&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;resnet50_fm_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s2"&gt;"resnet_v1_50/block1/unit_2/bottleneck_v1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"resnet_v1_50/block2/unit_3/bottleneck_v1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"resnet_v1_50/block3/unit_4/bottleneck_v1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"resnet_v1_50/block4"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;resnet50_fm_fpn_sizes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;160&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;160&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2048&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="resnet-for-ssd"&gt;Resnet for SSD&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predictBlock&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_anchors&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;loc_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_anchors&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'SAME'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;cls_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_anchors&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'SAME'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# loc_pred = [None, h, w, num_anchors*4]&lt;/span&gt;
    &lt;span class="c1"&gt;# -&amp;gt; reshape [None, h*w*num_anchors, 4]&lt;/span&gt;
    &lt;span class="n"&gt;num_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loc_pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_shape&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;h_w_num_anchors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_inputs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_inputs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;num_anchors&lt;/span&gt;
    &lt;span class="n"&gt;loc_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h_w_num_anchors&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="c1"&gt;# loc_pred = [None, h, w, num_anchors*num_classes]&lt;/span&gt;
    &lt;span class="c1"&gt;# -&amp;gt; reshape [None, h*w*num_anchors, num_classes]&lt;/span&gt;
    &lt;span class="n"&gt;cls_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h_w_num_anchors&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loc_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And the final &lt;code&gt;resnet&lt;/code&gt; with &lt;code&gt;prediction block&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;resnet50&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;num_anchors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;num_classes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

    &lt;span class="c1"&gt;# Create resnet50 graph&lt;/span&gt;
    &lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HEIGHT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;WIDTH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resnet_v1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet_arg_scope&lt;/span&gt;&lt;span class="p"&gt;()):&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end_points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;resnet_v1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet_v1_50&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;is_training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;resnet50_fm_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="s2"&gt;"resnet_v1_50/block1/unit_2/bottleneck_v1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"resnet_v1_50/block2/unit_3/bottleneck_v1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"resnet_v1_50/block3/unit_4/bottleneck_v1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"resnet_v1_50/block4"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;fm_points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;resnet50_fm_names&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;end_points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;fm_points&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;end_points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;loc_pred1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predictBlock&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fm_points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num_anchors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;loc_pred2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predictBlock&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fm_points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num_anchors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;loc_pred3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predictBlock&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fm_points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num_anchors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;loc_pred4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predictBlock&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fm_points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num_anchors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;loc_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;loc_pred1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loc_pred2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loc_pred3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loc_pred4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;cls_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cls_pred1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_pred4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"loc_pred"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"cls_pred"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;loc_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;cls_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cls_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loc_preds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cls_preds&lt;/span&gt;

&lt;span class="n"&gt;resnet50&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Resnet"></category><category term="Tensorflow"></category><category term="Pytorch"></category><category term="TUTORIAL"></category></entry><entry><title>Working with Pelican</title><link href="/blog/tutorial/2018/working-with-pelican/" rel="alternate"></link><published>2018-08-22T14:57:38+00:00</published><updated>2018-08-22T14:57:38+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-22:/blog/tutorial/2018/working-with-pelican/</id><summary type="html">&lt;h1 id="copying-faviconrobotstxt"&gt;Copying favicon/robots.txt&lt;/h1&gt;
&lt;h2 id="first-solution"&gt;First solution&lt;/h2&gt;
&lt;p&gt;If you used the pelican-quickstart command to create a Makefile and want certain files copied to your web root &amp;mdash; such as favicon.ico, robots.txt, or other files &amp;mdash; create a folder called extra next to your Makefile and edit your Makefile to look like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;OUTPUTDIR&lt;span class="k"&gt;)&lt;/span&gt;/%.html:
    &lt;span class="k"&gt;$(&lt;/span&gt;PELICAN&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;INPUTDIR&lt;span class="k"&gt;)&lt;/span&gt; -o &lt;span class="k"&gt;$(&lt;/span&gt;OUTPUTDIR&lt;span class="k"&gt;)&lt;/span&gt; -s &lt;span class="k"&gt;$(&lt;/span&gt;CONFFILE&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;PELICANOPTS&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="cp"&gt;    if test -d $(BASEDIR)/extra; then cp $(BASEDIR)/extra/* $(OUTPUTDIR)/; fi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nf"&gt;publish&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;$(&lt;/span&gt;PELICAN&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;INPUTDIR&lt;span class="k"&gt;)&lt;/span&gt; -o &lt;span class="k"&gt;$(&lt;/span&gt;OUTPUTDIR&lt;span class="k"&gt;)&lt;/span&gt; -s &lt;span class="k"&gt;$(&lt;/span&gt;PUBLISHCONF&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;PELICANOPTS&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="cp"&gt;    if test -d $(BASEDIR)/extra; then cp $(BASEDIR)/extra/* $(OUTPUTDIR)/; fi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="second-solution-using-static_paths_1"&gt;Second solution, using STATIC_PATHS&lt;/h1&gt;
&lt;p&gt;Add &lt;code&gt;favicon.ico&lt;/code&gt; and &lt;code&gt;robots.txt&lt;/code&gt; to the content/extra folder and add the following to &lt;code&gt;pelicanconf.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;STATIC_PATHS = ['images', 'extra/robots.txt', 'extra/favicon.ico']
EXTRA_PATH_METADATA = {
    'extra/robots.txt': {'path': 'robots.txt'},
    'extra/favicon.ico': {'path': 'favicon.ico'}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="make"&gt;Make&lt;/h1&gt;
&lt;p&gt;Make is available on almost any Unix-derived system but is old and can be clunky for …&lt;/p&gt;</summary><content type="html">&lt;h1 id="copying-faviconrobotstxt"&gt;Copying favicon/robots.txt&lt;/h1&gt;
&lt;h2 id="first-solution"&gt;First solution&lt;/h2&gt;
&lt;p&gt;If you used the pelican-quickstart command to create a Makefile and want certain files copied to your web root &amp;mdash; such as favicon.ico, robots.txt, or other files &amp;mdash; create a folder called extra next to your Makefile and edit your Makefile to look like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;OUTPUTDIR&lt;span class="k"&gt;)&lt;/span&gt;/%.html:
    &lt;span class="k"&gt;$(&lt;/span&gt;PELICAN&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;INPUTDIR&lt;span class="k"&gt;)&lt;/span&gt; -o &lt;span class="k"&gt;$(&lt;/span&gt;OUTPUTDIR&lt;span class="k"&gt;)&lt;/span&gt; -s &lt;span class="k"&gt;$(&lt;/span&gt;CONFFILE&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;PELICANOPTS&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="cp"&gt;    if test -d $(BASEDIR)/extra; then cp $(BASEDIR)/extra/* $(OUTPUTDIR)/; fi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nf"&gt;publish&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;$(&lt;/span&gt;PELICAN&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;INPUTDIR&lt;span class="k"&gt;)&lt;/span&gt; -o &lt;span class="k"&gt;$(&lt;/span&gt;OUTPUTDIR&lt;span class="k"&gt;)&lt;/span&gt; -s &lt;span class="k"&gt;$(&lt;/span&gt;PUBLISHCONF&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;PELICANOPTS&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="cp"&gt;    if test -d $(BASEDIR)/extra; then cp $(BASEDIR)/extra/* $(OUTPUTDIR)/; fi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="second-solution-using-static_paths_1"&gt;Second solution, using STATIC_PATHS&lt;/h1&gt;
&lt;p&gt;Add &lt;code&gt;favicon.ico&lt;/code&gt; and &lt;code&gt;robots.txt&lt;/code&gt; to the content/extra folder and add the following to &lt;code&gt;pelicanconf.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;STATIC_PATHS = ['images', 'extra/robots.txt', 'extra/favicon.ico']
EXTRA_PATH_METADATA = {
    'extra/robots.txt': {'path': 'robots.txt'},
    'extra/favicon.ico': {'path': 'favicon.ico'}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="make"&gt;Make&lt;/h1&gt;
&lt;p&gt;Make is available on almost any Unix-derived system but is old and can be clunky for building anything other than code. Many people prefer to use Rake (ruby make) or Fabric (a Pythonic tool for remote execution and deployment). Please post your examples and tips below for awesome development, testing, and deployment.&lt;/p&gt;
&lt;h2 id="make-newpost"&gt;Make newpost&lt;/h2&gt;
&lt;p&gt;You can edit your Makefile to give you a handful of new commands that might make it slightly easier to get writing. Just add the following lines to your Makefile:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;PAGESDIR&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;INPUTDIR&lt;span class="k"&gt;)&lt;/span&gt;/pages
DATE :&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;shell date +&lt;span class="s1"&gt;'%Y-%m-%d %H:%M:%S'&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
SLUG :&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;shell &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;'${NAME}'&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; sed -e &lt;span class="s1"&gt;'s/[^[:alnum:]]/-/g'&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; tr -s &lt;span class="s1"&gt;'-'&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; tr A-Z a-z&lt;span class="k"&gt;)&lt;/span&gt;
EXT ?&lt;span class="o"&gt;=&lt;/span&gt; md

newpost:
ifdef NAME
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;"Title: &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;NAME&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &amp;gt;  &lt;span class="k"&gt;$(&lt;/span&gt;INPUTDIR&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="k"&gt;$(&lt;/span&gt;SLUG&lt;span class="k"&gt;)&lt;/span&gt;.&lt;span class="k"&gt;$(&lt;/span&gt;EXT&lt;span class="k"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;"Slug: &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;SLUG&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &amp;gt;&amp;gt; &lt;span class="k"&gt;$(&lt;/span&gt;INPUTDIR&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="k"&gt;$(&lt;/span&gt;SLUG&lt;span class="k"&gt;)&lt;/span&gt;.&lt;span class="k"&gt;$(&lt;/span&gt;EXT&lt;span class="k"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;"Date: &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;DATE&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &amp;gt;&amp;gt; &lt;span class="k"&gt;$(&lt;/span&gt;INPUTDIR&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="k"&gt;$(&lt;/span&gt;SLUG&lt;span class="k"&gt;)&lt;/span&gt;.&lt;span class="k"&gt;$(&lt;/span&gt;EXT&lt;span class="k"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;              &amp;gt;&amp;gt; &lt;span class="k"&gt;$(&lt;/span&gt;INPUTDIR&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="k"&gt;$(&lt;/span&gt;SLUG&lt;span class="k"&gt;)&lt;/span&gt;.&lt;span class="k"&gt;$(&lt;/span&gt;EXT&lt;span class="k"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;              &amp;gt;&amp;gt; &lt;span class="k"&gt;$(&lt;/span&gt;INPUTDIR&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="k"&gt;$(&lt;/span&gt;SLUG&lt;span class="k"&gt;)&lt;/span&gt;.&lt;span class="k"&gt;$(&lt;/span&gt;EXT&lt;span class="k"&gt;)&lt;/span&gt;
        &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;EDITOR&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;INPUTDIR&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SLUG&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;EXT&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
        @echo &lt;span class="s1"&gt;'Variable NAME is not defined.'&lt;/span&gt;
        @echo &lt;span class="s1"&gt;'Do make newpost NAME='&lt;/span&gt;&lt;span class="s2"&gt;"'"&lt;/span&gt;&lt;span class="s1"&gt;'Post Name'&lt;/span&gt;&lt;span class="s2"&gt;"'"&lt;/span&gt;
endif

editpost:
ifdef NAME
        &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;EDITOR&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;INPUTDIR&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SLUG&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;EXT&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
        @echo &lt;span class="s1"&gt;'Variable NAME is not defined.'&lt;/span&gt;
        @echo &lt;span class="s1"&gt;'Do make editpost NAME='&lt;/span&gt;&lt;span class="s2"&gt;"'"&lt;/span&gt;&lt;span class="s1"&gt;'Post Name'&lt;/span&gt;&lt;span class="s2"&gt;"'"&lt;/span&gt;
endif

newpage:
ifdef NAME
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;"Title: &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;NAME&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &amp;gt;  &lt;span class="k"&gt;$(&lt;/span&gt;PAGESDIR&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="k"&gt;$(&lt;/span&gt;SLUG&lt;span class="k"&gt;)&lt;/span&gt;.&lt;span class="k"&gt;$(&lt;/span&gt;EXT&lt;span class="k"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;"Slug: &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;SLUG&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &amp;gt;&amp;gt; &lt;span class="k"&gt;$(&lt;/span&gt;PAGESDIR&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="k"&gt;$(&lt;/span&gt;SLUG&lt;span class="k"&gt;)&lt;/span&gt;.&lt;span class="k"&gt;$(&lt;/span&gt;EXT&lt;span class="k"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;              &amp;gt;&amp;gt; &lt;span class="k"&gt;$(&lt;/span&gt;PAGESDIR&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="k"&gt;$(&lt;/span&gt;SLUG&lt;span class="k"&gt;)&lt;/span&gt;.&lt;span class="k"&gt;$(&lt;/span&gt;EXT&lt;span class="k"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;              &amp;gt;&amp;gt; &lt;span class="k"&gt;$(&lt;/span&gt;PAGESDIR&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="k"&gt;$(&lt;/span&gt;SLUG&lt;span class="k"&gt;)&lt;/span&gt;.&lt;span class="k"&gt;$(&lt;/span&gt;EXT&lt;span class="k"&gt;)&lt;/span&gt;
        &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;EDITOR&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;PAGESDIR&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SLUG&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.&lt;span class="k"&gt;$(&lt;/span&gt;EXT&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
        @echo &lt;span class="s1"&gt;'Variable NAME is not defined.'&lt;/span&gt;
        @echo &lt;span class="s1"&gt;'Do make newpage NAME='&lt;/span&gt;&lt;span class="s2"&gt;"'"&lt;/span&gt;&lt;span class="s1"&gt;'Page Name'&lt;/span&gt;&lt;span class="s2"&gt;"'"&lt;/span&gt;
endif

editpage:
ifdef NAME
        &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;EDITOR&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;PAGESDIR&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;SLUG&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.&lt;span class="k"&gt;$(&lt;/span&gt;EXT&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
        @echo &lt;span class="s1"&gt;'Variable NAME is not defined.'&lt;/span&gt;
        @echo &lt;span class="s1"&gt;'Do make editpage NAME='&lt;/span&gt;&lt;span class="s2"&gt;"'"&lt;/span&gt;&lt;span class="s1"&gt;'Page Name'&lt;/span&gt;&lt;span class="s2"&gt;"'"&lt;/span&gt;
endif
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To use this, make sure you've set the EDITOR environment variable to the name of your favourite editor, (or set it again within the Makefile), and then do&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ make newpost &lt;span class="nv"&gt;NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Your Exciting Post Name Here'&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Your editor will appear with the beginnings of a new post. You can set 'EXT' to whatever extension you use most often, and then overwrite it by passing "EXT='newext'" to make. Equally, this change assumes that all your pages are stored in content/pages, which may not be true for you. This sample should work well enough for you to base any modifications off, though.&lt;/p&gt;</content><category term="Pelican"></category><category term="TUTORIAL"></category></entry><entry><title>Part III - Deep Learning</title><link href="/blog/book/2018/part-3-deep-learning/" rel="alternate"></link><published>2018-08-22T11:11:42+00:00</published><updated>2018-08-22T11:11:42+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-22:/blog/book/2018/part-3-deep-learning/</id><summary type="html">&lt;h1 id="part-iii"&gt;Part III&lt;/h1&gt;
&lt;p&gt;This part of the book describes the more ambitious and advanced approaches to deep learning, currently pursued by the research community. &lt;/p&gt;
&lt;p&gt;In the previous parts of the book, we have shown how to solve supervised learning problems - how to map one vector to another given enough examples of the mapping. &lt;/p&gt;
&lt;p&gt;Not all problems we might want to solve fall into this category. We may wish to generate new examples, or determine how likely some point is, or handle missing values and take advantage of a large set of unlabeled examples or examples from related tasks. A shortcoming of the current state of the art for industrial applications is that our learning algorithms require large amounts of supervised data to achieve good accuracy. In this part of the book, we discuss some of the speculative approaches to reducing the amount of labeled data necessary for existing models to work …&lt;/p&gt;</summary><content type="html">&lt;h1 id="part-iii"&gt;Part III&lt;/h1&gt;
&lt;p&gt;This part of the book describes the more ambitious and advanced approaches to deep learning, currently pursued by the research community. &lt;/p&gt;
&lt;p&gt;In the previous parts of the book, we have shown how to solve supervised learning problems - how to map one vector to another given enough examples of the mapping. &lt;/p&gt;
&lt;p&gt;Not all problems we might want to solve fall into this category. We may wish to generate new examples, or determine how likely some point is, or handle missing values and take advantage of a large set of unlabeled examples or examples from related tasks. A shortcoming of the current state of the art for industrial applications is that our learning algorithms require large amounts of supervised data to achieve good accuracy. In this part of the book, we discuss some of the speculative approaches to reducing the amount of labeled data necessary for existing models to work well and be applicable across a broader range of tasks. Accomplishing these goals usually requires some form of unsupervised or semi-supervised learning. &lt;/p&gt;
&lt;p&gt;Many deep learning algorithms have been designed to tackle un&lt;/p&gt;</content><category term="Deep Learning"></category><category term="BOOK"></category></entry><entry><title>Đối nhân xử thế trong Đắc Nhân Tâm</title><link href="/blog/other/2018/%C4%90%E1%BB%91i-nh%C3%A2n-x%E1%BB%AD-th%E1%BA%BF-trong-%C4%90%E1%BA%AFc-nh%C3%A2n-t%C3%A2m/" rel="alternate"></link><published>2018-08-21T22:29:12+00:00</published><updated>2018-08-21T22:29:12+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/other/2018/Đối-nhân-xử-thế-trong-Đắc-nhân-tâm/</id><summary type="html">&lt;p&gt;Cuộc sống l&amp;agrave; vậy dẫu x&amp;ocirc; bồ v&amp;agrave;o v&amp;ograve;ng mưu sinh - l&amp;agrave;m gi&amp;agrave;u - khẳng định m&amp;igrave;nh - ước mơ th&amp;igrave; bạn cũng h&amp;atilde;y nh&amp;igrave;n lại v&amp;agrave; giữ cho m&amp;igrave;nh những gi&amp;aacute; trị sống tốt đẹp v&amp;agrave; cao cả - giữ lương t&amp;acirc;m s&amp;aacute;ng suốt để tiến xa hơn nữa.&lt;/p&gt;
&lt;h1 id="nguyen-tac-1-khong-chi-trich-oan-than-than-phien"&gt;Nguy&amp;ecirc;n tắc 1 : Kh&amp;ocirc;ng chỉ tr&amp;iacute;ch o&amp;aacute;n th&amp;aacute;n than phiền&lt;/h1&gt;
&lt;p&gt;Những người bạn gặp tr&amp;ecirc;n đường đời sẽ ảnh hưởng đến cuộc sống của bạn. D&amp;ugrave; tốt hay xấu, họ cũng tặng bạn những kinh nghiệm sống hết sức tuyệt vời. Ch&amp;iacute;nh v&amp;igrave; thế, đừng n&amp;ecirc;n l&amp;ecirc;n &amp;aacute;n, chỉ tr&amp;iacute;ch hay than phiền ai cả. Thậm ch&amp;iacute;, nếu c&amp;oacute; ai đ&amp;oacute; l&amp;agrave;m tổn thương bạn, phản bội bạn hay lợi dụng l&amp;ograve;ng tốt của bạn th&amp;igrave; xin h&amp;atilde;y cứ tha thứ cho họ. Bởi v&amp;igrave; c&amp;oacute; thể, ch&amp;iacute;nh nhờ họ m&amp;agrave; bạn học được c&amp;aacute;ch khoan dung. Chỉ tr&amp;iacute;ch một người l&amp;agrave; việc …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Cuộc sống l&amp;agrave; vậy dẫu x&amp;ocirc; bồ v&amp;agrave;o v&amp;ograve;ng mưu sinh - l&amp;agrave;m gi&amp;agrave;u - khẳng định m&amp;igrave;nh - ước mơ th&amp;igrave; bạn cũng h&amp;atilde;y nh&amp;igrave;n lại v&amp;agrave; giữ cho m&amp;igrave;nh những gi&amp;aacute; trị sống tốt đẹp v&amp;agrave; cao cả - giữ lương t&amp;acirc;m s&amp;aacute;ng suốt để tiến xa hơn nữa.&lt;/p&gt;
&lt;h1 id="nguyen-tac-1-khong-chi-trich-oan-than-than-phien"&gt;Nguy&amp;ecirc;n tắc 1 : Kh&amp;ocirc;ng chỉ tr&amp;iacute;ch o&amp;aacute;n th&amp;aacute;n than phiền&lt;/h1&gt;
&lt;p&gt;Những người bạn gặp tr&amp;ecirc;n đường đời sẽ ảnh hưởng đến cuộc sống của bạn. D&amp;ugrave; tốt hay xấu, họ cũng tặng bạn những kinh nghiệm sống hết sức tuyệt vời. Ch&amp;iacute;nh v&amp;igrave; thế, đừng n&amp;ecirc;n l&amp;ecirc;n &amp;aacute;n, chỉ tr&amp;iacute;ch hay than phiền ai cả. Thậm ch&amp;iacute;, nếu c&amp;oacute; ai đ&amp;oacute; l&amp;agrave;m tổn thương bạn, phản bội bạn hay lợi dụng l&amp;ograve;ng tốt của bạn th&amp;igrave; xin h&amp;atilde;y cứ tha thứ cho họ. Bởi v&amp;igrave; c&amp;oacute; thể, ch&amp;iacute;nh nhờ họ m&amp;agrave; bạn học được c&amp;aacute;ch khoan dung. Chỉ tr&amp;iacute;ch một người l&amp;agrave; việc kh&amp;ocirc;ng kh&amp;oacute;. Vượt l&amp;ecirc;n tr&amp;ecirc;n sự ph&amp;aacute;n x&amp;eacute;t ấy để cư xử rộng lượng, vị tha mới l&amp;agrave; điều đ&amp;aacute;ng tự h&amp;agrave;o.&lt;/p&gt;
&lt;h1 id="nguyen-tac-2-thanh-that-khen-ngoi-cam-kich-nguoi-khac"&gt;Nguy&amp;ecirc;n tắc 2 : Th&amp;agrave;nh thật khen ngợi, cảm k&amp;iacute;ch người kh&amp;aacute;c&lt;/h1&gt;
&lt;p&gt;Biết khen ngợi v&amp;agrave; cảm ơn những người xung quanh một c&amp;aacute;ch ch&amp;acirc;n th&amp;agrave;nh ch&amp;iacute;nh l&amp;agrave; chiếc đũa thần tạo n&amp;ecirc;n n&amp;ecirc;n t&amp;igrave;nh th&amp;acirc;n &amp;aacute;i v&amp;agrave; nguồn động vi&amp;ecirc;n tinh thần to lớn. Đ&amp;oacute; l&amp;agrave; niềm vui rằng mỗi người đang được quan t&amp;acirc;m, c&amp;ocirc;ng nhận v&amp;agrave; y&amp;ecirc;u thương. Mỗi người được khen ngợi ch&amp;acirc;n th&amp;agrave;nh sẽ tự nhi&amp;ecirc;n sửa đổi những t&amp;iacute;nh xấu để trở n&amp;ecirc;n ho&amp;agrave;n thiện hơn. &amp;ldquo;Động cơ th&amp;uacute;c đẩy s&amp;acirc;u sắc nhất trong bản chất con người l&amp;agrave; sự khao kh&amp;aacute;t được thể hiện m&amp;igrave;nh.&amp;rdquo; &amp;ndash; Nh&amp;agrave; triết học Mỹ &amp;ndash; John Dewey&lt;/p&gt;
&lt;h1 id="nguyen-tac-3-khoi-goi-nguoi-khac-y-muon-dieu-ban-de-nghi-ho-lam"&gt;Nguy&amp;ecirc;n tắc 3 : Khơi gợi người kh&amp;aacute;c &amp;yacute; muốn điều bạn đề nghị họ l&amp;agrave;m&lt;/h1&gt;
&lt;p&gt;Dễ khi nhận nhưng kh&amp;oacute; khi cho. Dễ l&amp;agrave; khi nghĩ xấu về người kh&amp;aacute;c nhưng kh&amp;oacute; l&amp;agrave; khi tặng cho họ niềm tin. Dễ l&amp;agrave; khi dập tắt đi ước mơ của người kh&amp;aacute;c v&amp;agrave; kh&amp;oacute; l&amp;agrave; khi gợi cho người kh&amp;aacute;c một mong muốn tha thiết. Vậy tại sao ta kh&amp;ocirc;ng l&amp;agrave;m một điều &amp;ldquo;kh&amp;oacute;&amp;rdquo; m&amp;agrave; hiệu quả thật tốt như khơi gợi mong muốn thiết tha ở một con người?&lt;/p&gt;
&lt;h1 id="nguyen-tac-4-chan-thanh-quan-tam-den-nguoi-khac"&gt;Nguy&amp;ecirc;n tắc 4 : Ch&amp;acirc;n th&amp;agrave;nh quan t&amp;acirc;m đến người kh&amp;aacute;c&lt;/h1&gt;
&lt;p&gt;Khi ch&amp;uacute;ng ta cố g&amp;acirc;y ấn tượng với người kh&amp;aacute;c chỉ để người ấy quan t&amp;acirc;m đến m&amp;igrave;nh, ch&amp;uacute;ng ta sẽ kh&amp;ocirc;ng bao giờ c&amp;oacute; nhiều bạn b&amp;egrave; thật sự ch&amp;acirc;n th&amp;agrave;nh. Nếu muốn c&amp;oacute; những người bạn thật sự th&amp;igrave; h&amp;atilde;y nghĩ v&amp;agrave; l&amp;agrave;m việc g&amp;igrave; đ&amp;oacute; cho họ, d&amp;agrave;nh cho họ thời gian, sức lực v&amp;agrave; sự quan t&amp;acirc;m kh&amp;ocirc;ng vụ lợi. Biểu lộ sự quan t&amp;acirc;m ch&amp;acirc;n th&amp;agrave;nh đối với người kh&amp;aacute;c kh&amp;ocirc;ng những gi&amp;uacute;p bạn c&amp;oacute; th&amp;ecirc;m bạn b&amp;egrave; m&amp;agrave; c&amp;ograve;n c&amp;oacute; thể l&amp;agrave;m tăng l&amp;ograve;ng trung th&amp;agrave;nh của kh&amp;aacute;ch h&amp;agrave;ng đối với c&amp;ocirc;ng ty bạn. H&amp;atilde;y lu&amp;ocirc;n nhớ rằng bạn c&amp;oacute; hay c&amp;aacute;nh tay: một để tự gi&amp;uacute;p m&amp;igrave;nh v&amp;agrave; một để gi&amp;uacute;p người kh&amp;aacute;c. &amp;ldquo;Một người c&amp;oacute; thể th&amp;agrave;nh c&amp;ocirc;ng trong hầu hết mọi việc nếu anh ta c&amp;oacute; một l&amp;ograve;ng nhiệt t&amp;igrave;nh v&amp;ocirc; hạn.&amp;rdquo; Charles Schwab&lt;/p&gt;
&lt;h1 id="nguyen-tac-5-mim-cuoi"&gt;Nguy&amp;ecirc;n tắc 5 : Mỉm cười&lt;/h1&gt;
&lt;p&gt;Đừng qu&amp;ecirc;n mỉm cười trong cuộc sống. Nụ cười của bạn mang lại hạnh ph&amp;uacute;c cho những người xung quanh v&amp;agrave; do đ&amp;oacute; cũng mang lại hạnh ph&amp;uacute;c cho ch&amp;iacute;nh bản th&amp;acirc;n bạn. Nụ cười kh&amp;ocirc;ng chỉ l&amp;agrave;m gi&amp;agrave;u người nhận m&amp;agrave; cả người cho. Nụ cười xuất hiện trong nh&amp;aacute;y mắt nhưng c&amp;oacute; thể để lại dấu ấn suốt đời. H&amp;atilde;y mỉm cười với nhau &amp;ndash; d&amp;ugrave; đ&amp;oacute; l&amp;agrave; người bạn chưa quen biết. Nụ cười ấy sẽ soi chiếu đến những g&amp;oacute;c khuất của t&amp;acirc;m hồn v&amp;agrave; l&amp;agrave;m bừng s&amp;aacute;ng cả những nơi tăm tối nhất. &amp;hellip;. v&amp;igrave; kh&amp;ocirc;ng ai cần một nụ cười nhiều bằng người đ&amp;atilde; kh&amp;ocirc;ng c&amp;ograve;n một nụ cười n&amp;agrave;o nữa để cho đi !&lt;/p&gt;
&lt;h1 id="nguyen-tac-6-luon-nho-rang-ten-mot-nguoi-la-am-thanh-em-dem-ngot-ngao-va-quan-trong-nhat-voi-ho"&gt;Nguy&amp;ecirc;n tắc 6 :Lu&amp;ocirc;n nhớ rằng t&amp;ecirc;n một người l&amp;agrave; &amp;acirc;m thanh &amp;ecirc;m đềm, ngọt ng&amp;agrave;o v&amp;agrave; quan trọng nhất với họ&lt;/h1&gt;
&lt;p&gt;Ch&amp;uacute;ng ta cần nhận ra điều kỳ diệu ẩn chứa sau t&amp;ecirc;n gọi của mỗi con người v&amp;agrave; ghi nhớ rằng mỗi c&amp;aacute;i t&amp;ecirc;n, d&amp;ugrave; đơn giản đến đ&amp;acirc;u, cũng ch&amp;iacute;nh l&amp;agrave; điều quan trọng v&amp;agrave; niềm vui của người ấy. Do vậy, th&amp;ocirc;ng tin m&amp;agrave; ch&amp;uacute;ng ta đang trao đổi hay những c&amp;acirc;u chuyện giữa hai b&amp;ecirc;n sẽ trở n&amp;ecirc;n thật đặc biệt khi ch&amp;uacute;ng ta lồng v&amp;agrave;o trong đ&amp;oacute; t&amp;ecirc;n của người ch&amp;uacute;ng ta đang giao tiếp. Cho d&amp;ugrave; họ l&amp;agrave; ai, người hầu b&amp;agrave;n hay vị tổng gi&amp;aacute;m đốc, c&amp;aacute;i t&amp;ecirc;n vẫn lu&amp;ocirc;n đem lại điều k&amp;igrave; diệu khi ch&amp;uacute;ng ta gọi đ&amp;uacute;ng n&amp;oacute;. &amp;ldquo;Những đức t&amp;iacute;nh tốt được ph&amp;aacute;t triển v&amp;agrave; r&amp;egrave;n luyện từ rất nhiều c&amp;ocirc;ng sức v&amp;agrave; l&amp;ograve;ng quyết t&amp;acirc;m.&amp;rdquo; Ralph Waldo Emerson&lt;/p&gt;
&lt;h1 id="nguyen-tac-7-lang-nghe-nguoi-khac-khuyen-khich-nguoi-khac-noi-ve-ho"&gt;Nguy&amp;ecirc;n tắc 7 : Lắng nghe người kh&amp;aacute;c khuyến kh&amp;iacute;ch người kh&amp;aacute;c n&amp;oacute;i về họ&lt;/h1&gt;
&lt;p&gt;Trong tiếng trung Quốc, từ &amp;ldquo;lắng nghe&amp;rdquo; được viết bởi những n&amp;eacute;t chữ tạo n&amp;ecirc;n bằng 5 từ b&amp;ecirc;n trong &amp;ndash; tai, mắt, tim, một, v&amp;agrave; vua. Lắng nghe ở đ&amp;acirc;y bao h&amp;agrave;m &amp;yacute; ch&amp;uacute;ng ta lu&amp;ocirc;n cần phải mở rộng tai, mắt, tấm l&amp;ograve;ng, h&amp;ograve;a l&amp;agrave;m một với người đang giao tiếp v&amp;agrave; cho họ thấy được sự quan trọng của họ. Muốn c&amp;oacute; t&amp;agrave;i ăn n&amp;oacute;i th&amp;igrave; phải chăm ch&amp;uacute; lắng nghe. Muốn được người kh&amp;aacute;c quan t&amp;acirc;m, bạn n&amp;ecirc;n quan t&amp;acirc;m đến người kh&amp;aacute;c: hỏi những c&amp;acirc;u m&amp;agrave; họ th&amp;iacute;ch trả lời, khuyến kh&amp;iacute;ch họ n&amp;oacute;i về ch&amp;iacute;nh họ v&amp;agrave; th&amp;agrave;nh t&amp;iacute;ch của họ. Bởi v&amp;igrave; sự cảm th&amp;ocirc;ng chia sẻ mạnh hơn lời n&amp;oacute;i, v&amp;agrave; niềm vui c&amp;ugrave;ng với sự ch&amp;acirc;n th&amp;agrave;nh sẽ thật sự bền vững khi bạn biết quan t&amp;acirc;m. &amp;ldquo;Sự im lặng du dương hơn bất cứ bản nhạc n&amp;agrave;o.&amp;rdquo; Christina Rossetti &amp;ldquo;C&amp;agrave;ng trong tĩnh lặng, c&amp;agrave;ng nghe được nhiều.&amp;rdquo; Baba Ram Dass&lt;/p&gt;
&lt;h1 id="nguyen-tac-8-noi-ve-dieu-ma-nguoi-khac-quan-tam"&gt;Nguy&amp;ecirc;n tắc 8 : N&amp;oacute;i về điều m&amp;agrave; người kh&amp;aacute;c quan t&amp;acirc;m&lt;/h1&gt;
&lt;p&gt;Một lời n&amp;oacute;i bất cẩn, thiếu suy nghĩ c&amp;oacute; thể l&amp;agrave;m mất thiện cảm, g&amp;acirc;y ra bất h&amp;ograve;a. Một lời độc &amp;aacute;c c&amp;oacute; thể l&amp;agrave;m tổn thương một t&amp;acirc;m hồn. Một lời đ&amp;uacute;ng l&amp;uacute;c c&amp;oacute; thể mang lại b&amp;igrave;nh an. Một lời y&amp;ecirc;u thương c&amp;oacute; thể đem lại hạnh ph&amp;uacute;c thật sự. V&amp;agrave; c&amp;oacute; những lời n&amp;oacute;i c&amp;oacute; thể cứu được một con người&amp;hellip; Con đường nhanh nhất dẫn đến tr&amp;aacute;i tim một người l&amp;agrave; b&amp;agrave;n luận về những điều người ấy quan t&amp;acirc;m nhất.&lt;/p&gt;
&lt;h1 id="nguyen-tac-9-thanh-that-cho-nguoi-khac-thay-su-quan-trong-ho"&gt;Nguy&amp;ecirc;n tắc 9 : Th&amp;agrave;nh thật cho người kh&amp;aacute;c thấy sự quan trọng họ&lt;/h1&gt;
&lt;p&gt;Người c&amp;oacute; gi&amp;aacute; trị nhất l&amp;agrave; người gi&amp;uacute;p cho đồng loại m&amp;igrave;nh được nhiều nhất. L&amp;agrave;m cho người kh&amp;aacute;c cảm thấy họ quan trọng l&amp;agrave; một trong những c&amp;aacute;ch thức hữu &amp;iacute;ch nhất để gi&amp;uacute;p họ sống v&amp;agrave; l&amp;agrave;m việc tốt hơn. &amp;ldquo;L&amp;ograve;ng ham muốn được tỏ ra m&amp;igrave;nh quan trọng l&amp;agrave; một sự th&amp;ocirc;i th&amp;uacute;c mạnh mẽ nhất trong bản chất con người.&amp;rdquo; John Dewey&lt;/p&gt;
&lt;h1 id="nguyen-tac-10-cach-giai-quyet-tranh-cai-tot-nhat-la-dung-de-no-xay-ra"&gt;Nguy&amp;ecirc;n tắc 10 : C&amp;aacute;ch giải quyết tranh c&amp;atilde;i tốt nhất l&amp;agrave; đừng để n&amp;oacute; xảy ra&lt;/h1&gt;
&lt;p&gt;Đức Phật dạy: &amp;ldquo;O&amp;aacute;n kh&amp;ocirc;ng bao giờ diệt được o&amp;aacute;n, chỉ c&amp;oacute; t&amp;igrave;nh thương y&amp;ecirc;u mới diệt được o&amp;aacute;n.&amp;rdquo; Tranh c&amp;atilde;i kh&amp;ocirc;ng giải quyết được bất h&amp;ograve;a, chỉ c&amp;oacute; l&amp;ograve;ng khoan dung v&amp;agrave; thiện ch&amp;iacute; nh&amp;igrave;n nhận sự việc bằng quan điểm của đối phương mới h&amp;ograve;a giải được. &amp;ldquo;Nếu bạn cố tranh c&amp;atilde;i để thắng th&amp;igrave; đấy cũng chỉ l&amp;agrave; một chiến thắng v&amp;ocirc; nghĩa, bởi v&amp;igrave; bạn sẽ kh&amp;ocirc;ng bao giờ nhận được thiện ch&amp;iacute; v&amp;agrave; sự hợp t&amp;aacute;c của đối phương&amp;rdquo; Benjamin Franklin &amp;ldquo;Khi tranh luận với một người, bạn cần định hướng để sau cuộc tranh c&amp;atilde;i bạn sẽ c&amp;oacute; th&amp;ecirc;m một người bạn.&amp;rdquo; Diodore&lt;/p&gt;
&lt;h1 id="nguyen-tac-11-ton-trong-y-kien-cua-nguoi-khac-dung-bao-gio-noi-anh-chi-sai-roi"&gt;Nguy&amp;ecirc;n tắc 11 : T&amp;ocirc;n trọng &amp;yacute; kiến của người kh&amp;aacute;c đừng bao giờ n&amp;oacute;i anh/ chị sai rồi&lt;/h1&gt;
&lt;p&gt;&amp;ldquo;Dạy người, phải kh&amp;eacute;o l&amp;eacute;o như kh&amp;ocirc;ng dạy g&amp;igrave; cả, giảng điều chưa biết m&amp;agrave; cứ như nhắc lại chuyện đ&amp;atilde; qu&amp;ecirc;n. Bởi một điều đơn giản l&amp;agrave; đối với người hiểu biết th&amp;igrave; chỉ cần nửa lời cũng đủ cho họ nắm r&amp;otilde; mọi điều ch&amp;uacute;ng ta muốn n&amp;oacute;i.&amp;rdquo; &amp;ndash; Alexander Pope &amp;ldquo;T&amp;agrave;i năng qu&amp;yacute; hiếm nhất l&amp;agrave; t&amp;agrave;i năng của một người biết nh&amp;igrave;n nhận người kh&amp;aacute;c c&amp;oacute; t&amp;agrave;i năng.&amp;rdquo; &amp;ndash; Ernest Hemingway&lt;/p&gt;
&lt;h1 id="nguyen-tac-12-neu-ban-sai-nhanh-chong-thang-than-thua-nhan-loi-lam"&gt;Nguy&amp;ecirc;n tắc 12 : Nếu bạn sai nhanh ch&amp;oacute;ng thẳng thắn thừa nhận lỗi lầm&lt;/h1&gt;
&lt;p&gt;Nếu ch&amp;uacute;ng ta chịu nh&amp;igrave;n nhận những điều sai tr&amp;aacute;i của m&amp;igrave;nh trước khi người kh&amp;aacute;c c&amp;oacute; dịp n&amp;oacute;i ra, ch&amp;uacute;ng ta sẽ c&amp;oacute; 99% cơ hội được đối xử bằng th&amp;aacute;i độ h&amp;agrave;o hiệp. Trong quan hệ với con người, h&amp;atilde;y lu&amp;ocirc;n nhớ rằng ch&amp;uacute;ng ta đang giao tiếp với những sinh vật kh&amp;ocirc;ng những c&amp;oacute; l&amp;yacute; tr&amp;iacute; m&amp;agrave; c&amp;ograve;n c&amp;oacute; cảm x&amp;uacute;c, họ rất dễ bị tổn thương bởi định kiến nhưng lu&amp;ocirc;n c&amp;oacute; động lực khi c&amp;oacute; niềm tự h&amp;agrave;o v&amp;agrave; l&amp;ograve;ng ki&amp;ecirc;u h&amp;atilde;nh.&amp;ldquo;Nếu bạn đ&amp;atilde; sai lầm th&amp;igrave; kh&amp;ocirc;ng g&amp;igrave; hay bằng thẳng thắn n&amp;oacute;i rằng: &amp;ldquo;T&amp;ocirc;i đ&amp;atilde; sai&amp;rdquo;. Nhượng bộ kh&amp;ocirc;ng phải hạ m&amp;igrave;nh, nhận lỗi kh&amp;ocirc;ng l&amp;agrave; nhục nh&amp;atilde;.&amp;rdquo; &amp;ndash; Fenelon&lt;/p&gt;
&lt;h1 id="nguyen-tac-13-luon-bat-dau-bang-thai-do-than-thien"&gt;Nguy&amp;ecirc;n tắc 13 : Lu&amp;ocirc;n bắt đầu bằng th&amp;aacute;i độ th&amp;acirc;n thiện&lt;/h1&gt;
&lt;p&gt;Truyện ngụ ng&amp;ocirc;n của Aesop kể rằng: Mặt Trời v&amp;agrave; Gi&amp;oacute; tranh c&amp;atilde;i xem b&amp;ecirc;n n&amp;agrave;o mạnh hơn . Gi&amp;oacute; n&amp;oacute;i: &amp;ldquo;T&amp;ocirc;i sẽ chứng minh t&amp;ocirc;i mạnh hơn. &amp;Ocirc;ng c&amp;oacute; thấy cụ gi&amp;agrave; đằng kai kh&amp;ocirc;ng? T&amp;ocirc;i đ&amp;aacute;nh cuộc sẽ l&amp;agrave;m &amp;ocirc;ng cụ cởi &amp;aacute;o kho&amp;aacute;c của m&amp;igrave;nh ra nhanh hơn &amp;ocirc;ng&amp;rdquo;. Mặt Trời ẩn m&amp;igrave;nh sau một đ&amp;aacute;m m&amp;acirc;y để Gi&amp;oacute; chứng tỏ uy quyền của m&amp;igrave;nh. Gi&amp;oacute; đ&amp;atilde; thổi mạnh gần như một cơn b&amp;atilde;o. Nhưng Gi&amp;oacute; c&amp;agrave;ng thổi mạnh bao nhi&amp;ecirc;u th&amp;igrave; cụ gi&amp;agrave; lại c&amp;agrave;ng giữ chặt chiếc &amp;aacute;o kho&amp;aacute;c của m&amp;igrave;nh bấy nhi&amp;ecirc;u. Cuối c&amp;ugrave;ng, Gi&amp;oacute; lặng đi v&amp;agrave; chịu thua. Khi đ&amp;oacute; Mặt Trời rời khỏi những đ&amp;aacute;m m&amp;acirc;y, dịu d&amp;agrave;ng tỏa những tia nắng &amp;oacute;ng &amp;aacute;nh xuống mặt đất. Bỗng chốc tr&amp;aacute;n cụ gi&amp;agrave; lấm tấm mồ h&amp;ocirc;i, rồi cụ chau m&amp;agrave;y v&amp;agrave; cởi &amp;aacute;o kho&amp;aacute;c ra. Mặt Trời đ&amp;atilde; cho Gi&amp;oacute; một b&amp;agrave;i học, rằng đề nghị nhẹ nh&amp;agrave;ng bao giờ cũng mang lại hiệu quả cao hơn sự &amp;eacute;p buộc bằng vũ lực. Th&amp;aacute;i độ dịu d&amp;agrave;ng, th&amp;acirc;n thiện v&amp;agrave; những lời khen ngợi ch&amp;acirc;n th&amp;agrave;nh c&amp;oacute; thể khiến người ta thay đổi &amp;yacute; kiến dễ d&amp;agrave;ng hơn l&amp;agrave; g&amp;acirc;y căng thẳng kh&amp;oacute; chịu. &amp;ldquo;Một giọt mật ngọt bắt được nhiều ruồi hơn l&amp;agrave; một th&amp;ugrave;ng nước đắng.&amp;rdquo; &amp;ndash; Abraham Lincoln&lt;/p&gt;
&lt;h1 id="nguyen-tac-14-hoi-nhung-cau-khien-nguoi-khac-vang-co-ngay-lap-tuc"&gt;Nguy&amp;ecirc;n tắc 14 : Hỏi những c&amp;acirc;u khiến người kh&amp;aacute;c v&amp;acirc;ng c&amp;oacute; ngay lập tức&lt;/h1&gt;
&lt;p&gt;Khi n&amp;oacute;i chuyện với mọi người, bạn kh&amp;ocirc;ng n&amp;ecirc;n bắt đầu bằng những điểm kh&amp;aacute;c biệt, m&amp;agrave; n&amp;ecirc;n bắt đầu bằng c&amp;aacute;ch nhấn mạnh những điểm m&amp;agrave; hai b&amp;ecirc;n đều đồng &amp;yacute;. Nếu như c&amp;oacute; thể, bạn sẽ tiếp tục nhấn mạnh rằng cả hai đều phấn đấu cho c&amp;ugrave;ng một mục đ&amp;iacute;ch, sự kh&amp;aacute;c nhau duy nhất giữa hai người chỉ l&amp;agrave; về mặt phương ph&amp;aacute;p m&amp;agrave; th&amp;ocirc;i. C&amp;acirc;u trả lời &amp;ldquo;Đồng &amp;yacute;&amp;rdquo; ngay từ đầu sẽ tốt hơn rất nhiều. V&amp;igrave; khi một người đ&amp;atilde; thực sự n&amp;oacute;i &amp;ldquo;Kh&amp;ocirc;ng&amp;rdquo; th&amp;igrave; to&amp;agrave;n bộ c&amp;aacute;c gi&amp;aacute;c quan, hệ thần kinh, cơ bắp của người ấy đều tập trung trong t&amp;acirc;m thế từ chối. Tr&amp;aacute;i lại, khi một người n&amp;oacute;i &amp;ldquo;C&amp;oacute;&amp;rdquo; th&amp;igrave; từng tế b&amp;agrave;o trong cơ thể người đ&amp;oacute; gi&amp;atilde;n ra trong trạng th&amp;aacute;i sẵn s&amp;agrave;ng tiếp nhận. N&amp;ecirc;n nếu ngay từ đầu ch&amp;uacute;ng ta tranh thủ được nhiều tiếng &amp;ldquo;C&amp;oacute;&amp;rdquo; l&amp;agrave; ta đ&amp;atilde; mở rộng con đường cho việc tiếp nhận đề nghị sau c&amp;ugrave;ng &amp;ndash; mục đ&amp;iacute;ch của ch&amp;uacute;ng ta. &amp;ldquo;Ai bước nhẹ nh&amp;agrave;ng sẽ đi được xa&amp;rdquo; &amp;ndash; Ngạn ngữ Trung Hoa&lt;/p&gt;
&lt;h1 id="nguyen-tac-15-tao-dieu-kien-de-nguoi-khac-duoc-noi-thoa-thich"&gt;Nguy&amp;ecirc;n tắc 15 : Tạo điều kiện để người kh&amp;aacute;c được n&amp;oacute;i thỏa th&amp;iacute;ch&lt;/h1&gt;
&lt;p&gt;Điều quan trọng nhất của mọi cuộc tr&amp;ograve; chuyện l&amp;agrave; để người kh&amp;aacute;c bộc lộ m&amp;igrave;nh. Sau khi b&amp;agrave;y tỏ những quan t&amp;acirc;m của m&amp;igrave;nh, người đối thoại sẽ h&amp;agrave;i l&amp;ograve;ng về bản th&amp;acirc;n v&amp;agrave; kiến thức của họ, v&amp;agrave; tự nhi&amp;ecirc;n họ sẽ lắng nghe ch&amp;uacute;ng ta. &amp;ldquo;C&amp;aacute;c nh&amp;agrave; h&amp;ugrave;ng biện bao giờ cũng hiếm c&amp;oacute;. Nhưng hiếm hơn nữa l&amp;agrave; những người biết im lặng đ&amp;uacute;ng l&amp;uacute;c v&amp;agrave; c&amp;agrave;ng qu&amp;yacute; hơn l&amp;agrave; những ai biết nhường lời cho kẻ kh&amp;aacute;c.&amp;rdquo; &amp;ndash; M. F. Sovado&lt;/p&gt;
&lt;h1 id="nguyen-tac-16-lam-nguoi-khac-tin-rang-chinh-ho-la-nguoi-dua-ra-y-tuong-dau-tien"&gt;Nguy&amp;ecirc;n tắc 16 :L&amp;agrave;m người kh&amp;aacute;c tin rằng ch&amp;iacute;nh họ l&amp;agrave; người đưa ra &amp;yacute; tưởng đầu ti&amp;ecirc;n&lt;/h1&gt;
&lt;p&gt;Mọi người đều th&amp;iacute;ch l&amp;agrave;m theo &amp;yacute; m&amp;igrave;nh chứ kh&amp;ocirc;ng ai muốn h&amp;agrave;nh động theo lời người kh&amp;aacute;c sai bảo. Ai cũng th&amp;iacute;ch được hỏi về những mong muốn, nguyện vọng v&amp;agrave; suy nghĩ của họ. &amp;ldquo;Nước suối v&amp;agrave; mưa nguồn sở dĩ đều chảy về s&amp;ocirc;ng s&amp;acirc;u biển lớn v&amp;igrave; s&amp;ocirc;ng v&amp;agrave; biển d&amp;aacute;m chấp nhận ở vị tr&amp;iacute; thấp. Th&amp;aacute;nh nh&amp;acirc;n muốn thể hiện uy đức cao hơn người n&amp;ecirc;n đặt m&amp;igrave;nh dưới họ, muốn tr&amp;iacute; năng vượt trước thời đại th&amp;igrave; phải ẩn m&amp;igrave;nh ở ph&amp;iacute;a sau. V&amp;igrave; vậy, d&amp;ugrave; vị thế th&amp;aacute;nh nh&amp;acirc;n ở tr&amp;ecirc;n thi&amp;ecirc;n hạ cũng kh&amp;ocirc;ng ai tức tối, d&amp;ugrave; vượt trước thi&amp;ecirc;n hạ cũng kh&amp;ocirc;ng ai o&amp;aacute;n hờn.&amp;rdquo; &amp;ndash; L&amp;atilde;o Tử&lt;/p&gt;
&lt;h1 id="nguyen-tac-17-thanh-that-nhin-nhan-van-de-theo-quan-diem-cua-nguoi-khac"&gt;Nguy&amp;ecirc;n tắc 17 : Th&amp;agrave;nh thật nh&amp;igrave;n nhận vấn đề theo quan điểm của người kh&amp;aacute;c&lt;/h1&gt;
&lt;p&gt;Trong mọi mối quan hệ, phải biết bỏ qua c&amp;aacute;i t&amp;ocirc;i của m&amp;igrave;nh v&amp;agrave; đồng cảm với người kh&amp;aacute;c để suy x&amp;eacute;t mọi việc.&amp;ldquo;Mức độ lớn kh&amp;ocirc;n v&amp;agrave; trưởng th&amp;agrave;nh thực sự trong cuộc đời của mỗi con người t&amp;ugrave;y thuộc v&amp;agrave;o th&amp;aacute;i độ ứng xử của họ đối với người kh&amp;aacute;c: dịu d&amp;agrave;ng với người trẻ, cảm th&amp;ocirc;ng với người gi&amp;agrave;, chia sẻ với người bất hạnh, động vi&amp;ecirc;n người c&amp;oacute; ch&amp;iacute; hướng, tha thứ người mắc lỗi lầm, bao dung với kẻ yếu v&amp;agrave; khoan h&amp;ograve;a với kẻ mạnh. Bởi lẽ, đến một l&amp;uacute;c n&amp;agrave;o đ&amp;oacute; trong cuộc đời của mỗi con người, họ cũng sẽ l&amp;acirc;m v&amp;agrave;o những cảnh ngộ tương tự.&amp;rdquo; &amp;ndash; George Washington Carver&lt;/p&gt;
&lt;h1 id="nguyen-tac-18-dong-cam-voi-mong-muon-chia-se-cua-nguoi-khac"&gt;Nguy&amp;ecirc;n tắc 18 : ĐỒng cảm với mong muốn chia sẻ của người kh&amp;aacute;c&lt;/h1&gt;
&lt;p&gt;Điều đ&amp;aacute;ng qu&amp;yacute; nhất trong cuộc đời của mỗi người ch&amp;iacute;nh l&amp;agrave; những nghĩa cử tốt đẹp đối với người kh&amp;aacute;c &amp;ndash; những nghĩa cử nhỏ b&amp;eacute;, kh&amp;ocirc;ng t&amp;ecirc;n m&amp;agrave; ch&amp;iacute;nh người đ&amp;oacute; đ&amp;atilde; qu&amp;ecirc;n đi. Chỉ cần một c&amp;aacute;i &amp;ocirc;m thật chặt, một sự im lặng cảm th&amp;ocirc;ng , một c&amp;aacute;i chạm tay th&amp;acirc;n thiện, một đ&amp;ocirc;i tai biết lắng nghe l&amp;agrave; bạn c&amp;oacute; thể chia sẻ với tất cả mọi người. L&amp;ograve;ng tốt, sự quan t&amp;acirc;m chia sẻ, đồng cảm l&amp;agrave; ng&amp;ocirc;n ngữ đặc biệt m&amp;agrave; bất cứ ai cũng c&amp;oacute; thể cảm nhận được. Ba phần tư những người ch&amp;uacute;ng ta gặp ng&amp;agrave;y mai lu&amp;ocirc;n &amp;ldquo;đ&amp;oacute;i kh&amp;aacute;t&amp;rdquo; sự đồng cảm v&amp;agrave; chia sẻ. H&amp;atilde;y cho họ điều đ&amp;oacute; v&amp;agrave; họ sẽ y&amp;ecirc;u mến bạn.&amp;ldquo;Đừng để một ai chẳng nhận được g&amp;igrave; sau khi rời bạn mặc d&amp;ugrave; bạn biết rằng c&amp;oacute; thể bạn sẽ kh&amp;ocirc;ng bao giờ gặp lại họ. Đ&amp;ocirc;i khi, chỉ một &amp;aacute;nh mắt thiện cảm d&amp;agrave;nh cho người kh&amp;aacute;c cũng l&amp;agrave; một m&amp;oacute;n qu&amp;agrave; lớn lao trong đời.&amp;rdquo; &amp;ndash; Ngạn ngữ Ph&amp;aacute;p&lt;/p&gt;
&lt;h1 id="nguyen-tac-19-khoi-goi-su-cao-thuong-cua-nguoi-khac"&gt;Nguy&amp;ecirc;n tắc 19 :Khơi gợi sự cao thượng của người kh&amp;aacute;c&lt;/h1&gt;
&lt;p&gt;Nếu được y&amp;ecirc;u thương, con người sẽ biết y&amp;ecirc;u thương v&amp;agrave; trở n&amp;ecirc;n đ&amp;aacute;ng y&amp;ecirc;u hơn. &amp;ldquo;Nếu bạn cứ chỉ lu&amp;ocirc;n nh&amp;igrave;n v&amp;agrave;o mặt xấu của một ai đ&amp;oacute;, điều đ&amp;oacute; sẽ l&amp;agrave;m anh ta ng&amp;agrave;y c&amp;agrave;ng trở n&amp;ecirc;n tồi tệ hơn. Nhưng nếu khuyến kh&amp;iacute;ch anh ta vươn tới những điều tốt, chắc chắn anh ta sẽ l&amp;agrave;m được.&amp;rdquo; &amp;ndash; Johann Goethe&lt;/p&gt;
&lt;h1 id="nguyen-tac-20-lam-sinh-dong-y-tuong"&gt;Nguy&amp;ecirc;n tắc 20 : L&amp;agrave;m sinh động &amp;yacute; tưởng&lt;/h1&gt;
&lt;p&gt;Một m&amp;oacute;n ăn được tr&amp;igrave;nh b&amp;agrave;y sinh động sẽ l&amp;agrave;m thực kh&amp;aacute;ch cảm nhận r&amp;otilde; c&amp;aacute;i t&amp;agrave;i của người đầu bếp. Một truyện ngắn được viết sinh động sẽ l&amp;agrave;m độc giả nhận ra khả năng sử dụng ng&amp;ocirc;n ngữ của nh&amp;agrave; văn.Một &amp;yacute; kiến được m&amp;ocirc; tả sinh động l&amp;agrave;m người nghe cảm nhận tr&amp;iacute; tuệ tinh t&amp;uacute;y của người n&amp;oacute;i.&lt;/p&gt;
&lt;h1 id="nguyen-tac-21-thach-khoi-goi-su-thu-thach-o-nguoi-khac"&gt;Nguy&amp;ecirc;n tắc 21 :Th&amp;aacute;ch khơi gợi sự thử th&amp;aacute;ch ở người kh&amp;aacute;c&lt;/h1&gt;
&lt;p&gt;Frederic Herzberg l&amp;agrave; một trong những nh&amp;agrave; khoa học rất nổi tiếng về h&amp;agrave;nh vi con người. &amp;Ocirc;ng nghi&amp;ecirc;n cứu th&amp;aacute;i độ trong c&amp;ocirc;ng việc của h&amp;agrave;ng ng&amp;agrave;n c&amp;ocirc;ng nh&amp;acirc;n v&amp;agrave; c&amp;aacute;c nh&amp;agrave; quản trị cao cấp. &amp;Ocirc;ng đ&amp;atilde; t&amp;igrave;m ra những yếu tố động vi&amp;ecirc;n quan trọng nhất đối với người đi l&amp;agrave;m &amp;ndash; kh&amp;ocirc;ng phải l&amp;agrave; tiền bạc, m&amp;ocirc;i trường l&amp;agrave;m việc tốt, hay ph&amp;uacute;c lợi; m&amp;agrave; l&amp;agrave; bản th&amp;acirc;n c&amp;ocirc;ng việc. Nếu c&amp;ocirc;ng việc th&amp;uacute; vị, tạo điều kiện để ph&amp;aacute;t triển, thể hiện năng lực, nh&amp;acirc;n vi&amp;ecirc;n sẽ rất gắn b&amp;oacute; v&amp;agrave; lu&amp;ocirc;n c&amp;oacute; động lực ho&amp;agrave;n th&amp;agrave;nh c&amp;ocirc;ng việc thật tốt. Đ&amp;acirc;y cũng l&amp;agrave; điều m&amp;agrave; mọi người đều mong muốn: c&amp;oacute; cơ hội thử sức; cơ hội để bứt ph&amp;aacute;, thể hiện; cơ hội để chứng minh g&amp;aacute;i trị thực của m&amp;igrave;nh, để ph&amp;aacute;t triển v&amp;agrave; gi&amp;agrave;nh thắng lợi.&lt;/p&gt;
&lt;h1 id="nguyen-tac-22-bat-dau-bang-nhung-loi-khen-tang-thanh-that"&gt;Nguy&amp;ecirc;n tắc 22 :Bắt đầu bằng những lời khen tặng th&amp;agrave;nh thật&lt;/h1&gt;
&lt;p&gt;Khen ngợi trước khi g&amp;oacute;p &amp;yacute; cũng giống như nha sĩ bắt đầu c&amp;ocirc;ng việc bằng thuốc t&amp;ecirc;. N&amp;oacute; sẽ gi&amp;uacute;p bệnh nh&amp;acirc;n khỏi đau đớn khi bị nhổ răng. Những người l&amp;atilde;nh đạo v&amp;agrave; quản l&amp;yacute; cần ghi nhớ: kh&amp;aacute;t vọng s&amp;acirc;u xa của mỗi con người l&amp;agrave; được khen ngợi , được t&amp;ocirc;n trọng v&amp;agrave; được quan t&amp;acirc;m. &amp;ldquo;Kh&amp;ocirc;ng g&amp;igrave; &amp;iacute;t tốn k&amp;eacute;m bằng lời khen, lời c&amp;aacute;m ơn v&amp;agrave; lời xin lỗi.&amp;rdquo; &amp;ndash; Montlue&lt;/p&gt;
&lt;h1 id="nguyen-tac-23-gop-y-sai-lam-cua-nguoi-khac-mot-cach-gian-tiep"&gt;Nguy&amp;ecirc;n tắc 23 :G&amp;oacute;p &amp;yacute; sai lầm của người kh&amp;aacute;c một c&amp;aacute;ch gi&amp;aacute;n tiếp&lt;/h1&gt;
&lt;p&gt;Việc người kh&amp;aacute;c gi&amp;aacute;n tiếp ch&amp;uacute; &amp;yacute; tới những thiếu s&amp;oacute;t của m&amp;igrave;nh sẽ l&amp;agrave;m cho những người nhạy cảm rất cảm k&amp;iacute;ch, trong khi họ c&amp;oacute; thể cảm thấy rất kh&amp;oacute; chịu trước bất kỳ lời ph&amp;ecirc; ph&amp;aacute;n trực tiếp n&amp;agrave;o. Con người vốn c&amp;oacute; bản chất ki&amp;ecirc;u h&amp;atilde;nh tự nhi&amp;ecirc;n. Việc n&amp;oacute;i thẳng ra rằng người n&amp;agrave;o đ&amp;oacute; sai lầm ch&amp;iacute;nh l&amp;agrave; một sai lầm lớn nhất.&lt;/p&gt;
&lt;h1 id="nguyen-tac-24-xem-xet-nhin-nhan-ban-than-truoc-khi-gop-y-cho-nguoi-khac"&gt;Nguy&amp;ecirc;n tắc 24 : Xem x&amp;eacute;t nh&amp;igrave;n nhận bản th&amp;acirc;n trước khi g&amp;oacute;p &amp;yacute; cho người kh&amp;aacute;c&lt;/h1&gt;
&lt;p&gt;Tin tưởng rằng m&amp;igrave;nh c&amp;oacute; l&amp;yacute; tr&amp;iacute; v&amp;agrave; chỉ duy nhất m&amp;igrave;nh l&amp;agrave; người c&amp;oacute; l&amp;yacute; l&amp;agrave; biểu hiện của một tầm nh&amp;igrave;n hẹp v&amp;agrave; cố chấp. Nếu bạn kh&amp;ocirc;ng thể ngẩng cao đầu v&amp;agrave; thừa nhận lỗi lầm của m&amp;igrave;nh th&amp;igrave; lỗi lầm ấy sẽ khống chế bạn. Việc tự nhận lỗi lầm kh&amp;ocirc;ng chỉ l&amp;agrave;m cho người kh&amp;aacute;c t&amp;ocirc;n trọng bạn hơn m&amp;agrave; c&amp;ograve;n ph&amp;aacute;t triển sự tự trọng của bản th&amp;acirc;n m&amp;igrave;nh.Nếu người ph&amp;ecirc; ph&amp;aacute;n khi&amp;ecirc;m tốn thừa nhận rằng ch&amp;iacute;nh họ cũng từng phạm lỗi như thế , th&amp;igrave; c&amp;oacute; kh&amp;oacute; khăn g&amp;igrave; khi ch&amp;uacute;ng ta nghe về những lỗi lầm của ch&amp;uacute;ng ta? Việc nh&amp;igrave;n nhận sai lầm của ch&amp;iacute;nh m&amp;igrave;nh ngay cả khi chưa kịp sữa chữa, c&amp;oacute; thể gi&amp;uacute;p ch&amp;uacute;ng ta thuyết phục người kh&amp;aacute;c thay đổi h&amp;agrave;nh vi của họ.&lt;/p&gt;
&lt;h1 id="nguyen-tac-25-dat-cau-hoi-thay-vi-dua-ra-menh-lenh"&gt;Nguy&amp;ecirc;n tắc 25 : Đặt c&amp;acirc;u hỏi thay v&amp;igrave; đưa ra mệnh lệnh&lt;/h1&gt;
&lt;p&gt;Ra lệnh bằng c&amp;aacute;ch đặt c&amp;acirc;u hỏi l&amp;agrave; tạo điều kiện cho người nhận lệnh được c&amp;ugrave;ng đưa ra quyết định. V&amp;agrave; một khi họ tham gia ra quyết định, họ sẽ chủ động thực hiện quyết định ấy một c&amp;aacute;ch s&amp;aacute;ng tạo v&amp;agrave; t&amp;iacute;ch cực nhất. &amp;ldquo;Cố gắng đừng l&amp;agrave;m người kh&amp;aacute;c bị tổn thương, d&amp;ugrave; l&amp;agrave; chỉ một c&amp;acirc;u n&amp;oacute;i đ&amp;ugrave;a.&amp;rdquo; &amp;ndash; Publilius Syrus&lt;/p&gt;
&lt;h1 id="nguyen-tac-26-giu-the-dien-cho-nguoi-khac"&gt;Nguy&amp;ecirc;n tắc 26 : GIữ thể diện cho người kh&amp;aacute;c&lt;/h1&gt;
&lt;p&gt;Giữ thể diện cho người kh&amp;aacute;c l&amp;agrave; một điều hết sức quan trọng. Ch&amp;igrave; cần suy nghĩ v&amp;agrave;i ph&amp;uacute;t, với v&amp;agrave;i lời n&amp;oacute;i &amp;acirc;n cần, th&amp;ocirc;ng cảm l&amp;agrave; ch&amp;uacute;ng ta đ&amp;atilde; tr&amp;aacute;nh được việc l&amp;agrave;m tổn thương người kh&amp;aacute;c v&amp;agrave; cũng l&amp;agrave; tr&amp;aacute;nh l&amp;agrave;m thương tổn ch&amp;iacute;nh nh&amp;acirc;n c&amp;aacute;ch của m&amp;igrave;nh. &amp;ldquo;T&amp;ocirc;i kh&amp;ocirc;ng c&amp;oacute; quyền l&amp;agrave;m giảm gi&amp;aacute; trị một người trong ch&amp;iacute;nh mắt người ấy. Điều quan trọng kh&amp;ocirc;ng phải l&amp;agrave; t&amp;ocirc;i nghĩ g&amp;igrave; về anh ta m&amp;agrave; l&amp;agrave; anh ta nghĩ g&amp;igrave; về ch&amp;iacute;nh m&amp;igrave;nh. L&amp;agrave;m thương tổn phẩm gi&amp;aacute; con người l&amp;agrave; một tội &amp;aacute;c.&amp;rdquo; Antoine de Saint Exupery&lt;/p&gt;
&lt;h1 id="nguyen-tac-27-nhin-nhan-su-no-luc-dong-gop-cua-nguoi-khac-mot-cach-cong-bang-cho-du-do-la-nhung-viec-nho-nhat-dong-vien-khuyen-khich-bang-su-chan-thanh-va-huong-ung-nhiet-tinh-cua-ban"&gt;Nguy&amp;ecirc;n tắc 27 : Nh&amp;igrave;n nhận sự nỗ lực đ&amp;oacute;ng g&amp;oacute;p của người kh&amp;aacute;c một c&amp;aacute;ch c&amp;ocirc;ng bằng cho d&amp;ugrave; đ&amp;oacute; l&amp;agrave; những việc nhỏ nhất . Động vi&amp;ecirc;n khuyến kh&amp;iacute;ch bằng sự ch&amp;acirc;n th&amp;agrave;nh v&amp;agrave; hưởng ứng nhiệt t&amp;igrave;nh của bạn&lt;/h1&gt;
&lt;p&gt;Lời khen như tia nắng mặt trời, n&amp;oacute; cần thiết cho mu&amp;ocirc;n lo&amp;agrave;i, trong đ&amp;oacute; c&amp;oacute; con người ph&amp;aacute;t triển. Vậy m&amp;agrave; hầu hết ch&amp;uacute;ng ta lu&amp;ocirc;n sẵn s&amp;agrave;ng sử dụng những l&amp;agrave;n gi&amp;oacute; lạnh như cắt để ph&amp;ecirc; ph&amp;aacute;n v&amp;agrave; thường ngần ngại khi tặng người th&amp;acirc;n của m&amp;igrave;nh những tia nắng ấm &amp;aacute;p từ những lời khen tặng. Mọi người đều muốn được khen, nhưng lời khen phải cụ thể r&amp;otilde; r&amp;agrave;ng , thể hiện sự ch&amp;acirc;n th&amp;agrave;nh chứ kh&amp;ocirc;ng phải l&amp;agrave; lời s&amp;aacute;o rỗng nghe cho &amp;ecirc;m tai. Ch&amp;uacute;ng ta đều khao kh&amp;aacute;t được t&amp;aacute;n thưởng, được thừa nhận, đều sẵn s&amp;agrave;ng l&amp;agrave;m bất cứ điều g&amp;igrave; để được như thế. Nhưng kh&amp;ocirc;ng ai muốn sự giả dối v&amp;agrave; nịnh h&amp;oacute;t. Mọi tiềm năng đều nở hoa trong ngợi khen v&amp;agrave; h&amp;eacute;o t&amp;agrave;n trong chỉ tr&amp;iacute;ch.&amp;ldquo;Nụ cười ấm &amp;aacute;p v&amp;agrave; một c&amp;aacute;i vỗ vai th&amp;acirc;n thiện của bạn c&amp;oacute; thể cứu một con người đang b&amp;ecirc;n bờ vực thẳm.&amp;rdquo; &amp;ndash; Camellia Elliot&lt;/p&gt;
&lt;h1 id="nguyen-tac-28-khen-ngoi-de-nguoi-khac-luon-co-gang-phan-dau-xung-dang-voi-loi-khen-do"&gt;Nguy&amp;ecirc;n tắc 28 : Khen ngợi để người kh&amp;aacute;c lu&amp;ocirc;n cố gắng phấn đấu xứng đ&amp;aacute;ng với lợi khen đ&amp;oacute;&lt;/h1&gt;
&lt;p&gt;Khen ngợi để người kh&amp;aacute;c lu&amp;ocirc;n phấn đấu xứng đ&amp;aacute;ng với lời khen đ&amp;oacute; Trong đời thường, cho người kh&amp;aacute;c một thanh danh l&amp;agrave; quan trọng; nhưng ph&amp;ecirc; b&amp;igrave;nh một người m&amp;agrave; vẫn giữ được danh dự cho người ấy c&amp;ograve;n quan trọng hơn gấp nhiều lần. Nếu muốn khuyến kh&amp;iacute;ch một điều g&amp;igrave; ở ai đ&amp;oacute;, bạn h&amp;atilde;y l&amp;agrave;m như điều ấy ch&amp;iacute;nh l&amp;agrave; đặc điểm vượt trội của người đ&amp;oacute;. Họ nhất định sẽ nỗ lực phi thường để trở n&amp;ecirc;n như thế. &amp;ldquo;Trong c&amp;aacute;ch đối nh&amp;acirc;n xử thế, nếu ta đối xử với một người như thế n&amp;agrave;o th&amp;igrave; anh ta sẽ trở th&amp;agrave;nh người như thế ấy.&amp;rdquo; &amp;ndash; Johann Wolfgang von Goethe&lt;/p&gt;
&lt;h1 id="nguyen-tac-29-khuyen-khich-nguoi-khac-lam-cho-ho-thay-sai-lam-khong-kho-de-chinh-sua"&gt;Nguy&amp;ecirc;n tắc 29 : Khuyến kh&amp;iacute;ch người kh&amp;aacute;c l&amp;agrave;m cho họ thấy sai lầm kh&amp;ocirc;ng kh&amp;oacute; để chỉnh sửa&lt;/h1&gt;
&lt;p&gt;Nếu bạn bảo con c&amp;aacute;i, vợ/chồng hay nh&amp;acirc;n vi&amp;ecirc;n của bạn rằng họ ngốc nghếch, vụng về hay bất t&amp;agrave;i&amp;hellip; nghĩa l&amp;agrave; bạn đ&amp;atilde; hủy diệt hầu hết mọi động cơ để họ tiến bộ. Nhưng nếu ngược lại, bạn khuyến kh&amp;iacute;ch, l&amp;agrave;m cho sự việc c&amp;oacute; vẻ dễ d&amp;agrave;ng hơn , thể hiện sự tin tưởng rằng họ c&amp;oacute; năng khiếu nhưng chưa được ph&amp;aacute;t triển th&amp;igrave; họ nhất định sẽ cố gắng ph&amp;aacute;t triển năng khiếu m&amp;agrave; bạn đ&amp;atilde; ph&amp;aacute;t hiện hay thậm ch&amp;iacute; l&amp;agrave; g&amp;aacute;n cho họ. &amp;ldquo;Ch&amp;uacute;ng ta kh&amp;ocirc;ng thể dạy bảo ai bất cứ điều g&amp;igrave;. Ch&amp;uacute;ng ta chỉ c&amp;oacute; thể gi&amp;uacute;p họ ph&amp;aacute;t hiện ra những g&amp;igrave; c&amp;ograve;n tiềm ẩn b&amp;ecirc;n trong con người họ.&amp;rdquo; &amp;ndash; Galileo&lt;/p&gt;
&lt;p&gt;L&amp;agrave;m cho người kh&amp;aacute;c cảm thấy vui vẻ thực hiện ch&amp;iacute;nh đề nghị của bạn&lt;/p&gt;
&lt;p&gt;Con người dễ c&amp;oacute; xu hướng thay đổi th&amp;aacute;i độ nếu ch&amp;uacute;ng ta l&amp;agrave;m cho người kh&amp;aacute;c vui th&amp;iacute;ch thực hiện điều m&amp;igrave;nh được gợi &amp;yacute;. Gợi &amp;yacute; một điều g&amp;igrave; đ&amp;oacute; thật kh&amp;eacute;o l&amp;eacute;o để người kh&amp;aacute;c thực hiện l&amp;agrave; cả một nghệ thuật. Nếu ch&amp;uacute;ng ta ki&amp;ecirc;n tr&amp;igrave; r&amp;egrave;n luyện v&amp;agrave; c&amp;oacute; sự quan t&amp;acirc;m ch&amp;acirc;n th&amp;agrave;nh đến người kh&amp;aacute;c, ch&amp;uacute;ng ta sẽ l&amp;agrave;m được. &amp;ldquo;Khi thực sự quan t&amp;acirc;m v&amp;agrave; đủ ki&amp;ecirc;n nhẫn để truyền đạt th&amp;igrave; bao giờ ch&amp;uacute;ng ta cũng nhận được những hưởng ứng t&amp;iacute;ch cực v&amp;agrave; thu được kết quả tốt nhất.&amp;rdquo; &amp;ndash; Charlotte Bronte&amp;ldquo;C&amp;oacute; những điểm cao tr&amp;agrave;o trong cuộc sống, v&amp;agrave; hầu hết ch&amp;uacute;ng đều đến từ sự kh&amp;iacute;ch lệ của một ai đ&amp;oacute;.&amp;rdquo; &amp;ndash; George Adams&lt;/p&gt;</content><category term="Đắc Nhân Tâm"></category><category term="OTHER"></category></entry><entry><title>Summary of Object Detection Papers</title><link href="/blog/tutorial/2018/summary-of-object-detection-papers/" rel="alternate"></link><published>2018-08-21T21:13:24+00:00</published><updated>2018-08-21T21:13:24+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/tutorial/2018/summary-of-object-detection-papers/</id><summary type="html">&lt;h1 id="object-detection-papers"&gt;Object Detection Papers&lt;/h1&gt;
&lt;p&gt;There are a list of papers in Computer Vision in general which provides some short summary of each paper. The link on github can be found &lt;a href="https://github.com/sunshineatnoon/Paper-Collection/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="deep-neural-networks-for-object-detection"&gt;Deep Neural Networks for Object Detection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;paper: http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="overfeat-integrated-recognition-localization-and-detection-using-convolutional-networks"&gt;OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1312.6229&lt;/li&gt;
&lt;li&gt;github: https://github.com/sermanet/OverFeat&lt;/li&gt;
&lt;li&gt;code: http://cilvr.nyu.edu/doku.php?id=software:overfeat:start&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="r-cnn_1"&gt;R-CNN&lt;/h1&gt;
&lt;h2 id="rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation"&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;intro: R-CNN&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1311.2524&lt;/li&gt;
&lt;li&gt;supp: http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf&lt;/li&gt;
&lt;li&gt;slides: http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf&lt;/li&gt;
&lt;li&gt;slides: http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf&lt;/li&gt;
&lt;li&gt;github: https://github.com/rbgirshick/rcnn&lt;/li&gt;
&lt;li&gt;notes: http://zhangliliang.com/2014/07/23/paper-note-rcnn/&lt;/li&gt;
&lt;li&gt;caffe-pr(&amp;ldquo;Make R-CNN the Caffe detection example&amp;rdquo;): https …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h1 id="object-detection-papers"&gt;Object Detection Papers&lt;/h1&gt;
&lt;p&gt;There are a list of papers in Computer Vision in general which provides some short summary of each paper. The link on github can be found &lt;a href="https://github.com/sunshineatnoon/Paper-Collection/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="deep-neural-networks-for-object-detection"&gt;Deep Neural Networks for Object Detection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;paper: http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="overfeat-integrated-recognition-localization-and-detection-using-convolutional-networks"&gt;OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1312.6229&lt;/li&gt;
&lt;li&gt;github: https://github.com/sermanet/OverFeat&lt;/li&gt;
&lt;li&gt;code: http://cilvr.nyu.edu/doku.php?id=software:overfeat:start&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="r-cnn_1"&gt;R-CNN&lt;/h1&gt;
&lt;h2 id="rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation"&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;intro: R-CNN&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1311.2524&lt;/li&gt;
&lt;li&gt;supp: http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf&lt;/li&gt;
&lt;li&gt;slides: http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf&lt;/li&gt;
&lt;li&gt;slides: http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf&lt;/li&gt;
&lt;li&gt;github: https://github.com/rbgirshick/rcnn&lt;/li&gt;
&lt;li&gt;notes: http://zhangliliang.com/2014/07/23/paper-note-rcnn/&lt;/li&gt;
&lt;li&gt;caffe-pr(&amp;ldquo;Make R-CNN the Caffe detection example&amp;rdquo;): https://github.com/BVLC/caffe/pull/482&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="fast-r-cnn_1"&gt;Fast R-CNN&lt;/h1&gt;
&lt;h2 id="fast-r-cnn_2"&gt;Fast R-CNN&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1504.08083&lt;/li&gt;
&lt;li&gt;slides: http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf&lt;/li&gt;
&lt;li&gt;github: https://github.com/rbgirshick/fast-rcnn&lt;/li&gt;
&lt;li&gt;github(COCO-branch): https://github.com/rbgirshick/fast-rcnn/tree/coco&lt;/li&gt;
&lt;li&gt;webcam demo: https://github.com/rbgirshick/fast-rcnn/pull/29&lt;/li&gt;
&lt;li&gt;notes: http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/&lt;/li&gt;
&lt;li&gt;notes: http://blog.csdn.net/linj_m/article/details/48930179&lt;/li&gt;
&lt;li&gt;github(&amp;ldquo;Fast R-CNN in MXNet&amp;rdquo;): https://github.com/precedenceguo/mx-rcnn&lt;/li&gt;
&lt;li&gt;github: https://github.com/mahyarnajibi/fast-rcnn-torch&lt;/li&gt;
&lt;li&gt;github: https://github.com/apple2373/chainer-simple-fast-rnn&lt;/li&gt;
&lt;li&gt;github: https://github.com/zplizzi/tensorflow-fast-rcnn&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="a-fast-rcnn-hard-positive-generation-via-adversary-for-object-detection"&gt;A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;intro: CVPR 2017&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1704.03414&lt;/li&gt;
&lt;li&gt;paper: http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf&lt;/li&gt;
&lt;li&gt;github(Caffe): https://github.com/xiaolonw/adversarial-frcnn&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="faster-r-cnn_1"&gt;Faster R-CNN&lt;/h1&gt;
&lt;h2 id="faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks"&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;intro: NIPS 2015&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1506.01497&lt;/li&gt;
&lt;li&gt;gitxiv: http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region&lt;/li&gt;
&lt;li&gt;slides: http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf&lt;/li&gt;
&lt;li&gt;github(official, Matlab): https://github.com/ShaoqingRen/faster_rcnn&lt;/li&gt;
&lt;li&gt;github: https://github.com/rbgirshick/py-faster-rcnn&lt;/li&gt;
&lt;li&gt;github(MXNet): https://github.com/msracver/Deformable-ConvNets/tree/master/faster_rcnn&lt;/li&gt;
&lt;li&gt;github: https://github.com//jwyang/faster-rcnn.pytorch&lt;/li&gt;
&lt;li&gt;github: https://github.com/mitmul/chainer-faster-rcnn&lt;/li&gt;
&lt;li&gt;github: https://github.com/andreaskoepf/faster-rcnn.torch&lt;/li&gt;
&lt;li&gt;github: https://github.com/ruotianluo/Faster-RCNN-Densecap-torch&lt;/li&gt;
&lt;li&gt;github: https://github.com/smallcorgi/Faster-RCNN_TF&lt;/li&gt;
&lt;li&gt;github: https://github.com/CharlesShang/TFFRCNN&lt;/li&gt;
&lt;li&gt;github(C++ demo): https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus&lt;/li&gt;
&lt;li&gt;github: https://github.com/yhenon/keras-frcnn&lt;/li&gt;
&lt;li&gt;github: https://github.com/Eniac-Xie/faster-rcnn-resnet&lt;/li&gt;
&lt;li&gt;github(C++): https://github.com/D-X-Y/caffe-faster-rcnn/tree/dev&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="r-cnn-minus-r"&gt;R-CNN minus R&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;intro: BMVC 2015&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1506.06981&lt;/li&gt;
&lt;li&gt;
&lt;h2 id="faster-r-cnn-in-mxnet-with-distributed-implementation-and-data-parallelization"&gt;Faster R-CNN in MXNet with distributed implementation and data parallelization&lt;/h2&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;github: https://github.com/dmlc/mxnet/tree/master/example/rcnn&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="contextual-priming-and-feedback-for-faster-r-cnn_1"&gt;Contextual Priming and Feedback for Faster R-CNN&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: ECCV 2016. Carnegie Mellon University&lt;/li&gt;
&lt;li&gt;paper: http://abhinavsh.info/context_priming_feedback.pdf&lt;/li&gt;
&lt;li&gt;poster: http://www.eccv2016.org/files/posters/P-1A-20.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="an-implementation-of-faster-rcnn-with-study-for-region-sampling"&gt;An Implementation of Faster RCNN with Study for Region Sampling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;intro: Technical Report, 3 pages. CMU&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1702.02138&lt;/li&gt;
&lt;li&gt;github: https://github.com/endernewton/tf-faster-rcnn&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="interpretable-r-cnn"&gt;Interpretable R-CNN&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;intro: North Carolina State University &amp;amp; Alibaba&lt;/li&gt;
&lt;li&gt;keywords: AND-OR Graph (AOG)&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1711.05226&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="light-head-r-cnn_1"&gt;Light-Head R-CNN&lt;/h1&gt;
&lt;h2 id="light-head-r-cnn-in-defense-of-two-stage-object-detector"&gt;Light-Head R-CNN: In Defense of Two-Stage Object Detector&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;intro: Tsinghua University &amp;amp; Megvii Inc&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1711.07264&lt;/li&gt;
&lt;li&gt;github(official, Tensorflow): https://github.com/zengarden/light_head_rcnn&lt;/li&gt;
&lt;li&gt;github: https://github.com/terrychenism/Deformable-ConvNets/blob/master/rfcn/symbols/resnet_v1_101_rfcn_light.py#L784&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="cascade-r-cnn_1"&gt;Cascade R-CNN&lt;/h1&gt;
&lt;h2 id="cascade-r-cnn-delving-into-high-quality-object-detection"&gt;Cascade R-CNN: Delving into High Quality Object Detection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;intro: CVPR 2018. UC San Diego&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1712.00726&lt;/li&gt;
&lt;li&gt;github(Caffe, official): https://github.com/zhaoweicai/cascade-rcnn&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="multibox-scalable-object-detection-using-deep-neural-networks_1"&gt;MultiBox Scalable Object Detection using Deep Neural Networks&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: first MultiBox. Train a CNN to predict Region of Interest.&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1312.2249&lt;/li&gt;
&lt;li&gt;github: https://github.com/google/multibox&lt;/li&gt;
&lt;li&gt;blog: https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="scalable-high-quality-object-detection"&gt;Scalable, High-Quality Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: second MultiBox&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1412.1441&lt;/li&gt;
&lt;li&gt;github: https://github.com/google/multibox&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="spp-net-spatial-pyramid-pooling-in-deep-convolutional-networks-for-visual-recognition"&gt;SPP-Net Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: ECCV 2014 / TPAMI 2015&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1406.4729&lt;/li&gt;
&lt;li&gt;github: https://github.com/ShaoqingRen/SPP_net&lt;/li&gt;
&lt;li&gt;notes: http://zhangliliang.com/2014/09/13/paper-note-sppnet/&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="deepid-net-deformable-deep-convolutional-neural-networks-for-object-detection"&gt;DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: PAMI 2016&lt;/li&gt;
&lt;li&gt;intro: an extension of R-CNN. box pre-training, cascade on region proposals, deformation layers and context representations&lt;/li&gt;
&lt;li&gt;project page: http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1412.5661&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="object-detectors-emerge-in-deep-scene-cnns"&gt;Object Detectors Emerge in Deep Scene CNNs&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: ICLR 2015&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1412.6856&lt;/li&gt;
&lt;li&gt;paper: https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf&lt;/li&gt;
&lt;li&gt;paper: https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf&lt;/li&gt;
&lt;li&gt;slides: http://places.csail.mit.edu/slide_iclr2015.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="segdeepm-exploiting-segmentation-and-context-in-deep-neural-networks-for-object-detection"&gt;segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: CVPR 2015&lt;/li&gt;
&lt;li&gt;project(code+data): https://www.cs.toronto.edu/~yukun/segdeepm.html&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1502.04275&lt;/li&gt;
&lt;li&gt;github: https://github.com/YknZhu/segDeepM&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="object-detection-networks-on-convolutional-feature-maps"&gt;Object Detection Networks on Convolutional Feature Maps&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: TPAMI 2015&lt;/li&gt;
&lt;li&gt;keywords: NoC&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1504.06066&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="improving-object-detection-with-deep-convolutional-networks-via-bayesian-optimization-and-structured-prediction"&gt;Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1504.03293&lt;/li&gt;
&lt;li&gt;slides: http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf&lt;/li&gt;
&lt;li&gt;github: https://github.com/YutingZhang/fgs-obj&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="deepbox-learning-objectness-with-convolutional-networks"&gt;DeepBox: Learning Objectness with Convolutional Networks&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;keywords: DeepBox&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1505.02146&lt;/li&gt;
&lt;li&gt;github: https://github.com/weichengkuo/DeepBox&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="mr-cnn-object-detection-via-a-multi-region-semantic-segmentation-aware-cnn-model"&gt;MR-CNN Object detection via a multi-region &amp;amp; semantic segmentation-aware CNN model&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: ICCV 2015. MR-CNN&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1505.01749&lt;/li&gt;
&lt;li&gt;github: https://github.com/gidariss/mrcnn-object-detection&lt;/li&gt;
&lt;li&gt;notes: http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/&lt;/li&gt;
&lt;li&gt;notes: http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="yolo-you-only-look-once-unified-real-time-object-detection"&gt;YOLO You Only Look Once: Unified, Real-Time Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1506.02640&lt;/li&gt;
&lt;li&gt;code: http://pjreddie.com/darknet/yolo/&lt;/li&gt;
&lt;li&gt;github: https://github.com/pjreddie/darknet&lt;/li&gt;
&lt;li&gt;blog: https://pjreddie.com/publications/yolo/&lt;/li&gt;
&lt;li&gt;slides: https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&amp;amp;loop=false&amp;amp;delayms=3000&amp;amp;slide=id.p&lt;/li&gt;
&lt;li&gt;reddit: https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/&lt;/li&gt;
&lt;li&gt;github: https://github.com/gliese581gg/YOLO_tensorflow&lt;/li&gt;
&lt;li&gt;github: https://github.com/xingwangsfu/caffe-yolo&lt;/li&gt;
&lt;li&gt;github: https://github.com/frankzhangrui/Darknet-Yolo&lt;/li&gt;
&lt;li&gt;github: https://github.com/BriSkyHekun/py-darknet-yolo&lt;/li&gt;
&lt;li&gt;github: https://github.com/tommy-qichang/yolo.torch&lt;/li&gt;
&lt;li&gt;github: https://github.com/frischzenger/yolo-windows&lt;/li&gt;
&lt;li&gt;github: https://github.com/AlexeyAB/yolo-windows&lt;/li&gt;
&lt;li&gt;github: https://github.com/nilboy/tensorflow-yolo&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;darkflow - translate darknet to tensorflow. Load trained weights, retrain/fine-tune them using tensorflow, export constant graph def to C++&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;blog: https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;github: https://github.com/thtrieu/darkflow&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="start-training-yolo-with-our-own-data"&gt;Start Training YOLO with Our Own Data&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: train with customized data and class numbers/labels. Linux / Windows version for darknet.&lt;/li&gt;
&lt;li&gt;blog: http://guanghan.info/blog/en/my-works/train-yolo/&lt;/li&gt;
&lt;li&gt;github: https://github.com/Guanghan/darknet&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="yolo-core-ml-versus-mpsnngraph"&gt;YOLO: Core ML versus MPSNNGraph&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: Tiny YOLO for iOS implemented using CoreML but also using the new MPS graph API.&lt;/li&gt;
&lt;li&gt;blog: http://machinethink.net/blog/yolo-coreml-versus-mps-graph/&lt;/li&gt;
&lt;li&gt;github: https://github.com/hollance/YOLO-CoreML-MPSNNGraph&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="tensorflow-yolo-object-detection-on-android"&gt;TensorFlow YOLO object detection on Android&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: Real-time object detection on Android using the YOLO network with TensorFlow&lt;/li&gt;
&lt;li&gt;github: https://github.com/natanielruiz/android-yolo&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="computer-vision-in-ios-object-detection"&gt;Computer Vision in iOS &amp;ndash; Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;blog: https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/&lt;/li&gt;
&lt;li&gt;github:https://github.com/r4ghu/iOS-CoreML-Yolo&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="yolov2"&gt;YOLOv2&lt;/h1&gt;
&lt;h1 id="yolo9000-better-faster-stronger"&gt;YOLO9000: Better, Faster, Stronger&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1612.08242&lt;/li&gt;
&lt;li&gt;code: http://pjreddie.com/yolo9000/&lt;/li&gt;
&lt;li&gt;github(Chainer): https://github.com/leetenki/YOLOv2&lt;/li&gt;
&lt;li&gt;github(Keras): https://github.com/allanzelener/YAD2K&lt;/li&gt;
&lt;li&gt;github(PyTorch): https://github.com/longcw/yolo2-pytorch&lt;/li&gt;
&lt;li&gt;github(Tensorflow): https://github.com/hizhangp/yolo_tensorflow&lt;/li&gt;
&lt;li&gt;github(Windows): https://github.com/AlexeyAB/darknet&lt;/li&gt;
&lt;li&gt;github: https://github.com/choasUp/caffe-yolo9000&lt;/li&gt;
&lt;li&gt;github: https://github.com/philipperemy/yolo-9000&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;darknet_scripts&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;intro: Auxilary scripts to work with (YOLO) darknet deep learning famework. AKA -&amp;gt; How to generate YOLO anchors?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;github: https://github.com/Jumabek/darknet_scripts&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="yolo_mark-gui-for-marking-bounded-boxes-of-objects-in-images-for-training-yolo-v2"&gt;Yolo_mark: GUI for marking bounded boxes of objects in images for training Yolo v2&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;github: https://github.com/AlexeyAB/Yolo_mark&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="lightnet-bringing-pjreddies-darknet-out-of-the-shadows"&gt;LightNet: Bringing pjreddie&amp;rsquo;s DarkNet out of the shadows&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;https://github.com//explosion/lightnet&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="yolo-v2-bounding-box-tool"&gt;YOLO v2 Bounding Box Tool&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: Bounding box labeler tool to generate the training data in the format YOLO v2 requires.&lt;/li&gt;
&lt;li&gt;github: https://github.com/Cartucho/yolo-boundingbox-labeler-GUI&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="yolov3-yolov3-an-incremental-improvement"&gt;YOLOv3 YOLOv3: An Incremental Improvement&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;project page: https://pjreddie.com/darknet/yolo/&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1804.02767&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="attentionnet-aggregating-weak-directions-for-accurate-object-detection"&gt;AttentionNet: Aggregating Weak Directions for Accurate Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: ICCV 2015&lt;/li&gt;
&lt;li&gt;intro: state-of-the-art performance of 65% (AP) on PASCAL VOC 2007&amp;frasl;2012 human detection task&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1506.07704&lt;/li&gt;
&lt;li&gt;slides: https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf&lt;/li&gt;
&lt;li&gt;slides: http://image-net.org/challenges/talks/lunit-kaist-slide.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="densebox-unifying-landmark-localization-with-end-to-end-object-detection"&gt;DenseBox: Unifying Landmark Localization with End to End Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1509.04874&lt;/li&gt;
&lt;li&gt;demo: http://pan.baidu.com/s/1mgoWWsS&lt;/li&gt;
&lt;li&gt;KITTI result: http://www.cvlibs.net/datasets/kitti/eval_object.php&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="ssd-single-shot-multibox-detector"&gt;SSD: Single Shot MultiBox Detector&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: ECCV 2016 Oral&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1512.02325&lt;/li&gt;
&lt;li&gt;paper: http://www.cs.unc.edu/~wliu/papers/ssd.pdf&lt;/li&gt;
&lt;li&gt;slides: http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf&lt;/li&gt;
&lt;li&gt;github(Official): https://github.com/weiliu89/caffe/tree/ssd&lt;/li&gt;
&lt;li&gt;video: http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973&lt;/li&gt;
&lt;li&gt;github: https://github.com/zhreshold/mxnet-ssd&lt;/li&gt;
&lt;li&gt;github: https://github.com/zhreshold/mxnet-ssd.cpp&lt;/li&gt;
&lt;li&gt;github: https://github.com/rykov8/ssd_keras&lt;/li&gt;
&lt;li&gt;github: https://github.com/balancap/SSD-Tensorflow&lt;/li&gt;
&lt;li&gt;github: https://github.com/amdegroot/ssd.pytorch&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;github(Caffe): https://github.com/chuanqi305/MobileNet-SSD&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What&amp;rsquo;s the diffience in performance between this new code you pushed and the previous code? #327&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;https://github.com/weiliu89/caffe/issues/327&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="dssd-deconvolutional-single-shot-detector"&gt;DSSD : Deconvolutional Single Shot Detector&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: UNC Chapel Hill &amp;amp; Amazon Inc&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1701.06659&lt;/li&gt;
&lt;li&gt;github: https://github.com/chengyangfu/caffe/tree/dssd&lt;/li&gt;
&lt;li&gt;github: https://github.com/MTCloudVision/mxnet-dssd&lt;/li&gt;
&lt;li&gt;demo: http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="enhancement-of-ssd-by-concatenating-feature-maps-for-object-detection"&gt;Enhancement of SSD by concatenating feature maps for object detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: rainbow SSD (R-SSD)&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1705.09587&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="context-aware-single-shot-detector"&gt;Context-aware Single-Shot Detector&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;keywords: CSSD, DiCSSD, DeCSSD, effective receptive fields (ERFs), theoretical receptive fields (TRFs)&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1707.08682&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="feature-fused-ssd-fast-detection-for-small-objects"&gt;Feature-Fused SSD: Fast Detection for Small Objects&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;https://arxiv.org/abs/1709.05054&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="fssd-feature-fusion-single-shot-multibox-detector"&gt;FSSD: Feature Fusion Single Shot Multibox Detector&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;https://arxiv.org/abs/1712.00960&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="weaving-multi-scale-context-for-single-shot-detector"&gt;Weaving Multi-scale Context for Single Shot Detector&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: WeaveNet&lt;/li&gt;
&lt;li&gt;keywords: fuse multi-scale information&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1712.03149&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="essd-extend-the-shallow-part-of-single-shot-multibox-detector-via-convolutional-neural-network"&gt;ESSD Extend the shallow part of Single Shot MultiBox Detector via Convolutional Neural Network&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;https://arxiv.org/abs/1801.05918&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="tiny-ssd-a-tiny-single-shot-detection-deep-convolutional-neural-network-for-real-time-embedded-object-detection"&gt;Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;https://arxiv.org/abs/1802.06488&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="mdssd-multi-scale-deconvolutional-single-shot-detector-for-small-objects"&gt;MDSSD: Multi-scale Deconvolutional Single Shot Detector for small objects&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: Zhengzhou University&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1805.07009&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="inside-outside-net-detecting-objects-in-context-with-skip-pooling-and-recurrent-neural-networks"&gt;Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: &amp;ldquo;0.8s per image on a Titan X GPU (excluding proposal generation) without two-stage bounding-box regression and 1.15s per image with it&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1512.04143&lt;/li&gt;
&lt;li&gt;slides: http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf&lt;/li&gt;
&lt;li&gt;coco-leaderboard: http://mscoco.org/dataset/#detections-leaderboard&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="adaptive-object-detection-using-adjacency-and-zoom-prediction"&gt;Adaptive Object Detection Using Adjacency and Zoom Prediction&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: CVPR 2016. AZ-Net&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1512.07711&lt;/li&gt;
&lt;li&gt;github: https://github.com/luyongxi/az-net&lt;/li&gt;
&lt;li&gt;youtube: https://www.youtube.com/watch?v=YmFtuNwxaNM&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;G-CNN: an Iterative Grid Based Object Detector&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;arxiv: http://arxiv.org/abs/1512.07729&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="factors-in-finetuning-deep-model-for-object-detection"&gt;Factors in Finetuning Deep Model for object detection&lt;/h1&gt;
&lt;h1 id="factors-in-finetuning-deep-model-for-object-detection-with-long-tail-distribution"&gt;Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2016.rank 3rd for provided data and 2nd for external data on ILSVRC 2015 object detection
project page: http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html
arxiv: http://arxiv.org/abs/1601.05150&lt;/p&gt;
&lt;h1 id="we-dont-need-no-bounding-boxes-training-object-class-detectors-using-only-human-verification"&gt;We don&amp;rsquo;t need no bounding-boxes: Training object class detectors using only human verification&lt;/h1&gt;
&lt;p&gt;arxiv: http://arxiv.org/abs/1602.08405&lt;/p&gt;
&lt;h1 id="hypernet-towards-accurate-region-proposal-generation-and-joint-object-detection"&gt;HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection&lt;/h1&gt;
&lt;p&gt;arxiv: http://arxiv.org/abs/1604.00600&lt;/p&gt;
&lt;h1 id="a-multipath-network-for-object-detection"&gt;A MultiPath Network for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: BMVC 2016. Facebook AI Research (FAIR)
arxiv: http://arxiv.org/abs/1604.02135
github: https://github.com/facebookresearch/multipathnet&lt;/p&gt;
&lt;h1 id="craft-objects-from-images"&gt;CRAFT Objects from Images&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2016. Cascade Region-proposal-network And FasT-rcnn. an extension of Faster R-CNN
project page: http://byangderek.github.io/projects/craft.html
arxiv: https://arxiv.org/abs/1604.03239
paper: http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf
github: https://github.com/byangderek/CRAFT&lt;/p&gt;
&lt;h1 id="ohem-training-region-based-object-detectors-with-online-hard-example-mining"&gt;OHEM Training Region-based Object Detectors with Online Hard Example Mining&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2016 Oral. Online hard example mining (OHEM)
arxiv: http://arxiv.org/abs/1604.03540
paper: http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf
github(Official): https://github.com/abhi2610/ohem
author page: http://abhinav-shrivastava.info/&lt;/p&gt;
&lt;h1 id="s-ohem-stratified-online-hard-example-mining-for-object-detection"&gt;S-OHEM: Stratified Online Hard Example Mining for Object Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1705.02233&lt;/p&gt;
&lt;h1 id="exploit-all-the-layers-fast-and-accurate-cnn-object-detector-with-scale-dependent-pooling-and-cascaded-rejection-classifiers"&gt;Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2016
keywords: scale-dependent pooling (SDP), cascaded rejection classifiers (CRC)
paper: http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf&lt;/p&gt;
&lt;h1 id="r-fcn-object-detection-via-region-based-fully-convolutional-networks"&gt;R-FCN: Object Detection via Region-based Fully Convolutional Networks&lt;/h1&gt;
&lt;p&gt;arxiv: http://arxiv.org/abs/1605.06409
github: https://github.com/daijifeng001/R-FCN
github(MXNet): https://github.com/msracver/Deformable-ConvNets/tree/master/rfcn
github: https://github.com/Orpine/py-R-FCN
github: https://github.com/PureDiors/pytorch_RFCN
github: https://github.com/bharatsingh430/py-R-FCN-multiGPU
github: https://github.com/xdever/RFCN-tensorflow&lt;/p&gt;
&lt;h1 id="r-fcn-3000-at-30fps-decoupling-detection-and-classification"&gt;R-FCN-3000 at 30fps: Decoupling Detection and Classification&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1712.01802&lt;/p&gt;
&lt;h1 id="recycle-deep-features-for-better-object-detection"&gt;Recycle deep features for better object detection&lt;/h1&gt;
&lt;p&gt;arxiv: http://arxiv.org/abs/1607.05066&lt;/p&gt;
&lt;h1 id="ms-cnn-a-unified-multi-scale-deep-convolutional-neural-network-for-fast-object-detection"&gt;MS-CNN A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection&lt;/h1&gt;
&lt;p&gt;intro: ECCV 2016
intro: 640&amp;times;480: 15 fps, 960&amp;times;720: 8 fps
arxiv: http://arxiv.org/abs/1607.07155
github: https://github.com/zhaoweicai/mscnn
poster: http://www.eccv2016.org/files/posters/P-2B-38.pdf&lt;/p&gt;
&lt;h1 id="multi-stage-object-detection-with-group-recursive-learning"&gt;Multi-stage Object Detection with Group Recursive Learning&lt;/h1&gt;
&lt;p&gt;intro: VOC2007: 78.6%, VOC2012: 74.9%
arxiv: http://arxiv.org/abs/1608.05159&lt;/p&gt;
&lt;h1 id="subcategory-aware-convolutional-neural-networks-for-object-proposals-and-detection"&gt;Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection&lt;/h1&gt;
&lt;p&gt;intro: WACV 2017. SubCNN
arxiv: http://arxiv.org/abs/1604.04693
github: https://github.com/tanshen/SubCNN&lt;/p&gt;
&lt;h1 id="pvanet-lightweight-deep-neural-networks-for-real-time-object-detection"&gt;PVANet: Lightweight Deep Neural Networks for Real-time Object Detection&lt;/h1&gt;
&lt;p&gt;intro: Presented at NIPS 2016 Workshop on Efficient Methods for Deep Neural Networks (EMDNN). Continuation of arXiv:1608.08021
arxiv: https://arxiv.org/abs/1611.08588
github: https://github.com/sanghoon/pva-faster-rcnn
leaderboard(PVANet 9.0): http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;amp;compid=4&lt;/p&gt;
&lt;h1 id="gated-bi-directional-cnn-for-object-detection"&gt;Gated Bi-directional CNN for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: The Chinese University of Hong Kong &amp;amp; Sensetime Group Limited
paper: http://link.springer.com/chapter/10.1007&amp;frasl;978-3-319-46478-7_22
mirror: https://pan.baidu.com/s/1dFohO7v&lt;/p&gt;
&lt;h1 id="crafting-gbd-net-for-object-detection"&gt;Crafting GBD-Net for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: winner of the ImageNet object detection challenge of 2016. CUImage and CUVideo
intro: gated bi-directional CNN (GBD-Net)
arxiv: https://arxiv.org/abs/1610.02579
github: https://github.com/craftGBD/craftGBD&lt;/p&gt;
&lt;h1 id="stuffnet-using-stuff-to-improve-object-detection"&gt;StuffNet: Using &amp;lsquo;Stuff&amp;rsquo; to Improve Object Detection&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1610.05861&lt;/p&gt;
&lt;h1 id="generalized-haar-filter-based-deep-networks-for-real-time-object-detection-in-traffic-scene"&gt;Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1610.09609&lt;/p&gt;
&lt;h1 id="hierarchical-object-detection-with-deep-reinforcement-learning"&gt;Hierarchical Object Detection with Deep Reinforcement Learning&lt;/h1&gt;
&lt;p&gt;intro: Deep Reinforcement Learning Workshop (NIPS 2016)
project page: https://imatge-upc.github.io/detection-2016-nipsws/
arxiv: https://arxiv.org/abs/1611.03718
slides: http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning
github: https://github.com/imatge-upc/detection-2016-nipsws
blog: http://jorditorres.org/nips/&lt;/p&gt;
&lt;h1 id="learning-to-detect-and-localize-many-objects-from-few-examples"&gt;Learning to detect and localize many objects from few examples&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1611.05664&lt;/p&gt;
&lt;h1 id="speedaccuracy-trade-offs-for-modern-convolutional-object-detectors"&gt;Speed/accuracy trade-offs for modern convolutional object detectors&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2017. Google Research
arxiv: https://arxiv.org/abs/1611.10012&lt;/p&gt;
&lt;h1 id="squeezedet-unified-small-low-power-fully-convolutional-neural-networks-for-real-time-object-detection-for-autonomous-driving"&gt;SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1612.01051
github: https://github.com/BichenWuUCB/squeezeDet
github: https://github.com/fregu856/2D_detection&lt;/p&gt;
&lt;h1 id="feature-pyramid-network-fpn"&gt;Feature Pyramid Network (FPN)&lt;/h1&gt;
&lt;p&gt;Feature Pyramid Networks for Object Detection&lt;/p&gt;
&lt;p&gt;intro: Facebook AI Research
arxiv: https://arxiv.org/abs/1612.03144&lt;/p&gt;
&lt;h1 id="action-driven-object-detection-with-top-down-visual-attentions"&gt;Action-Driven Object Detection with Top-Down Visual Attentions&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1612.06704&lt;/p&gt;
&lt;h1 id="beyond-skip-connections-top-down-modulation-for-object-detection"&gt;Beyond Skip Connections: Top-Down Modulation for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: CMU &amp;amp; UC Berkeley &amp;amp; Google Research
arxiv: https://arxiv.org/abs/1612.06851&lt;/p&gt;
&lt;h1 id="wide-residual-inception-networks-for-real-time-object-detection"&gt;Wide-Residual-Inception Networks for Real-time Object Detection&lt;/h1&gt;
&lt;p&gt;intro: Inha University
arxiv: https://arxiv.org/abs/1702.01243&lt;/p&gt;
&lt;h1 id="attentional-network-for-visual-object-detection"&gt;Attentional Network for Visual Object Detection&lt;/h1&gt;
&lt;p&gt;intro: University of Maryland &amp;amp; Mitsubishi Electric Research Laboratories
arxiv: https://arxiv.org/abs/1702.01478&lt;/p&gt;
&lt;h1 id="learning-chained-deep-features-and-classifiers-for-cascade-in-object-detection"&gt;Learning Chained Deep Features and Classifiers for Cascade in Object Detection&lt;/h1&gt;
&lt;p&gt;keykwords: CC-Net
intro: chained cascade network (CC-Net). 81.1% mAP on PASCAL VOC 2007
arxiv: https://arxiv.org/abs/1702.07054&lt;/p&gt;
&lt;h1 id="denet-scalable-real-time-object-detection-with-directed-sparse-sampling"&gt;DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling&lt;/h1&gt;
&lt;p&gt;intro: ICCV 2017 (poster)
arxiv: https://arxiv.org/abs/1703.10295&lt;/p&gt;
&lt;h1 id="discriminative-bimodal-networks-for-visual-localization-and-detection-with-natural-language-queries"&gt;Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2017
arxiv: https://arxiv.org/abs/1704.03944&lt;/p&gt;
&lt;h1 id="spatial-memory-for-context-reasoning-in-object-detection"&gt;Spatial Memory for Context Reasoning in Object Detection&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1704.04224&lt;/p&gt;
&lt;h1 id="accurate-single-stage-detector-using-recurrent-rolling-convolution"&gt;Accurate Single Stage Detector Using Recurrent Rolling Convolution&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2017. SenseTime
keywords: Recurrent Rolling Convolution (RRC)
arxiv: https://arxiv.org/abs/1704.05776
github: https://github.com/xiaohaoChen/rrc_detection&lt;/p&gt;
&lt;h1 id="deep-occlusion-reasoning-for-multi-camera-multi-target-detection"&gt;Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1704.05775&lt;/p&gt;
&lt;h1 id="lcdet-low-complexity-fully-convolutional-neural-networks-for-object-detection-in-embedded-systems"&gt;LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems&lt;/h1&gt;
&lt;p&gt;intro: Embedded Vision Workshop in CVPR. UC San Diego &amp;amp; Qualcomm Inc
arxiv: https://arxiv.org/abs/1705.05922&lt;/p&gt;
&lt;h1 id="point-linking-network-for-object-detection"&gt;Point Linking Network for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: Point Linking Network (PLN)
arxiv: https://arxiv.org/abs/1706.03646&lt;/p&gt;
&lt;h1 id="perceptual-generative-adversarial-networks-for-small-object-detection"&gt;Perceptual Generative Adversarial Networks for Small Object Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1706.05274&lt;/p&gt;
&lt;h1 id="few-shot-object-detection"&gt;Few-shot Object Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1706.08249&lt;/p&gt;
&lt;h1 id="yes-net-an-effective-detector-based-on-global-information"&gt;Yes-Net: An effective Detector Based on Global Information&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1706.09180&lt;/p&gt;
&lt;h1 id="smc-faster-r-cnn-toward-a-scene-specialized-multi-object-detector"&gt;SMC Faster R-CNN: Toward a scene-specialized multi-object detector&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1706.10217&lt;/p&gt;
&lt;h1 id="towards-lightweight-convolutional-neural-networks-for-object-detection"&gt;Towards lightweight convolutional neural networks for object detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1707.01395&lt;/p&gt;
&lt;h1 id="ron-reverse-connection-with-objectness-prior-networks-for-object-detection"&gt;RON: Reverse Connection with Objectness Prior Networks for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2017
arxiv: https://arxiv.org/abs/1707.01691
github: https://github.com/taokong/RON&lt;/p&gt;
&lt;h1 id="mimicking-very-efficient-network-for-object-detection"&gt;Mimicking Very Efficient Network for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2017. SenseTime &amp;amp; Beihang University
paper: http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf&lt;/p&gt;
&lt;h1 id="residual-features-and-unified-prediction-network-for-single-stage-detection"&gt;Residual Features and Unified Prediction Network for Single Stage Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1707.05031&lt;/p&gt;
&lt;h1 id="deformable-part-based-fully-convolutional-network-for-object-detection"&gt;Deformable Part-based Fully Convolutional Network for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: BMVC 2017 (oral). Sorbonne Universit&amp;eacute;s &amp;amp; CEDRIC
arxiv: https://arxiv.org/abs/1707.06175&lt;/p&gt;
&lt;h1 id="adaptive-feeding-achieving-fast-and-accurate-detections-by-adaptively-combining-object-detectors"&gt;Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors&lt;/h1&gt;
&lt;p&gt;intro: ICCV 2017
arxiv: https://arxiv.org/abs/1707.06399&lt;/p&gt;
&lt;h1 id="recurrent-scale-approximation-for-object-detection-in-cnn"&gt;Recurrent Scale Approximation for Object Detection in CNN&lt;/h1&gt;
&lt;p&gt;intro: ICCV 2017
keywords: Recurrent Scale Approximation (RSA)
arxiv: https://arxiv.org/abs/1707.09531
github: https://github.com/sciencefans/RSA-for-object-detection&lt;/p&gt;
&lt;h1 id="dsod-learning-deeply-supervised-object-detectors-from-scratch"&gt;DSOD: Learning Deeply Supervised Object Detectors from Scratch&lt;/h1&gt;
&lt;p&gt;intro: ICCV 2017. Fudan University &amp;amp; Tsinghua University &amp;amp; Intel Labs China
arxiv: https://arxiv.org/abs/1708.01241
github: https://github.com/szq0214/DSOD&lt;/p&gt;
&lt;h1 id="focal-loss-for-dense-object-detection"&gt;Focal Loss for Dense Object Detection&lt;/h1&gt;
&lt;p&gt;intro: ICCV 2017 Best student paper award. Facebook AI Research
keywords: RetinaNet
arxiv: https://arxiv.org/abs/1708.02002&lt;/p&gt;
&lt;h1 id="focal-loss-dense-detector-for-vehicle-surveillance"&gt;Focal Loss Dense Detector for Vehicle Surveillance&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1803.01114&lt;/p&gt;
&lt;h1 id="couplenet-coupling-global-structure-with-local-parts-for-object-detection"&gt;CoupleNet: Coupling Global Structure with Local Parts for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: ICCV 2017
arxiv: https://arxiv.org/abs/1708.02863&lt;/p&gt;
&lt;h1 id="incremental-learning-of-object-detectors-without-catastrophic-forgetting"&gt;Incremental Learning of Object Detectors without Catastrophic Forgetting&lt;/h1&gt;
&lt;p&gt;intro: ICCV 2017. Inria
arxiv: https://arxiv.org/abs/1708.06977&lt;/p&gt;
&lt;h1 id="zoom-out-and-in-network-with-map-attention-decision-for-region-proposal-and-object-detection"&gt;Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1709.04347&lt;/p&gt;
&lt;h1 id="stairnet-top-down-semantic-aggregation-for-accurate-one-shot-detection"&gt;StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1709.05788&lt;/p&gt;
&lt;h1 id="dynamic-zoom-in-network-for-fast-object-detection-in-large-images"&gt;Dynamic Zoom-in Network for Fast Object Detection in Large Images&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1711.05187&lt;/p&gt;
&lt;h1 id="zero-annotation-object-detection-with-web-knowledge-transfer"&gt;Zero-Annotation Object Detection with Web Knowledge Transfer&lt;/h1&gt;
&lt;p&gt;intro: NTU, Singapore &amp;amp; Amazon
keywords: multi-instance multi-label domain adaption learning framework
arxiv: https://arxiv.org/abs/1711.05954&lt;/p&gt;
&lt;h1 id="megdet-a-large-mini-batch-object-detector"&gt;MegDet: A Large Mini-Batch Object Detector&lt;/h1&gt;
&lt;p&gt;intro: Peking University &amp;amp; Tsinghua University &amp;amp; Megvii Inc
arxiv: https://arxiv.org/abs/1711.07240&lt;/p&gt;
&lt;h1 id="single-shot-refinement-neural-network-for-object-detection"&gt;Single-Shot Refinement Neural Network for Object Detection&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1711.06897
github: https://github.com/sfzhang15/RefineDet&lt;/p&gt;
&lt;h1 id="receptive-field-block-net-for-accurate-and-fast-object-detection"&gt;Receptive Field Block Net for Accurate and Fast Object Detection&lt;/h1&gt;
&lt;p&gt;intro: RFBNet
arxiv: https://arxiv.org/abs/1711.07767
github: https://github.com//ruinmessi/RFBNet&lt;/p&gt;
&lt;h1 id="an-analysis-of-scale-invariance-in-object-detection-snip"&gt;An Analysis of Scale Invariance in Object Detection - SNIP&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1711.08189
github: https://github.com/bharatsingh430/snip&lt;/p&gt;
&lt;h1 id="feature-selective-networks-for-object-detection"&gt;Feature Selective Networks for Object Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1711.08879&lt;/p&gt;
&lt;h1 id="learning-a-rotation-invariant-detector-with-rotatable-bounding-box"&gt;Learning a Rotation Invariant Detector with Rotatable Bounding Box&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1711.09405
github(official, Caffe): https://github.com/liulei01/DRBox&lt;/p&gt;
&lt;h1 id="scalable-object-detection-for-stylized-objects"&gt;Scalable Object Detection for Stylized Objects&lt;/h1&gt;
&lt;p&gt;intro: Microsoft AI &amp;amp; Research Munich
arxiv: https://arxiv.org/abs/1711.09822&lt;/p&gt;
&lt;h1 id="learning-object-detectors-from-scratch-with-gated-recurrent-feature-pyramids"&gt;Learning Object Detectors from Scratch with Gated Recurrent Feature Pyramids&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1712.00886
github: https://github.com/szq0214/GRP-DSOD&lt;/p&gt;
&lt;h1 id="deep-regionlets-for-object-detection"&gt;Deep Regionlets for Object Detection&lt;/h1&gt;
&lt;p&gt;keywords: region selection network, gating network
arxiv: https://arxiv.org/abs/1712.02408&lt;/p&gt;
&lt;h1 id="training-and-testing-object-detectors-with-virtual-images"&gt;Training and Testing Object Detectors with Virtual Images&lt;/h1&gt;
&lt;p&gt;intro: IEEE/CAA Journal of Automatica Sinica
arxiv: https://arxiv.org/abs/1712.08470&lt;/p&gt;
&lt;h1 id="large-scale-object-discovery-and-detector-adaptation-from-unlabeled-video"&gt;Large-Scale Object Discovery and Detector Adaptation from Unlabeled Video&lt;/h1&gt;
&lt;p&gt;keywords: object mining, object tracking, unsupervised object discovery by appearance-based clustering, self-supervised detector adaptation
arxiv: https://arxiv.org/abs/1712.08832&lt;/p&gt;
&lt;h1 id="spot-the-difference-by-object-detection"&gt;Spot the Difference by Object Detection&lt;/h1&gt;
&lt;p&gt;intro: Tsinghua University &amp;amp; JD Group
arxiv: https://arxiv.org/abs/1801.01051&lt;/p&gt;
&lt;h1 id="localization-aware-active-learning-for-object-detection"&gt;Localization-Aware Active Learning for Object Detection&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1801.05124&lt;/p&gt;
&lt;h1 id="object-detection-with-mask-based-feature-encoding"&gt;Object Detection with Mask-based Feature Encoding&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1802.03934&lt;/p&gt;
&lt;h1 id="lstd-a-low-shot-transfer-detector-for-object-detection"&gt;LSTD: A Low-Shot Transfer Detector for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: AAAI 2018
arxiv: https://arxiv.org/abs/1803.01529&lt;/p&gt;
&lt;h1 id="domain-adaptive-faster-r-cnn-for-object-detection-in-the-wild"&gt;Domain Adaptive Faster R-CNN for Object Detection in the Wild&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2018. ETH Zurich &amp;amp; ESAT/PSI
arxiv: https://arxiv.org/abs/1803.03243&lt;/p&gt;
&lt;h1 id="pseudo-mask-augmented-object-detection"&gt;Pseudo Mask Augmented Object Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1803.05858&lt;/p&gt;
&lt;h1 id="revisiting-rcnn-on-awakening-the-classification-power-of-faster-rcnn"&gt;Revisiting RCNN: On Awakening the Classification Power of Faster RCNN&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1803.06799&lt;/p&gt;
&lt;h1 id="learning-region-features-for-object-detection"&gt;Learning Region Features for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: Peking University &amp;amp; MSRA
arxiv: https://arxiv.org/abs/1803.07066&lt;/p&gt;
&lt;h1 id="single-shot-bidirectional-pyramid-networks-for-high-quality-object-detection"&gt;Single-Shot Bidirectional Pyramid Networks for High-Quality Object Detection&lt;/h1&gt;
&lt;p&gt;intro: Singapore Management University &amp;amp; Zhejiang University
arxiv: https://arxiv.org/abs/1803.08208&lt;/p&gt;
&lt;h1 id="object-detection-for-comics-using-manga109-annotations"&gt;Object Detection for Comics using Manga109 Annotations&lt;/h1&gt;
&lt;p&gt;intro: University of Tokyo &amp;amp; National Institute of Informatics, Japan
arxiv: https://arxiv.org/abs/1803.08670&lt;/p&gt;
&lt;h1 id="task-driven-super-resolution-object-detection-in-low-resolution-images"&gt;Task-Driven Super Resolution: Object Detection in Low-resolution Images&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1803.11316&lt;/p&gt;
&lt;h1 id="transferring-common-sense-knowledge-for-object-detection"&gt;Transferring Common-Sense Knowledge for Object Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1804.01077&lt;/p&gt;
&lt;h1 id="multi-scale-location-aware-kernel-representation-for-object-detection"&gt;Multi-scale Location-aware Kernel Representation for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2018
arxiv: https://arxiv.org/abs/1804.00428
github: https://github.com/Hwang64/MLKP&lt;/p&gt;
&lt;h1 id="loss-rank-mining-a-general-hard-example-mining-method-for-real-time-detectors"&gt;Loss Rank Mining: A General Hard Example Mining Method for Real-time Detectors&lt;/h1&gt;
&lt;p&gt;intro: National University of Defense Technology
arxiv: https://arxiv.org/abs/1804.04606&lt;/p&gt;
&lt;h1 id="detnet-a-backbone-network-for-object-detection"&gt;DetNet: A Backbone network for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: Tsinghua University &amp;amp; Megvii Inc
arxiv: https://arxiv.org/abs/1804.06215&lt;/p&gt;
&lt;h1 id="robust-physical-adversarial-attack-on-faster-r-cnn-object-detector"&gt;Robust Physical Adversarial Attack on Faster R-CNN Object Detector&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1804.05810&lt;/p&gt;
&lt;h1 id="advdetpatch-attacking-object-detectors-with-adversarial-patches"&gt;AdvDetPatch: Attacking Object Detectors with Adversarial Patches&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1806.02299&lt;/p&gt;
&lt;h1 id="quantization-mimic-towards-very-tiny-cnn-for-object-detection"&gt;Quantization Mimic: Towards Very Tiny CNN for Object Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1805.02152&lt;/p&gt;
&lt;h1 id="object-detection-at-200-frames-per-second"&gt;Object detection at 200 Frames Per Second&lt;/h1&gt;
&lt;p&gt;intro: United Technologies Research Center-Ireland
arxiv: https://arxiv.org/abs/1805.06361&lt;/p&gt;
&lt;h1 id="object-detection-using-domain-randomization-and-generative-adversarial-refinement-of-synthetic-images"&gt;Object Detection using Domain Randomization and Generative Adversarial Refinement of Synthetic Images&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2018 Deep Vision Workshop
arxiv: https://arxiv.org/abs/1805.11778&lt;/p&gt;
&lt;h1 id="non-maximum-suppression-nms-end-to-end-integration-of-a-convolutional-network-deformable-parts-model-and-non-maximum-suppression"&gt;Non-Maximum Suppression (NMS) End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2015
arxiv: http://arxiv.org/abs/1411.5309
paper: http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf&lt;/p&gt;
&lt;h1 id="a-convnet-for-non-maximum-suppression"&gt;A convnet for non-maximum suppression&lt;/h1&gt;
&lt;p&gt;arxiv: http://arxiv.org/abs/1511.06437&lt;/p&gt;
&lt;h1 id="improving-object-detection-with-one-line-of-code"&gt;Improving Object Detection With One Line of Code&lt;/h1&gt;
&lt;h1 id="soft-nms-improving-object-detection-with-one-line-of-code"&gt;Soft-NMS &amp;ndash; Improving Object Detection With One Line of Code&lt;/h1&gt;
&lt;p&gt;intro: ICCV 2017. University of Maryland
keywords: Soft-NMS
arxiv: https://arxiv.org/abs/1704.04503
github: https://github.com/bharatsingh430/soft-nms&lt;/p&gt;
&lt;h1 id="learning-non-maximum-suppression"&gt;Learning non-maximum suppression&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2017
project page: https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/object-recognition-and-scene-understanding/learning-nms/
arxiv: https://arxiv.org/abs/1705.02950
github: https://github.com/hosang/gossipnet&lt;/p&gt;
&lt;h1 id="relation-networks-for-object-detection"&gt;Relation Networks for Object Detection&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2018
arxiv: https://arxiv.org/abs/1711.11575
Adversarial Examples&lt;/p&gt;
&lt;h1 id="adversarial-examples-that-fool-detectors"&gt;Adversarial Examples that Fool Detectors&lt;/h1&gt;
&lt;p&gt;intro: University of Illinois
arxiv: https://arxiv.org/abs/1712.02494&lt;/p&gt;
&lt;h1 id="adversarial-examples-are-not-easily-detected-bypassing-ten-detection-methods"&gt;Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods&lt;/h1&gt;
&lt;p&gt;project page: http://nicholas.carlini.com/code/nn_breaking_detection/
arxiv: https://arxiv.org/abs/1705.07263
github: https://github.com/carlini/nn_breaking_detection&lt;/p&gt;
&lt;h1 id="weakly-supervised-object-detection-track-and-transfer-watching-videos-to-simulate-strong-human-supervision-for-weakly-supervised-object-detection"&gt;Weakly Supervised Object Detection Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2016
arxiv: http://arxiv.org/abs/1604.05766&lt;/p&gt;
&lt;h1 id="weakly-supervised-object-detection-using-pseudo-strong-labels"&gt;Weakly supervised object detection using pseudo-strong labels&lt;/h1&gt;
&lt;p&gt;arxiv: http://arxiv.org/abs/1607.04731&lt;/p&gt;
&lt;h1 id="saliency-guided-end-to-end-learning-for-weakly-supervised-object-detection"&gt;Saliency Guided End-to-End Learning for Weakly Supervised Object Detection&lt;/h1&gt;
&lt;p&gt;intro: IJCAI 2017
arxiv: https://arxiv.org/abs/1706.06768&lt;/p&gt;
&lt;h1 id="visual-and-semantic-knowledge-transfer-for-large-scale-semi-supervised-object-detection"&gt;Visual and Semantic Knowledge Transfer for Large Scale Semi-supervised Object Detection&lt;/h1&gt;
&lt;p&gt;intro: TPAMI 2017. National Institutes of Health (NIH) Clinical Center
arxiv: https://arxiv.org/abs/1801.03145&lt;/p&gt;
&lt;h1 id="video-object-detection-learning-object-class-detectors-from-weakly-annotated-video"&gt;Video Object Detection Learning Object Class Detectors from Weakly Annotated Video&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2012
paper: https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf&lt;/p&gt;
&lt;h1 id="analysing-domain-shift-factors-between-videos-and-images-for-object-detection"&gt;Analysing domain shift factors between videos and images for object detection&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1501.01186&lt;/p&gt;
&lt;h1 id="video-object-recognition"&gt;Video Object Recognition&lt;/h1&gt;
&lt;p&gt;slides: http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx&lt;/p&gt;
&lt;h1 id="deep-learning-for-saliency-prediction-in-natural-video"&gt;Deep Learning for Saliency Prediction in Natural Video&lt;/h1&gt;
&lt;p&gt;intro: Submitted on 12 Jan 2016
keywords: Deep learning, saliency map, optical flow, convolution network, contrast features
paper: https://hal.archives-ouvertes.fr/hal-01251614/document&lt;/p&gt;
&lt;h1 id="t-cnn-tubelets-with-convolutional-neural-networks-for-object-detection-from-videos"&gt;T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos&lt;/h1&gt;
&lt;p&gt;intro: Winning solution in ILSVRC2015 Object Detection from Video(VID) Task
arxiv: http://arxiv.org/abs/1604.02532
github: https://github.com/myfavouritekk/T-CNN&lt;/p&gt;
&lt;h1 id="object-detection-from-video-tubelets-with-convolutional-neural-networks"&gt;Object Detection from Video Tubelets with Convolutional Neural Networks&lt;/h1&gt;
&lt;p&gt;intro: CVPR 2016 Spotlight paper
arxiv: https://arxiv.org/abs/1604.04053
paper: http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf
gihtub: https://github.com/myfavouritekk/vdetlib&lt;/p&gt;
&lt;h1 id="object-detection-in-videos-with-tubelets-and-multi-context-cues"&gt;Object Detection in Videos with Tubelets and Multi-context Cues&lt;/h1&gt;
&lt;p&gt;intro: SenseTime Group
slides: http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf
slides: http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf&lt;/p&gt;
&lt;h1 id="context-matters-refining-object-detection-in-video-with-recurrent-neural-networks"&gt;Context Matters: Refining Object Detection in Video with Recurrent Neural Networks&lt;/h1&gt;
&lt;p&gt;intro: BMVC 2016
keywords: pseudo-labeler
arxiv: http://arxiv.org/abs/1607.04648
paper: http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf&lt;/p&gt;
&lt;h1 id="cnn-based-object-detection-in-large-video-images"&gt;CNN Based Object Detection in Large Video Images&lt;/h1&gt;
&lt;p&gt;intro: WangTao @ 爱奇艺
keywords: object retrieval, object detection, scene classification
slides: http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf&lt;/p&gt;
&lt;h1 id="object-detection-in-videos-with-tubelet-proposal-networks"&gt;Object Detection in Videos with Tubelet Proposal Networks&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1702.06355&lt;/p&gt;
&lt;h1 id="flow-guided-feature-aggregation-for-video-object-detection"&gt;Flow-Guided Feature Aggregation for Video Object Detection&lt;/h1&gt;
&lt;p&gt;intro: MSRA
arxiv: https://arxiv.org/abs/1703.10025&lt;/p&gt;
&lt;h1 id="video-object-detection-using-faster-r-cnn"&gt;Video Object Detection using Faster R-CNN&lt;/h1&gt;
&lt;p&gt;blog: http://andrewliao11.github.io/object_detection/faster_rcnn/
github: https://github.com/andrewliao11/py-faster-rcnn-imagenet&lt;/p&gt;
&lt;h1 id="improving-context-modeling-for-video-object-detection-and-tracking"&gt;Improving Context Modeling for Video Object Detection and Tracking&lt;/h1&gt;
&lt;p&gt;http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster).pdf&lt;/p&gt;
&lt;h1 id="temporal-dynamic-graph-lstm-for-action-driven-video-object-detection"&gt;Temporal Dynamic Graph LSTM for Action-driven Video Object Detection&lt;/h1&gt;
&lt;p&gt;intro: ICCV 2017
arxiv: https://arxiv.org/abs/1708.00666&lt;/p&gt;
&lt;h1 id="mobile-video-object-detection-with-temporally-aware-feature-maps"&gt;Mobile Video Object Detection with Temporally-Aware Feature Maps&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1711.06368&lt;/p&gt;
&lt;h1 id="towards-high-performance-video-object-detection"&gt;Towards High Performance Video Object Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1711.11577&lt;/p&gt;
&lt;h1 id="impression-network-for-video-object-detection"&gt;Impression Network for Video Object Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1712.05896&lt;/p&gt;
&lt;h1 id="spatial-temporal-memory-networks-for-video-object-detection"&gt;Spatial-Temporal Memory Networks for Video Object Detection&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1712.06317&lt;/p&gt;
&lt;h1 id="3d-detnet-a-single-stage-video-based-vehicle-detector"&gt;3D-DETNet: a Single Stage Video-Based Vehicle Detector&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1801.01769&lt;/p&gt;
&lt;h1 id="object-detection-in-videos-by-short-and-long-range-object-linking"&gt;Object Detection in Videos by Short and Long Range Object Linking&lt;/h1&gt;
&lt;p&gt;https://arxiv.org/abs/1801.09823&lt;/p&gt;
&lt;h1 id="object-detection-in-video-with-spatiotemporal-sampling-networks"&gt;Object Detection in Video with Spatiotemporal Sampling Networks&lt;/h1&gt;
&lt;p&gt;intro: University of Pennsylvania, 2Dartmouth College
arxiv: https://arxiv.org/abs/1803.05549&lt;/p&gt;
&lt;h1 id="towards-high-performance-video-object-detection-for-mobiles"&gt;Towards High Performance Video Object Detection for Mobiles&lt;/h1&gt;
&lt;p&gt;intro: Microsoft Research Asia
arxiv: https://arxiv.org/abs/1804.05830&lt;/p&gt;
&lt;h1 id="object-detection-on-mobile-devices"&gt;Object Detection on Mobile Devices&lt;/h1&gt;
&lt;h1 id="pelee-a-real-time-object-detection-system-on-mobile-devices"&gt;Pelee: A Real-Time Object Detection System on Mobile Devices&lt;/h1&gt;
&lt;p&gt;intro: ICLR 2018 workshop track
intro: based on the SSD
arxiv: https://arxiv.org/abs/1804.06882
github: https://github.com/Robert-JunWang/Pelee&lt;/p&gt;
&lt;h1 id="object-detection-in-3d"&gt;Object Detection in 3D&lt;/h1&gt;
&lt;h1 id="vote3deep-fast-object-detection-in-3d-point-clouds-using-efficient-convolutional-neural-networks"&gt;Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks&lt;/h1&gt;
&lt;p&gt;arxiv: https://arxiv.org/abs/1609.06666&lt;/p&gt;
&lt;h1 id="complex-yolo-real-time-3d-object-detection-on-point-clouds"&gt;Complex-YOLO: Real-time 3D Object Detection on Point Clouds&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: Valeo Schalter und Sensoren GmbH &amp;amp; Ilmenau University of Technology&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1803.06199&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="object-detection-on-rgb-d"&gt;Object Detection on RGB-D&lt;/h1&gt;
&lt;h1 id="learning-rich-features-from-rgb-d-images-for-object-detection-and-segmentation"&gt;Learning Rich Features from RGB-D Images for Object Detection and Segmentation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1407.5736&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="differential-geometry-boosts-convolutional-neural-networks-for-object-detection"&gt;Differential Geometry Boosts Convolutional Neural Networks for Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: CVPR 2016&lt;/li&gt;
&lt;li&gt;paper: http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="a-self-supervised-learning-system-for-object-detection-using-physics-simulation-and-multi-view-pose-estimation"&gt;A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;https://arxiv.org/abs/1703.03347&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="zero-shot-object-detection"&gt;Zero-Shot Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: Australian National University&lt;/li&gt;
&lt;li&gt;keywords: YOLO&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1803.07113&lt;/li&gt;
&lt;li&gt;
&lt;h1 id="zero-shot-object-detection_1"&gt;Zero-Shot Object Detection&lt;/h1&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;https://arxiv.org/abs/1804.04340&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="zero-shot-object-detection-learning-to-simultaneously-recognize-and-localize-novel-concepts"&gt;Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: Australian National University&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1803.06049&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="zero-shot-object-detection-by-hybrid-region-embedding"&gt;Zero-Shot Object Detection by Hybrid Region Embedding&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: Middle East Technical University &amp;amp; Hacettepe University&lt;/li&gt;
&lt;li&gt;arxiv: https://arxiv.org/abs/1805.06157&lt;/li&gt;
&lt;li&gt;
&lt;h1 id="salient-object-detection"&gt;Salient Object Detection&lt;/h1&gt;
This task involves predicting the salient regions of an image given by human eye fixations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="best-deep-saliency-detection-models-cvpr-2016-2015"&gt;Best Deep Saliency Detection Models (CVPR 2016 &amp;amp; 2015)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;http://i.cs.hku.hk/~yzyu/vision.html&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="large-scale-optimization-of-hierarchical-features-for-saliency-prediction-in-natural-images"&gt;Large-scale optimization of hierarchical features for saliency prediction in natural images&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;paper: http://coxlab.org/pdfs/cvpr2014_vig_saliency.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="predicting-eye-fixations-using-convolutional-neural-networks"&gt;Predicting Eye Fixations using Convolutional Neural Networks&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;paper: http://www.escience.cn/system/file?fileId=72648&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="saliency-detection-by-multi-context-deep-learning"&gt;Saliency Detection by Multi-Context Deep Learning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;paper: http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="deepsaliency-multi-task-deep-neural-network-model-for-salient-object-detection"&gt;DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1510.05484&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="supercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection"&gt;SuperCNN: A Superpixelwise Convolutional Neural Network for Salient Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;paper: www.shengfenghe.com/supercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="shallow-and-deep-convolutional-networks-for-saliency-prediction"&gt;Shallow and Deep Convolutional Networks for Saliency Prediction&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: CVPR 2016&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1603.00845&lt;/li&gt;
&lt;li&gt;github: https://github.com/imatge-upc/saliency-2016-cvpr&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="recurrent-attentional-networks-for-saliency-detection"&gt;Recurrent Attentional Networks for Saliency Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: CVPR 2016. recurrent attentional convolutional-deconvolution network (RACDNN)&lt;/li&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1604.03227&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="two-stream-convolutional-networks-for-dynamic-saliency-prediction"&gt;Two-Stream Convolutional Networks for Dynamic Saliency Prediction&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;arxiv: http://arxiv.org/abs/1607.04730&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="unconstrained-salient-object-detection-via-proposal-subset-optimization"&gt;Unconstrained Salient Object Detection via Proposal Subset Optimization&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;intro: CVPR 2016&lt;/li&gt;
&lt;li&gt;project page: http://cs-people.bu.edu/jmzhang/sod.html&lt;/li&gt;
&lt;li&gt;paper: http://cs-people.bu.edu/jmzhang/SOD/CVPR16SOD_camera_ready.pdf&lt;/li&gt;
&lt;li&gt;github: https://github.com/jimmie33/SOD&lt;/li&gt;
&lt;li&gt;caffe model zoo: https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-object-proposal-models-for-salient-object-detection&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="dhsnet-deep-hierarchical-saliency-network-for-salient-object-detection"&gt;DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;paper: http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="salient-object-subitizingbject-detection-cv-cnn-tutorial"&gt;Salient Object Subitizingbject Detection, CV, CNN, TUTORIAL&lt;/h1&gt;</content><category term="Object Detection"></category><category term="TUTORIAL"></category></entry><entry><title>Some Computer Vision Challenges</title><link href="/blog/tutorial/2018/some-computer-vision-challenges/" rel="alternate"></link><published>2018-08-21T21:10:14+00:00</published><updated>2018-08-21T21:10:14+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/tutorial/2018/some-computer-vision-challenges/</id><summary type="html">&lt;p&gt;The contest can be found at the &lt;a href="https://posetrack.net/leaderboard.php"&gt;link&lt;/a&gt; with 3 challenges.&lt;/p&gt;
&lt;h1 id="challenge-1-single-frame-person-pose-estimation"&gt;Challenge 1: Single-frame Person Pose Estimation&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;The best baseline. https://arxiv.org/pdf/1804.06208.pdf
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="challenge-2-multi-frame-person-pose-estimation"&gt;Challenge 2: Multi-frame Person Pose Estimation&lt;/h1&gt;
&lt;h1 id="challenge-3-multi-person-pose-tracking"&gt;Challenge 3: Multi-Person Pose Tracking&lt;/h1&gt;</summary><content type="html">&lt;p&gt;The contest can be found at the &lt;a href="https://posetrack.net/leaderboard.php"&gt;link&lt;/a&gt; with 3 challenges.&lt;/p&gt;
&lt;h1 id="challenge-1-single-frame-person-pose-estimation"&gt;Challenge 1: Single-frame Person Pose Estimation&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;The best baseline. https://arxiv.org/pdf/1804.06208.pdf
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="challenge-2-multi-frame-person-pose-estimation"&gt;Challenge 2: Multi-frame Person Pose Estimation&lt;/h1&gt;
&lt;h1 id="challenge-3-multi-person-pose-tracking"&gt;Challenge 3: Multi-Person Pose Tracking&lt;/h1&gt;</content><category term="Challenge"></category><category term="Contest"></category><category term="TUTORIAL"></category></entry><entry><title>Freeze some layers in Pytorch</title><link href="/blog/tutorial/2018/freeze-some-layers-in-pytorch/" rel="alternate"></link><published>2018-08-21T20:53:37+00:00</published><updated>2018-08-21T20:53:37+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/tutorial/2018/freeze-some-layers-in-pytorch/</id><summary type="html">&lt;h1 id="motivation"&gt;Motivation&lt;/h1&gt;
&lt;p&gt;I faced this just a few days ago, so I&amp;rsquo;m sure this code should be up to date. Here&amp;rsquo;s my answer for &lt;code&gt;Resnet&lt;/code&gt;, but this answer can be used for literally any model.&lt;/p&gt;
&lt;p&gt;The basic idea is that all models have a function &lt;code&gt;model.children()&lt;/code&gt; which returns it&amp;rsquo;s layers. Within each layer, there are parameters (or weights), which can be obtained using &lt;code&gt;.parameters()&lt;/code&gt; on any child (i.e. layer). Now, every parameter has an attribute called &lt;code&gt;requires_grad&lt;/code&gt; which is by default &lt;code&gt;True&lt;/code&gt;. True means it will be backpropagrated and hence to freeze a layer you need to set &lt;code&gt;requires_grad&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; for all parameters of a layer. This can be done like this:&lt;/p&gt;
&lt;h1 id="tldr-code"&gt;TLDR: code&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model_ft&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet50&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;child&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_ft&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;children&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;span class="n"&gt;ct&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ct&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;child&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;requires_grad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This freezes layers 1-6 in the …&lt;/p&gt;</summary><content type="html">&lt;h1 id="motivation"&gt;Motivation&lt;/h1&gt;
&lt;p&gt;I faced this just a few days ago, so I&amp;rsquo;m sure this code should be up to date. Here&amp;rsquo;s my answer for &lt;code&gt;Resnet&lt;/code&gt;, but this answer can be used for literally any model.&lt;/p&gt;
&lt;p&gt;The basic idea is that all models have a function &lt;code&gt;model.children()&lt;/code&gt; which returns it&amp;rsquo;s layers. Within each layer, there are parameters (or weights), which can be obtained using &lt;code&gt;.parameters()&lt;/code&gt; on any child (i.e. layer). Now, every parameter has an attribute called &lt;code&gt;requires_grad&lt;/code&gt; which is by default &lt;code&gt;True&lt;/code&gt;. True means it will be backpropagrated and hence to freeze a layer you need to set &lt;code&gt;requires_grad&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; for all parameters of a layer. This can be done like this:&lt;/p&gt;
&lt;h1 id="tldr-code"&gt;TLDR: code&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model_ft&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet50&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;child&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_ft&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;children&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
&lt;span class="n"&gt;ct&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ct&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;child&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;requires_grad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This freezes layers 1-6 in the total 10 layers of &lt;code&gt;Resnet50&lt;/code&gt;. Hope this helps!&lt;/p&gt;
&lt;h1 id="a-toy-example"&gt;A toy example&lt;/h1&gt;
&lt;p&gt;Below is another toy example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.autograd&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn.functional&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;F&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.optim&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;optim&lt;/span&gt;

&lt;span class="c1"&gt;# toy feed-forward net&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;


&lt;span class="c1"&gt;# define random data&lt;/span&gt;
&lt;span class="n"&gt;random_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;random_target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;

&lt;span class="c1"&gt;# define net&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# print fc2 weight&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fc2 weight before train:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# train the net&lt;/span&gt;
&lt;span class="n"&gt;criterion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MSELoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SGD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# print the trained fc2 weight&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fc2 weight after train:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# save the net&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state_dict&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="s1"&gt;'model'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# delete and redefine the net&lt;/span&gt;
&lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# load the weight&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_state_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'model'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# print the pre-trained fc2 weight&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fc2 pretrained weight (same as the one above):'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# define new random data&lt;/span&gt;
&lt;span class="n"&gt;random_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;random_target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;

&lt;span class="c1"&gt;# we want to freeze the fc2 layer this time: only train fc1 and fc3&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;requires_grad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;requires_grad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;

&lt;span class="c1"&gt;# train again&lt;/span&gt;
&lt;span class="n"&gt;criterion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MSELoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# NOTE: pytorch optimizer explicitly accepts parameter that requires grad&lt;/span&gt;
&lt;span class="c1"&gt;# see https://github.com/pytorch/pytorch/issues/679&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;requires_grad&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# this raises ValueError: optimizing a parameter that doesn't require gradients&lt;/span&gt;
&lt;span class="c1"&gt;#optimizer = optim.Adam(net.parameters(), lr=0.1)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# print the retrained fc2 weight&lt;/span&gt;
&lt;span class="c1"&gt;# note that the weight is same as the one before retraining: only fc1 &amp;amp; fc3 changed&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fc2 weight (frozen) after retrain:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# let's unfreeze the fc2 layer this time for extra tuning&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;requires_grad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;requires_grad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;

&lt;span class="c1"&gt;# add the unfrozen fc2 weight to the current optimizer&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_param_group&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;'params'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;()})&lt;/span&gt;

&lt;span class="c1"&gt;# re-retrain&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# print the re-retrained fc2 weight&lt;/span&gt;
&lt;span class="c1"&gt;# note that this time the fc2 weight also changed&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fc2 weight (unfrozen) after re-retrain:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Pytorch"></category><category term="TUTORIAL"></category></entry><entry><title>Infer in_features for the linear layer in Pytorch</title><link href="/blog/tutorial/2018/infer-in-features-for-the-linear-layer-in-pytorch/" rel="alternate"></link><published>2018-08-21T20:32:34+00:00</published><updated>2018-08-21T20:32:34+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/tutorial/2018/infer-in-features-for-the-linear-layer-in-pytorch/</id><summary type="html">&lt;h1 id="motivation"&gt;Motivation&lt;/h1&gt;
&lt;p&gt;We may wonder how it is possible to infer the &lt;code&gt;in_features&lt;/code&gt; for &lt;code&gt;self.fc1&lt;/code&gt; when there is a transition from a &lt;code&gt;conv&lt;/code&gt; layer to &lt;code&gt;linear&lt;/code&gt; layer. How to obtain the value of &lt;code&gt;320&lt;/code&gt; as the code of the network definition below?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn.functional&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;F&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.autograd&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dropout2d&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;320&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 320&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2_drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;h1 id="motivation"&gt;Motivation&lt;/h1&gt;
&lt;p&gt;We may wonder how it is possible to infer the &lt;code&gt;in_features&lt;/code&gt; for &lt;code&gt;self.fc1&lt;/code&gt; when there is a transition from a &lt;code&gt;conv&lt;/code&gt; layer to &lt;code&gt;linear&lt;/code&gt; layer. How to obtain the value of &lt;code&gt;320&lt;/code&gt; as the code of the network definition below?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn.functional&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;F&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.autograd&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dropout2d&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;320&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 320&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2_drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the CNN model definition, the number of flatten features (&lt;code&gt;in_features&lt;/code&gt; for the dense layer) depends on the input size of the image. The &lt;code&gt;in_features&lt;/code&gt; = &lt;code&gt;320&lt;/code&gt;, if the &lt;code&gt;input_shape&lt;/code&gt; of the image is (3, 28, 28). Here is just an example of MNIST problem.&lt;/p&gt;
&lt;p&gt;Then, there might be a solution of performing a forward pass on the network to get the value of &lt;code&gt;320&lt;/code&gt; like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn.functional&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;F&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.autograd&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dropout2d&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="n"&gt;n_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_get_conv_output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# generate input sample and forward to get shape&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_get_conv_output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;bs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;output_feat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_forward_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;n_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_feat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;n_size&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_forward_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2_drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_forward_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c1"&gt;# x.size(0) - batch_size&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When you just want to write the clean code without doing a foward pass like the above solution, try the second solution:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;any_number&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20000&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;any_number&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# first try&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;    &lt;span class="c1"&gt;# flatten the tensor&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;              &lt;span class="c1"&gt;# the last solution &lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;      
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;# generate an batch of 1 gray image&lt;/span&gt;
&lt;span class="n"&gt;batch_imgs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_imgs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="approach-1"&gt;Approach 1&lt;/h1&gt;
&lt;p&gt;Start with any random number any_number as line 8 and pass it to the &lt;code&gt;in_features&lt;/code&gt; argument in line 9. Then you will get the RuntimeError, the value of 320 is replaced from the bug. After we figure out the right &lt;code&gt;in_features&lt;/code&gt; = &lt;code&gt;320&lt;/code&gt;, we can replace any_number by that value.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;RuntimeError&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="n"&gt;mismatch&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="mi"&gt;320&lt;/span&gt;&lt;span class="o"&gt;],&lt;/span&gt; &lt;span class="n"&gt;m2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="approach-2"&gt;Approach 2&lt;/h1&gt;
&lt;p&gt;The other way is putting the statement to know about the size after flattening (see the comment the last solution) as line 16:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="advantages-and-disadvantages-of-2-above-approaches"&gt;Advantages and Disadvantages of 2 above approaches&lt;/h1&gt;
&lt;h2 id="advantages"&gt;Advantages:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Don&amp;rsquo;t need to write a forward pass function till the fc layers.&lt;/li&gt;
&lt;li&gt;Clean code.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="disadvantages"&gt;Disadvantages:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;We need to figure out the in_features of fc layers manually.&lt;/li&gt;
&lt;li&gt;This is inconvenient in case we want to try with different input_shape.&lt;/li&gt;
&lt;/ol&gt;</content><category term="Pytorch"></category><category term="TUTORIAL"></category></entry><entry><title>Add Parameters in Pytorch</title><link href="/blog/tutorial/2018/add-parameters-in-pytorch/" rel="alternate"></link><published>2018-08-21T20:10:40+00:00</published><updated>2018-08-21T20:10:40+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/tutorial/2018/add-parameters-in-pytorch/</id><summary type="html">&lt;p&gt;In this post, we will discuss about add &lt;code&gt;Paramter&lt;/code&gt; to &lt;code&gt;Module&lt;/code&gt; as the attributes which are listed in &lt;code&gt;Module.parameters&lt;/code&gt; for further optimization steps.&lt;/p&gt;
&lt;h1 id="some-important-notes-about-pytorch-04"&gt;Some important notes about PyTorch 0.4&lt;/h1&gt;
&lt;h2 id="variable-and-tensor-class-are-merged-in-pytorch-04"&gt;Variable and Tensor class are merged in PyTorch 0.4&lt;/h2&gt;
&lt;p&gt;In previous version of PyTorch, Module&amp;rsquo;s inputs and outputs must be &lt;code&gt;Variables&lt;/code&gt;. Data &lt;code&gt;Tensor&lt;/code&gt;s should be wrapped before forwarding them into a &lt;code&gt;Module&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But now, &lt;code&gt;Variable&lt;/code&gt; class and &lt;code&gt;Tensor&lt;/code&gt; class are merge into one. Therefore, we don&amp;rsquo;t need to wrap Tensor anymore in PyTorch version 0.4.&lt;/p&gt;
&lt;h2 id="inputs-to-functions-and-modules-from-torchnn"&gt;Inputs to functions and modules from torch.nn&lt;/h2&gt;
&lt;p&gt;Functions and modules from &lt;code&gt;torch.nn&lt;/code&gt; process only batches of inputs stored in a tensor with an additional first dimension to index them, and produce a corresponding tensor with an additional dimension.&lt;/p&gt;
&lt;p&gt;E.g. a fully connected layer &lt;span class="math"&gt;\(R^C -&amp;gt; R^D\)&lt;/span&gt; expects as input a tensor of size N &amp;times; C …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post, we will discuss about add &lt;code&gt;Paramter&lt;/code&gt; to &lt;code&gt;Module&lt;/code&gt; as the attributes which are listed in &lt;code&gt;Module.parameters&lt;/code&gt; for further optimization steps.&lt;/p&gt;
&lt;h1 id="some-important-notes-about-pytorch-04"&gt;Some important notes about PyTorch 0.4&lt;/h1&gt;
&lt;h2 id="variable-and-tensor-class-are-merged-in-pytorch-04"&gt;Variable and Tensor class are merged in PyTorch 0.4&lt;/h2&gt;
&lt;p&gt;In previous version of PyTorch, Module&amp;rsquo;s inputs and outputs must be &lt;code&gt;Variables&lt;/code&gt;. Data &lt;code&gt;Tensor&lt;/code&gt;s should be wrapped before forwarding them into a &lt;code&gt;Module&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But now, &lt;code&gt;Variable&lt;/code&gt; class and &lt;code&gt;Tensor&lt;/code&gt; class are merge into one. Therefore, we don&amp;rsquo;t need to wrap Tensor anymore in PyTorch version 0.4.&lt;/p&gt;
&lt;h2 id="inputs-to-functions-and-modules-from-torchnn"&gt;Inputs to functions and modules from torch.nn&lt;/h2&gt;
&lt;p&gt;Functions and modules from &lt;code&gt;torch.nn&lt;/code&gt; process only batches of inputs stored in a tensor with an additional first dimension to index them, and produce a corresponding tensor with an additional dimension.&lt;/p&gt;
&lt;p&gt;E.g. a fully connected layer &lt;span class="math"&gt;\(R^C -&amp;gt; R^D\)&lt;/span&gt; expects as input a tensor of size N &amp;times; C and compute a tensor of size N &amp;times; D, where &lt;code&gt;N&lt;/code&gt; is the number of samples (size of a batch).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;: For a sample input, we can turn it into a batch of size 1:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;batch_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unsqueeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unsqueeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# for input of modules and functions.&lt;/span&gt;
&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unsqueeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="parameters-of-a-module_1"&gt;Parameters of a module&lt;/h1&gt;
&lt;p&gt;Please refer to this tutorial &lt;a href="https://documents.epfl.ch/users/f/fl/fleuret/www/dlc/dlc-handout-4b-modules-batch.pdf"&gt;document&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2 id="create-a-module"&gt;Create a &lt;code&gt;module&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;To create a Module, one has to inherit from the base class and implement the constructor &lt;code&gt;__init__(self, ...)&lt;/code&gt; and forward pass &lt;code&gt;forward(self, x)&lt;/code&gt;. Notice that &lt;code&gt;x&lt;/code&gt; in &lt;code&gt;forward(self, x)&lt;/code&gt; pass is a batch.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Net&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As long as you use autograd-compliant operations, the backward pass is implemented automatically.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Module&lt;/code&gt;s added as attributes are seen by &lt;code&gt;Module.parameters()&lt;/code&gt;, which returns an iterator over the model&amp;rsquo;s parameters for optimization.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Net&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Net&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note: &lt;code&gt;nn.Conv2d&lt;/code&gt; and &lt;code&gt;nn.Linear&lt;/code&gt; actually are Modules.&lt;/p&gt;
&lt;p&gt;Parameters added as attributes are also seen by &lt;code&gt;Module.parameters()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Parameters added in dictionaries or arrays are not seen.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Buggy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Buggy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;456&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ouch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ouch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;542&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Buggy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="c1"&gt;# Output&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;456&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The proper policy then is to use &lt;code&gt;Module.add_module(name, module)&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Buggy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Buggy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;456&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'ahhh_0'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;542&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Buggy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="c1"&gt;# Output&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;456&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;543&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;These modules are added as attributes, and can be accessed with &lt;code&gt;getattr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Module.register_parameter(name, parameter)&lt;/code&gt; allows to similarly register &lt;code&gt;Parameter&lt;/code&gt;s explicitly.&lt;/p&gt;
&lt;p&gt;Another option is to add modules in a field of type &lt;code&gt;nn.ModuleList&lt;/code&gt;, which is a list of modules properly dealt with by PyTorch&amp;rsquo;s machinery.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Parameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;456&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;module_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModuleList&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;module_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;75&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;module_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;125&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;999&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="c1"&gt;# Output&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;123&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;456&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;75&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;75&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;999&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;125&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Size&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;999&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "0em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Pytorch"></category><category term="TUTORIAL"></category></entry><entry><title>Some notes on Object Detection</title><link href="/blog/tutorial/2018/some-notes-on-object-detection/" rel="alternate"></link><published>2018-08-21T17:17:38+00:00</published><updated>2018-08-21T17:17:38+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/tutorial/2018/some-notes-on-object-detection/</id><summary type="html">&lt;p&gt;In this post, I&amp;rsquo;m going to note what I have learnt about Object Detection. I hope that it will be useful for my later reference as well as to some people who have the same interest.&lt;/p&gt;
&lt;h1 id="object-detection-utils"&gt;Object Detection Utils&lt;/h1&gt;
&lt;h2 id="drawing-the-bounding-boxes"&gt;Drawing the bounding boxes&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Using PIL package:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;PIL&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ImageDraw&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;draw_bboxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bboxes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;'''&lt;/span&gt;
&lt;span class="sd"&gt;    img - shape [h, w, channels] with values of `0-255` range.&lt;/span&gt;
&lt;span class="sd"&gt;    bboxes - shape [N, 4] with the order of [N, [xmin, ymin, xmax, ymax]]&lt;/span&gt;
&lt;span class="sd"&gt;    '''&lt;/span&gt;
    &lt;span class="c1"&gt;# convert `numpy array` format `img` into `PIL` format `img`&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# draw the image with its bounding boxes&lt;/span&gt;
    &lt;span class="n"&gt;draw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ImageDraw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;bboxes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rectangle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;outline&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'red'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;Using matplotlib package:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;draw_bboxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bboxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;'''&lt;/span&gt;
&lt;span class="sd"&gt;    img - shape [h, w, channels] with values of `0-255` range.&lt;/span&gt;
&lt;span class="sd"&gt;    bboxes - shape [N, 4] with the order of [N …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;In this post, I&amp;rsquo;m going to note what I have learnt about Object Detection. I hope that it will be useful for my later reference as well as to some people who have the same interest.&lt;/p&gt;
&lt;h1 id="object-detection-utils"&gt;Object Detection Utils&lt;/h1&gt;
&lt;h2 id="drawing-the-bounding-boxes"&gt;Drawing the bounding boxes&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Using PIL package:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;PIL&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ImageDraw&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;draw_bboxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bboxes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;'''&lt;/span&gt;
&lt;span class="sd"&gt;    img - shape [h, w, channels] with values of `0-255` range.&lt;/span&gt;
&lt;span class="sd"&gt;    bboxes - shape [N, 4] with the order of [N, [xmin, ymin, xmax, ymax]]&lt;/span&gt;
&lt;span class="sd"&gt;    '''&lt;/span&gt;
    &lt;span class="c1"&gt;# convert `numpy array` format `img` into `PIL` format `img`&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# draw the image with its bounding boxes&lt;/span&gt;
    &lt;span class="n"&gt;draw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ImageDraw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Draw&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;bboxes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rectangle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;outline&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'red'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;Using matplotlib package:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;draw_bboxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bboxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;'''&lt;/span&gt;
&lt;span class="sd"&gt;    img - shape [h, w, channels] with values of `0-255` range.&lt;/span&gt;
&lt;span class="sd"&gt;    bboxes - shape [N, 4] with the order of [N, [xmin, ymin, xmax, ymax]]&lt;/span&gt;
&lt;span class="sd"&gt;    '''&lt;/span&gt;
    &lt;span class="n"&gt;current_axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bboxes&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;box&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bboxes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;conf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;xmax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="c1"&gt;# Draw box&lt;/span&gt;
        &lt;span class="n"&gt;current_axis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_patch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Rectangle&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymin&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; 
            &lt;span class="n"&gt;xmax&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymax&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ymin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'green'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
        &lt;span class="c1"&gt;# Add confidence score&lt;/span&gt;
        &lt;span class="n"&gt;current_axis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"{conf:.2f}"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'x-large'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'white'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bbox&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'facecolor'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'alpha'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="pytorch-format-for-image-input-in-preprocessing_1"&gt;Pytorch format for image input in preprocessing&lt;/h1&gt;
&lt;h2 id="mean-and-std-for-images"&gt;Mean and std for images&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel &lt;strong&gt;RGB&lt;/strong&gt; images of shape (3 x H x W), where &lt;code&gt;H&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt; are expected to be at least &lt;code&gt;224&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images have to be loaded in to a range of [0, 1] and then normalized using &lt;code&gt;mean&lt;/code&gt; = [0.485, 0.456, 0.406] and &lt;code&gt;std&lt;/code&gt; =[0.229, 0.224, 0.225] with respect to [R,G,B] of scale 0-1.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Attributes&lt;/th&gt;
&lt;th&gt;Representation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;scale range&lt;/td&gt;
&lt;td&gt;0 - 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Channel order&lt;/td&gt;
&lt;td&gt;R-G-B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mean&lt;/td&gt;
&lt;td&gt;[0.485, 0.456, 0.406]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;std&lt;/td&gt;
&lt;td&gt;[0.229, 0.224, 0.225]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="encode-the-original-image-to-normalized-array"&gt;Encode the original image to normalized array&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First, we need to normalize the image data into values of the range 0-1, divided 255 if the &lt;code&gt;original_img&lt;/code&gt; is in range of 0-255:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;normalized_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;original_img&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;255&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Then later, we substract &lt;code&gt;mean&lt;/code&gt; from the &lt;code&gt;original_img&lt;/code&gt; and divide it by &lt;code&gt;std&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;new_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;normalized_img&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;new_img&lt;/code&gt; can be obtained from the &lt;code&gt;Dataloader&lt;/code&gt; and will be used in training phase.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="decode-into-the-original-image"&gt;Decode into the original image&lt;/h2&gt;
&lt;p&gt;In order to decode into the original images, we use the below formula:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;normalized_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;new_img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt; If we use Normalize with mean and std as below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Compose&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ToTensor&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normalize&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;normalized_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, the image will be normalized to the the scale of &lt;code&gt;-1&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To convert back, we use the script below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;normalized_img&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt; 
&lt;span class="n"&gt;original_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ToPILImage&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="transforms-methods-in-pytorch"&gt;&lt;code&gt;transforms&lt;/code&gt; methods in Pytorch&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;transforms.ToPILImage&lt;/code&gt; or &lt;code&gt;F.to_pil_image&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;Convert a &lt;code&gt;tensor&lt;/code&gt; or an &lt;code&gt;ndarray&lt;/code&gt; to &lt;code&gt;PIL.Image&lt;/code&gt;. Converts a &lt;code&gt;torch.*Tensor&lt;/code&gt; of shape C x H x W or a numpy &lt;code&gt;ndarray&lt;/code&gt; of shape (H x W x C) to a &lt;code&gt;PIL.Image&lt;/code&gt; while preserving the value range.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;transforms.ToTensor&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;Convert a &lt;code&gt;PIL.Image&lt;/code&gt; or &lt;code&gt;np.ndarray&lt;/code&gt; to &lt;code&gt;tensor&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Converts a &lt;code&gt;PIL.Image&lt;/code&gt; or numpy &lt;code&gt;ndarray&lt;/code&gt; shape of (H x W x C) in the range [0, 255] to a &lt;code&gt;torch.FloatTensor&lt;/code&gt; of shape (C x H x W) in the range [0.0, 1.0]. The channel order is still &lt;code&gt;RGB&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="reading-images-with-different-packages"&gt;Reading images with different packages&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;With &lt;code&gt;opencv&lt;/code&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When we read an image using opencv, the input usually in format [H, W, C] where C = 3 for color images. However, the order of R, G, B channel is BGR. First, we need to transpose the image into [C, H, W]. Then, we exchange the order of channels into RGB:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;cv2&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cv2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imread&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Transpose&lt;/span&gt;
&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Swap the channels&lt;/span&gt;
&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;skimage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When we read an image using skimage.io, the input usually in format [H, W, C]&lt;/p&gt;
&lt;h1 id="load-pretrained-weights-in-pytorch_1"&gt;Load pretrained weights in Pytorch&lt;/h1&gt;
&lt;p&gt;After model_dict.update(pretrained_dict), the model_dict may still have keys that pretrained_model doesn&amp;rsquo;t have, which will cause a error.&lt;/p&gt;
&lt;p&gt;Assum following situation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;pretrained_dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'D'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;model_dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'E'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After we perform:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;pretrained_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pretrained_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; 
                   &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_dict&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; 

&lt;span class="n"&gt;model_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;They become:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;pretrained_dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;model_dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'C'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'E'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So when performing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model.load_state_dict(pretrained_dict)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then &lt;code&gt;model_dict&lt;/code&gt; still has key &lt;code&gt;E&lt;/code&gt; that &lt;code&gt;pretrained_dict&lt;/code&gt; doesn&amp;rsquo;t have.
So how about using:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_state_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The complete snippet is therefore as follow:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_weights&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pretrained_net&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pretrained_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pretrained_net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state_dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;model_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state_dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;pretrained_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pretrained_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model_dict&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;model_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_state_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Load pretrained_net from some sources...&lt;/span&gt;
        &lt;span class="n"&gt;pretrained_net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
        &lt;span class="n"&gt;update_weights&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pretrained_net&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Object Detection"></category><category term="Pytorch"></category><category term="TUTORIAL"></category></entry><entry><title>Convolution visualization</title><link href="/blog/tutorial/2018/convolution-visualization/" rel="alternate"></link><published>2018-08-21T16:53:40+00:00</published><updated>2018-08-21T16:53:40+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/tutorial/2018/convolution-visualization/</id><summary type="html">&lt;div class="article-style" itemprop="articleBody"&gt;
&lt;p&gt;This post attributes to Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning(&lt;a href="https://gist.github.com/fvisin/165ca9935392fa9600a6c94664a01214" target="_blank"&gt;BibTeX&lt;/a&gt;) for the nice visualization.
  &lt;/p&gt;
&lt;h2 id="convolution-animations"&gt;Convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table style="width:100%; table-layout:fixed;"&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/arbitrary_padding_no_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/full_padding_no_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No padding, no strides&lt;/td&gt;
&lt;td&gt;Arbitrary padding, no strides&lt;/td&gt;
&lt;td&gt;Half padding, no strides&lt;/td&gt;
&lt;td&gt;Full padding, no strides&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_odd.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No padding, strides&lt;/td&gt;
&lt;td&gt;Padding, strides&lt;/td&gt;
&lt;td&gt;Padding, strides (odd)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id="transposed-convolution-animations"&gt;Transposed convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table style="width:100%; table-layout:fixed;"&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/arbitrary_padding_no_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/full_padding_no_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No padding, no strides, transposed&lt;/td&gt;
&lt;td&gt;Arbitrary padding, no strides, transposed&lt;/td&gt;
&lt;td&gt;Half padding, no strides, transposed&lt;/td&gt;
&lt;td&gt;Full padding, no strides, transposed&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_odd_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No padding, strides, transposed&lt;/td&gt;
&lt;td&gt;Padding, strides, transposed&lt;/td&gt;
&lt;td&gt;Padding, strides, transposed (odd)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id="dilated-convolution-animations"&gt;Dilated convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table ;="" style="width:25%" table-layout:fixed;=""&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif" width="150px"/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No padding, no stride, dilation&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;</summary><content type="html">&lt;div class="article-style" itemprop="articleBody"&gt;
&lt;p&gt;This post attributes to Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning(&lt;a href="https://gist.github.com/fvisin/165ca9935392fa9600a6c94664a01214" target="_blank"&gt;BibTeX&lt;/a&gt;) for the nice visualization.
  &lt;/p&gt;
&lt;h2 id="convolution-animations"&gt;Convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table style="width:100%; table-layout:fixed;"&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/arbitrary_padding_no_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/full_padding_no_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No padding, no strides&lt;/td&gt;
&lt;td&gt;Arbitrary padding, no strides&lt;/td&gt;
&lt;td&gt;Half padding, no strides&lt;/td&gt;
&lt;td&gt;Full padding, no strides&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_odd.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No padding, strides&lt;/td&gt;
&lt;td&gt;Padding, strides&lt;/td&gt;
&lt;td&gt;Padding, strides (odd)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id="transposed-convolution-animations"&gt;Transposed convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table style="width:100%; table-layout:fixed;"&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/arbitrary_padding_no_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/full_padding_no_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No padding, no strides, transposed&lt;/td&gt;
&lt;td&gt;Arbitrary padding, no strides, transposed&lt;/td&gt;
&lt;td&gt;Half padding, no strides, transposed&lt;/td&gt;
&lt;td&gt;Full padding, no strides, transposed&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_odd_transposed.gif" width="150px"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No padding, strides, transposed&lt;/td&gt;
&lt;td&gt;Padding, strides, transposed&lt;/td&gt;
&lt;td&gt;Padding, strides, transposed (odd)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;h2 id="dilated-convolution-animations"&gt;Dilated convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table ;="" style="width:25%" table-layout:fixed;=""&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif" width="150px"/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No padding, no stride, dilation&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;</content><category term="CNN"></category><category term="TUTORIAL"></category></entry><entry><title>Working with Git</title><link href="/blog/tutorial/2018/working-with-git/" rel="alternate"></link><published>2018-08-21T16:31:44+00:00</published><updated>2018-08-21T16:31:44+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/tutorial/2018/working-with-git/</id><summary type="html">&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;h2 id="whats-a-version-control-system"&gt;What's a version control system?&lt;/h2&gt;
&lt;p&gt;A version control system, or VCS, tracks the history of changes as people and teams collaborate on projects together. As the project evolves, teams can run tests, fix bugs, and contribute new code with the confidence that any version can be recovered at any time. Developers can review project history to find out:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Which changes were made?
Who made the changes?
When were the changes made?
Why were changes needed?
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="whats-a-distributed-version-control-system"&gt;What&amp;rsquo;s a distributed version control system?&lt;/h2&gt;
&lt;p&gt;Git is an example of a distributed version control system (DVCS) commonly used for open source and commercial software development. DVCSs allow full access to every file, branch, and iteration of a project, and allows every user access to a full and self-contained history of all changes. Unlike once popular centralized version control systems, DVCSs like Git don&amp;rsquo;t need a constant connection to a central repository …&lt;/p&gt;</summary><content type="html">&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;h2 id="whats-a-version-control-system"&gt;What's a version control system?&lt;/h2&gt;
&lt;p&gt;A version control system, or VCS, tracks the history of changes as people and teams collaborate on projects together. As the project evolves, teams can run tests, fix bugs, and contribute new code with the confidence that any version can be recovered at any time. Developers can review project history to find out:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Which changes were made?
Who made the changes?
When were the changes made?
Why were changes needed?
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="whats-a-distributed-version-control-system"&gt;What&amp;rsquo;s a distributed version control system?&lt;/h2&gt;
&lt;p&gt;Git is an example of a distributed version control system (DVCS) commonly used for open source and commercial software development. DVCSs allow full access to every file, branch, and iteration of a project, and allows every user access to a full and self-contained history of all changes. Unlike once popular centralized version control systems, DVCSs like Git don&amp;rsquo;t need a constant connection to a central repository. Developers can work anywhere and collaborate asynchronously from any time zone.&lt;/p&gt;
&lt;p&gt;Without version control, team members are subject to redundant tasks, slower timelines, and multiple copies of a single project. To eliminate unnecessary work, Git and other VCSs give each contributor a unified and consistent view of a project, surfacing work that&amp;rsquo;s already in progress. Seeing a transparent history of changes, who made them, and how they contribute to the development of a project helps team members stay aligned while working independently.&lt;/p&gt;
&lt;h2 id="why-git"&gt;Why Git?&lt;/h2&gt;
&lt;p&gt;According to the latest Stack Overflow developer survey, more than 70 percent of developers use Git, making it the most-used VCS in the world. Git is commonly used for both open source and commercial software development, with significant benefits for individuals, teams and businesses.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Git lets developers see the entire timeline of their changes, decisions, and progression of any project in one place. From the moment they access the history of a project, the developer has all the context they need to understand it and start contributing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Developers work in every time zone. With a DVCS like Git, collaboration can happen any time while maintaining source code integrity. Using branches, developers can safely propose changes to production code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Businesses using Git can break down communication barriers between teams and keep them focused on doing their best work. Plus, Git makes it possible to align experts across a business to collaborate on major projects.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="whats-a-repository"&gt;What&amp;rsquo;s a repository?&lt;/h2&gt;
&lt;p&gt;A repository, or Git project, encompasses the entire collection of files and folders associated with a project, along with each file&amp;rsquo;s revision history. The file history appears as snapshots in time called commits, and the commits exist as a linked-list relationship, and can be organized into multiple lines of development called branches. Because Git is a DVCS, repositories are self-contained units and anyone who owns a copy of the repository can access the entire codebase and its history. Using the command line or other ease-of-use interfaces, a git repository also allows for: interaction with the history, cloning, creating branches, committing, merging, comparing changes across versions of code, and more.&lt;/p&gt;
&lt;p&gt;Working in repositories keeps development projects organized and protected. Developers are encouraged to fix bugs, or create fresh features, without fear of derailing mainline development efforts. Git facilitates this through the use of topic branches: lightweight pointers to commits in history that can be easily created and deprecated when no longer needed.&lt;/p&gt;
&lt;p&gt;Through platforms like GitHub, Git also provides more opportunities for project transparency and collaboration. Public repositories help teams work together to build the best possible final product.&lt;/p&gt;
&lt;h2 id="basic-git-commands"&gt;Basic Git commands&lt;/h2&gt;
&lt;p&gt;To use Git, developers use specific commands to copy, create, change, and combine code. These commands can be executed directly from the command line or by using an application like GitHub Desktop or Git Kraken. Here are some common commands for using Git:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;git init&lt;/code&gt; initializes a brand new Git repository and begins tracking an existing directory. It adds a hidden subfolder within the existing directory that houses the internal data structure required for version control.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; creates a local copy of a project that already exists remotely. The clone includes all the project&amp;rsquo;s files, history, and branches.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;git add&lt;/code&gt; stages a change. Git tracks changes to a developer&amp;rsquo;s codebase, but it&amp;rsquo;s necessary to stage and take a snapshot of the changes to include them in the project&amp;rsquo;s history. This command performs staging, the first part of that two-step process. Any changes that are staged will become a part of the next snapshot and a part of the project&amp;rsquo;s history. Staging and committing separately gives developers complete control over the history of their project without changing how they code and work.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;git commit&lt;/code&gt; saves the snapshot to the project history and completes the change-tracking process. In short, a commit functions like taking a photo. Anything that&amp;rsquo;s been staged with git add will become a part of the snapshot with git commit.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;git status&lt;/code&gt; shows the status of changes as untracked, modified, or staged.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;git branch&lt;/code&gt; shows the branches being worked on locally.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;git merge&lt;/code&gt; merges lines of development together. This command is typically used to combine changes made on two distinct branches. For example, a developer would merge when they want to combine changes from a feature branch into the master branch for deployment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;git pull&lt;/code&gt; updates the local line of development with updates from its remote counterpart. Developers use this command if a teammate has made commits to a branch on a remote, and they would like to reflect those changes in their local environment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;git push&lt;/code&gt; updates the remote repository with any commits made locally to a branch.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learn more from a full reference guide to Git commands.&lt;/p&gt;
&lt;h2 id="explore-more-git-commands"&gt;Explore more Git commands&lt;/h2&gt;
&lt;p&gt;For a detailed look at Git practices, the &lt;a href="https://guides.github.com/introduction/git-handbook/"&gt;videos&lt;/a&gt; below show how to get the most out of some Git commands.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Working locally&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git status&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Two-step commits&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git pull&lt;/code&gt; and &lt;code&gt;git push&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="how-github-fits-in"&gt;How GitHub fits in&lt;/h2&gt;
&lt;p&gt;GitHub is a Git hosting repository that provides developers with tools to ship better code through command line features, issues (threaded discussions), pull requests, code review, or the use of a collection of free and for-purchase apps in the GitHub Marketplace. With collaboration layers like the GitHub flow, a community of 15 million developers, and an ecosystm with hundreds of integrations, GitHub changes the way software is built.&lt;/p&gt;
&lt;h2 id="how-github-works"&gt;How GitHub works&lt;/h2&gt;
&lt;p&gt;GitHub builds collaboration directly into the developement process. Work is organized into repositories, where developers can outline requirements or direction and set expectations for team members. Then, using the GitHub flow, developers simply create a branch to work on updates, commit changes to save them, open a pull request to propose and discuss changes, and merge pull requests once everyone is on the same page.&lt;/p&gt;
&lt;h2 id="the-github-flow"&gt;The GitHub flow&lt;/h2&gt;
&lt;p&gt;The GitHub flow is a lightweight, branch-based workflow built around core Git commands used by teams around the globe&amp;mdash;including ours.&lt;/p&gt;
&lt;p&gt;The GitHub flow has six steps, each with distinct benefits when implemented:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a branch: Topic branches created from the canonical deployment branch (usually master) allow teams to contribute to many parallel efforts. Short-lived topic branches, in particular, keep teams focused and results in quick ships.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add commits: Snapshots of development efforts within a branch create safe, revertible points in the project&amp;rsquo;s history.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Open a pull request: Pull requests publicize a project&amp;rsquo;s ongoing efforts and set the tone for a transparent development process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Discuss and review code: Teams participate in code reviews by commenting, testing, and reviewing open pull requests. Code review is at the core of an open and participatory culture.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Merge: Upon clicking merge, GitHub automatically performs the equivalent of a local &amp;lsquo;git merge&amp;rsquo; operation. GitHub also keeps the entire branch development history on the merged pull request.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deploy: Teams can choose the best release cycles or incorporate continuous integration tools and operate with the assurance that code on the deployment branch has gone through a robust workflow.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="learn-more-about-the-github-flow"&gt;Learn more about the GitHub flow&lt;/h2&gt;
&lt;p&gt;Developers can find more information about the GitHub flow in the resources provided below.&lt;/p&gt;
&lt;h1 id="git-cheat-sheet_1"&gt;Git Cheat Sheet&lt;/h1&gt;
&lt;p&gt;Git is the open source distributed version control system that facilitates GitHub activities on your laptop or desktop. This cheat sheet summarizes commonly used Git command line instructions for quick reference.&lt;/p&gt;
&lt;h2 id="install-git"&gt;Install GIT&lt;/h2&gt;
&lt;p&gt;GitHub provides desktop clients that include a graphical user
interface for the most common repository actions and an automatically
updating command line edition of Git for advanced scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GitHub for Windows
    htps://windows.github.com&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GitHub for Mac
    htps://mac.github.com&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Git distributions for Linux and POSIX systems are available on the
official Git SCM web site.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Git for All Platforms
    htp://git-scm.com&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="configure-tooling"&gt;Configure Tooling&lt;/h2&gt;
&lt;p&gt;Configure user information for all local repositories&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git config --global user.name &lt;span class="s2"&gt;"[name]"&lt;/span&gt;
&lt;span class="c1"&gt;# Sets the name you want atached to your commit transactions&lt;/span&gt;

$ git config --global user.email &lt;span class="s2"&gt;"[email address]"&lt;/span&gt;
&lt;span class="c1"&gt;# Sets the email you want atached to your commit transactions&lt;/span&gt;

$ git config --global color.ui auto
&lt;span class="c1"&gt;# Enables helpful colorization of command line output&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="create-repositories"&gt;Create Repositories&lt;/h2&gt;
&lt;p&gt;Start a new repository or obtain one from an existing URL&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git init &lt;span class="o"&gt;[&lt;/span&gt;project-name&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Creates a new local repository with the specified name&lt;/span&gt;

$ git clone &lt;span class="o"&gt;[&lt;/span&gt;url&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Downloads a project and its entire version history&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="make-changes"&gt;Make Changes&lt;/h2&gt;
&lt;p&gt;Review edits and craf a commit transaction&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git status
&lt;span class="c1"&gt;# Lists all new or modified files to be commited&lt;/span&gt;

$ git add &lt;span class="o"&gt;[&lt;/span&gt;file&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Snapshots the file in preparation for versioning&lt;/span&gt;

$ git reset &lt;span class="o"&gt;[&lt;/span&gt;file&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Unstages the file, but preserve its contents&lt;/span&gt;

$ git diff
&lt;span class="c1"&gt;# Shows file differences not yet staged&lt;/span&gt;

$ git diff --staged
&lt;span class="c1"&gt;# Shows file differences between staging and the last file version&lt;/span&gt;

$ git commit -m &lt;span class="s2"&gt;"[descriptive message]"&lt;/span&gt;
&lt;span class="c1"&gt;# Records file snapshots permanently in version history&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="group-changes"&gt;Group Changes&lt;/h2&gt;
&lt;p&gt;Name a series of commits and combine completed efforts&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git branch
&lt;span class="c1"&gt;# Lists all local branches in the current repository&lt;/span&gt;

$ git branch &lt;span class="o"&gt;[&lt;/span&gt;branch-name&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Creates a new branch&lt;/span&gt;

$ git checkout &lt;span class="o"&gt;[&lt;/span&gt;branch-name&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Switches to the specified branch and updates the working directory&lt;/span&gt;

$ git merge &lt;span class="o"&gt;[&lt;/span&gt;branch&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Combines the specified branch&amp;rsquo;s history into the current branch&lt;/span&gt;

$ git branch -d &lt;span class="o"&gt;[&lt;/span&gt;branch-name&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Deletes the specified branch&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="refactor-filenames"&gt;Refactor Filenames&lt;/h2&gt;
&lt;p&gt;Relocate and remove versioned files&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git rm &lt;span class="o"&gt;[&lt;/span&gt;file&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Deletes the file from the working directory and stages the deletion&lt;/span&gt;

$ git rm --cached &lt;span class="o"&gt;[&lt;/span&gt;file&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Removes the file from version control but preserves the file locally&lt;/span&gt;

$ git mv &lt;span class="o"&gt;[&lt;/span&gt;file-original&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;file-renamed&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Changes the file name and prepares it for commit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="suppress-tracking"&gt;Suppress tracking&lt;/h2&gt;
&lt;p&gt;Exclude temporary files and paths&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;*.log
build/
temp-*
&lt;span class="c1"&gt;#A text file named .gitignore suppresses accidental versioning of files and paths matching the specified paterns&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git ls-files --other --ignored --exclude-standard
&lt;span class="c1"&gt;# Lists all ignored files in this project&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="save-fragments"&gt;Save Fragments&lt;/h2&gt;
&lt;p&gt;Shelve and restore incomplete changes&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git stash
&lt;span class="c1"&gt;# Temporarily stores all modified tracked files&lt;/span&gt;

$ git stash list
&lt;span class="c1"&gt;# Lists all stashed changesets&lt;/span&gt;

$ git stash pop
&lt;span class="c1"&gt;# Restores the most recently stashed files&lt;/span&gt;

$ git stash drop
&lt;span class="c1"&gt;# Discards the most recently stashed changeset&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="review-history"&gt;Review History&lt;/h2&gt;
&lt;p&gt;Browse and inspect the evolution of project files&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git log
&lt;span class="c1"&gt;# Lists version history for the current branch&lt;/span&gt;

$ git log --follow &lt;span class="o"&gt;[&lt;/span&gt;file&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Lists version history for a file, including renames&lt;/span&gt;

$ git diff &lt;span class="o"&gt;[&lt;/span&gt;first-branch&lt;span class="o"&gt;]&lt;/span&gt;...&lt;span class="o"&gt;[&lt;/span&gt;second-branch&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Shows content differences between two branches&lt;/span&gt;

$ git show &lt;span class="o"&gt;[&lt;/span&gt;commit&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Outputs metadata and content changes of the specified commit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Redo commits 
Erase mistakes and craf replacement history&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git reset &lt;span class="o"&gt;[&lt;/span&gt;commit&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Undoes all commits afer [commit], preserving changes locally&lt;/span&gt;

$ git reset --hard &lt;span class="o"&gt;[&lt;/span&gt;commit&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Discards all history and changes back to the specified commit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="synchronize-changes"&gt;Synchronize changes&lt;/h2&gt;
&lt;p&gt;Register a repository bookmark and exchange version history&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git fetch &lt;span class="o"&gt;[&lt;/span&gt;bookmark&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Downloads all history from the repository bookmark&lt;/span&gt;

$ git merge &lt;span class="o"&gt;[&lt;/span&gt;bookmark&lt;span class="o"&gt;]&lt;/span&gt;/&lt;span class="o"&gt;[&lt;/span&gt;branch&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Combines bookmark&amp;rsquo;s branch into current local branch&lt;/span&gt;

$ git push &lt;span class="o"&gt;[&lt;/span&gt;alias&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;branch&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Uploads all local branch commits to GitHub&lt;/span&gt;

$ git pull
&lt;span class="c1"&gt;#Downloads bookmark history and incorporates changes&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="some-frequent-templates_1"&gt;Some frequent templates&lt;/h1&gt;
&lt;h2 id="example-contribute-to-an-existing-repository"&gt;Example: Contribute to an existing repository&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# download a repository on GitHub.com to our machine&lt;/span&gt;
git clone https://github.com/me/repo.git

&lt;span class="c1"&gt;# change into the `repo` directory&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; repo

&lt;span class="c1"&gt;# create a new branch to store any new changes&lt;/span&gt;
git branch my-branch

&lt;span class="c1"&gt;# switch to that branch (line of development)&lt;/span&gt;
git checkout my-branch

&lt;span class="c1"&gt;# make changes, for example, edit `file1.md` and `file2.md` using the text editor&lt;/span&gt;

&lt;span class="c1"&gt;# stage the changed files&lt;/span&gt;
git add file1.md file2.md

&lt;span class="c1"&gt;# take a snapshot of the staging area (anything that's been added)&lt;/span&gt;
git commit -m &lt;span class="s2"&gt;"my snapshot"&lt;/span&gt;

&lt;span class="c1"&gt;# push changes to github&lt;/span&gt;
git push --set-upstream origin my-branch
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="example-start-a-new-repository-and-publish-it-to-github"&gt;Example: Start a new repository and publish it to GitHub&lt;/h2&gt;
&lt;p&gt;First, you will need to create a new repository on GitHub. You can learn how to create a new repository in our Hello World guide. Do not initialize the repository with a README, .gitignore or License. This empty repository will await your code.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# create a new directory, and initialize it with git-specific functions&lt;/span&gt;
git init my-repo

&lt;span class="c1"&gt;# change into the `my-repo` directory&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; my-repo

&lt;span class="c1"&gt;# create the first file in the project&lt;/span&gt;
touch README.md

&lt;span class="c1"&gt;# git isn't aware of the file, stage it&lt;/span&gt;
git add README.md

&lt;span class="c1"&gt;# take a snapshot of the staging area&lt;/span&gt;
git commit -m &lt;span class="s2"&gt;"add README to initial commit"&lt;/span&gt;

&lt;span class="c1"&gt;# provide the path for the repository you created on github&lt;/span&gt;
git remote add origin https://github.com/YOUR-USERNAME/YOUR-REPOSITORY.git

&lt;span class="c1"&gt;# push changes to github&lt;/span&gt;
git push --set-upstream origin master
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="example-contribute-to-an-existing-branch-on-github"&gt;Example: contribute to an existing branch on GitHub&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# assumption: a project called `repo` already exists on the machine, and a new branch has been pushed to GitHub.com since the last time changes were made locally&lt;/span&gt;

&lt;span class="c1"&gt;# change into the `repo` directory&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; repo

&lt;span class="c1"&gt;# update all remote tracking branches, and the currently checked out branch&lt;/span&gt;
git pull

&lt;span class="c1"&gt;# change into the existing branch called `feature-a`&lt;/span&gt;
git checkout feature-a

&lt;span class="c1"&gt;# make changes, for example, edit `file1.md` using the text editor&lt;/span&gt;

&lt;span class="c1"&gt;# stage the changed file&lt;/span&gt;
git add file1.md

&lt;span class="c1"&gt;# take a snapshot of the staging area&lt;/span&gt;
git commit -m &lt;span class="s2"&gt;"edit file1"&lt;/span&gt;

&lt;span class="c1"&gt;# push changes to github&lt;/span&gt;
git push
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Source: &lt;a href="https://guides.github.com/introduction/git-handbook/"&gt;git-handbook&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id="models-for-collaborative-development_1"&gt;Models for collaborative development&lt;/h1&gt;
&lt;p&gt;There are two primary ways people collaborate on GitHub:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Shared repository&lt;/li&gt;
&lt;li&gt;Fork and pull&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With a shared repository, individuals and teams are explicitly designated as contributors with read, write, or administrator access. This simple permission structure, combined with features like protected branches and Marketplace, helps teams progress quickly when they adopt GitHub.&lt;/p&gt;
&lt;p&gt;For an open source project, or for projects to which anyone can contribute, managing individual permissions can be challenging, but a fork and pull model allows anyone who can view the project to contribute. A fork is a copy of a project under an developer&amp;rsquo;s personal account. Every developer has full control of their fork and is free to implement a fix or new feature. Work completed in forks is either kept separate, or is surfaced back to the original project via a pull request. There, maintainers can review the suggested changes before they&amp;rsquo;re merged. See the &lt;a href="https://guides.github.com/activities/forking/"&gt;Forking Projects Guide&lt;/a&gt; for more information.&lt;/p&gt;</content><category term="GIT"></category><category term="TUTORIAL"></category></entry><entry><title>Những điểm nhấn và nuối tiếc của cố Tổng Thư ký LHQ Kofi Annan</title><link href="/blog/other/2018/kofi-annan/" rel="alternate"></link><published>2018-08-21T16:05:42+00:00</published><updated>2018-08-21T16:05:42+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/other/2018/kofi-annan/</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;Một trong những nh&amp;agrave; ngoại giao lừng danh v&amp;agrave; l&amp;agrave; biểu tượng của Li&amp;ecirc;n hợp quốc (LHQ) &amp;ndash; cựu Tổng Thư k&amp;yacute; Kofi Annan đ&amp;atilde; ra đi m&amp;atilde;i m&amp;atilde;i. Cống hiện tận tụy của Tổng Thư k&amp;yacute; LHQ người da m&amp;agrave;u đầu ti&amp;ecirc;n v&amp;igrave; h&amp;ograve;a b&amp;igrave;nh thế giới l&amp;agrave; kh&amp;ocirc;ng thể đong đếm.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ng&amp;agrave;y 18/8, truyền th&amp;ocirc;ng quốc tế đồng loạt đưa tin về sự ra đi của cựu Tổng thư k&amp;yacute; LHQ Kofi Annan. &amp;Ocirc;ng Kofi Annan giữ vai tr&amp;ograve; Tổng Thư k&amp;yacute; LHQ từ năm 1997-2006. Năm 2001, c&amp;ocirc;ng d&amp;acirc;n Ghana Kofi Annan c&amp;ugrave;ng LHQ đ&amp;atilde; được vinh danh nhận Giải Nobel H&amp;ograve;a B&amp;igrave;nh.&lt;/p&gt;
&lt;h1 id="7-dieu-ve-nha-ngoai-giao-kofi-annan"&gt;7 Điều về nh&amp;agrave; ngoại giao Kofi Annan&lt;/h1&gt;
&lt;p&gt;Dưới đ&amp;acirc;y l&amp;agrave; 7 điểm thực tế về huyền thoại ngoại giao Kofi Annan c&amp;ocirc;ng ch&amp;uacute;ng c&amp;oacute; thể chưa biết đến, được tạp ch&amp;iacute; Newsweek (Mỹ) đăng tải.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cố Tổng Thư k&amp;yacute; LHQ Kofi Annan. Ảnh: CNN" src="https://media.baotintuc.vn/Upload/yTwlGtgJTRZkeJAfcpWR4g/files/2018/08/8A/kofi.jpg"/&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&amp;Ocirc;ng Annan sinh ng&amp;agrave;y 8/4/1938 tại …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;Một trong những nh&amp;agrave; ngoại giao lừng danh v&amp;agrave; l&amp;agrave; biểu tượng của Li&amp;ecirc;n hợp quốc (LHQ) &amp;ndash; cựu Tổng Thư k&amp;yacute; Kofi Annan đ&amp;atilde; ra đi m&amp;atilde;i m&amp;atilde;i. Cống hiện tận tụy của Tổng Thư k&amp;yacute; LHQ người da m&amp;agrave;u đầu ti&amp;ecirc;n v&amp;igrave; h&amp;ograve;a b&amp;igrave;nh thế giới l&amp;agrave; kh&amp;ocirc;ng thể đong đếm.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ng&amp;agrave;y 18/8, truyền th&amp;ocirc;ng quốc tế đồng loạt đưa tin về sự ra đi của cựu Tổng thư k&amp;yacute; LHQ Kofi Annan. &amp;Ocirc;ng Kofi Annan giữ vai tr&amp;ograve; Tổng Thư k&amp;yacute; LHQ từ năm 1997-2006. Năm 2001, c&amp;ocirc;ng d&amp;acirc;n Ghana Kofi Annan c&amp;ugrave;ng LHQ đ&amp;atilde; được vinh danh nhận Giải Nobel H&amp;ograve;a B&amp;igrave;nh.&lt;/p&gt;
&lt;h1 id="7-dieu-ve-nha-ngoai-giao-kofi-annan"&gt;7 Điều về nh&amp;agrave; ngoại giao Kofi Annan&lt;/h1&gt;
&lt;p&gt;Dưới đ&amp;acirc;y l&amp;agrave; 7 điểm thực tế về huyền thoại ngoại giao Kofi Annan c&amp;ocirc;ng ch&amp;uacute;ng c&amp;oacute; thể chưa biết đến, được tạp ch&amp;iacute; Newsweek (Mỹ) đăng tải.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cố Tổng Thư k&amp;yacute; LHQ Kofi Annan. Ảnh: CNN" src="https://media.baotintuc.vn/Upload/yTwlGtgJTRZkeJAfcpWR4g/files/2018/08/8A/kofi.jpg"/&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&amp;Ocirc;ng Annan sinh ng&amp;agrave;y 8/4/1938 tại Comassie, Gold Coast &amp;ndash; ng&amp;agrave;y nay l&amp;agrave; Kumasi, Ghana. &amp;Ocirc;ng Annan c&amp;ograve;n c&amp;oacute; một chị g&amp;aacute;i song sinh t&amp;ecirc;n l&amp;agrave; Efua Atta. B&amp;agrave; Efua Atta đ&amp;atilde; qua đời năm 1991. Cả &amp;ocirc;ng Annan v&amp;agrave; chị g&amp;aacute;i đều c&amp;oacute; t&amp;ecirc;n đệm Atta, theo tiếng địa phương c&amp;oacute; nghĩa l&amp;agrave; "sinh đ&amp;ocirc;i".&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Theo dữ liệu của LHQ, &amp;ocirc;ng Annan từng theo học tại Đại học Khoa học v&amp;agrave; C&amp;ocirc;ng nghệ Kumasi sau đ&amp;oacute; ho&amp;agrave;n th&amp;agrave;nh chương tr&amp;igrave;nh cử nh&amp;acirc;n tại Mỹ năm 1961. &amp;Ocirc;ng Annan gi&amp;agrave;nh học bổng v&amp;agrave; theo học ng&amp;agrave;nh kinh tế tại trường cao đẳng Macalester ở St. Paul, Minnesota (Mỹ). Tiếp đ&amp;oacute;, &amp;ocirc;ng học ng&amp;agrave;nh kinh tế chương tr&amp;igrave;nh sau đại học tại Viện Quốc tế Nghi&amp;ecirc;n cứu v&amp;agrave; Ph&amp;aacute;t triển ở Geneva, Thụy Sĩ.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Từ năm 1962, &amp;ocirc;ng Annan bước ch&amp;acirc;n v&amp;agrave;o lĩnh vực ngoại giao khi l&amp;agrave;m việc tại Tổ chức Y tế Thế giới ở Geneva. Trong thời kỳ từ thập ni&amp;ecirc;n 60 đến 80 của thế kỷ trước, &amp;ocirc;ng Annan đ&amp;atilde; c&amp;ocirc;ng t&amp;aacute;c cho nhiều cơ quan của LHQ như Ủy ban Kinh tế ch&amp;acirc;u Phi Li&amp;ecirc;n hợp quốc, Cao ủy LHQ về người tị nạn&amp;hellip; V&amp;agrave;o th&amp;aacute;ng 3/1993, &amp;ocirc;ng Annan l&amp;agrave; Trợ l&amp;yacute; của Tổng Thư k&amp;yacute; Lực lượng g&amp;igrave;n giữ ho&amp;agrave; b&amp;igrave;nh Li&amp;ecirc;n Hiệp Quốc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Khi Iraq đưa qu&amp;acirc;n đến Kuwait trong th&amp;aacute;ng 8/1990, Tổng Thư k&amp;yacute; LHQ khi đ&amp;oacute; Javier Perez de Cuellar đ&amp;atilde; đề nghị &amp;ocirc;ng Annan tổ chức chiến dịch giải cứu con tin v&amp;agrave; sơ t&amp;aacute;n gần 1.000 nh&amp;acirc;n vi&amp;ecirc;n quốc tế tại Iraq. &amp;Ocirc;ng Annan cũng l&amp;agrave; người dẫn đầu đ&amp;agrave;m ph&amp;aacute;n với Iraq về việc b&amp;aacute;n dầu th&amp;ocirc; lấy kinh ph&amp;iacute; hỗ trợ nh&amp;acirc;n đạo.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Trong thời gian giữ vai tr&amp;ograve; Tổng Thư k&amp;yacute; LHQ, &amp;ocirc;ng Annan đ&amp;atilde; th&amp;agrave;nh lập Quỹ Kofi Annan &amp;ndash; một tổ chức phi lợi nhuận c&amp;oacute; mục ti&amp;ecirc;u x&amp;uacute;c tiến một thế giới h&amp;ograve;a b&amp;igrave;nh hơn.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;Ocirc;ng Annan từ giữ vai tr&amp;ograve; Đặc ph&amp;aacute;i vi&amp;ecirc;n chung của Li&amp;ecirc;n đo&amp;agrave;n Arab-LHQ về Syria trong năm 2012, nhưng trong th&amp;aacute;ng 8 c&amp;ugrave;ng năm &amp;ocirc;ng đ&amp;atilde; từ chức.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;Ocirc;ng Annan th&amp;ocirc;ng thạo tiếng Anh, tiếng Ph&amp;aacute;p v&amp;agrave; một số ng&amp;ocirc;n ngữ thổ d&amp;acirc;n ch&amp;acirc;u Phi. &amp;Ocirc;ng Annan từng kết h&amp;ocirc;n hai lần v&amp;agrave; c&amp;oacute; 3 người con.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="nhung-nuoi-tiec-cua-kofi-annan-trong-su-nghiep-tai-lien-hop-quoc"&gt;Những nuối tiếc của Kofi Annan trong sự nghiệp tại Li&amp;ecirc;n Hợp Quốc&lt;/h1&gt;
&lt;p&gt;Cựu tổng thư k&amp;yacute; Li&amp;ecirc;n Hợp Quốc (Li&amp;ecirc;n Hợp Quốc) Kofi Annan qua đời ở Thụy Sĩ ng&amp;agrave;y 18/8 ở tuổi 80. Nhiều l&amp;atilde;nh đạo thế giới đ&amp;atilde; b&amp;agrave;y tỏ l&amp;ograve;ng tiếc thương v&amp;agrave; ca ngợi những đ&amp;oacute;ng g&amp;oacute;p của &amp;ocirc;ng cho thế giới.&lt;/p&gt;
&lt;p&gt;Annan nổi danh l&amp;agrave; một nh&amp;agrave; trung gian h&amp;ograve;a giải với th&amp;agrave;nh c&amp;ocirc;ng trong việc chấm dứt xung đột đẫm m&amp;aacute;u trong cuộc đấu đ&amp;aacute; quyền lực khiến hơn 1.200 người chết ở Kenya năm 2007. &amp;Ocirc;ng cũng được ghi nhận với những nỗ lực trong việc x&amp;oacute;a đ&amp;oacute;i giảm ngh&amp;egrave;o v&amp;agrave; ph&amp;ograve;ng chống bệnh AIDS, điều gi&amp;uacute;p &amp;ocirc;ng nhận giải Nobel H&amp;ograve;a b&amp;igrave;nh năm 2001.&lt;/p&gt;
&lt;p&gt;Tuy nhi&amp;ecirc;n, sự nghiệp tại Li&amp;ecirc;n Hợp Quốc của Annan kh&amp;ocirc;ng phải l&amp;uacute;c n&amp;agrave;o cũng gặt h&amp;aacute;i th&amp;agrave;nh c&amp;ocirc;ng. &amp;Ocirc;ng thừa nhận rằng khi l&amp;agrave; người đứng đầu lực lượng g&amp;igrave;n giữ h&amp;ograve;a b&amp;igrave;nh Li&amp;ecirc;n Hợp Quốc năm 1994, &amp;ocirc;ng n&amp;ecirc;n l&amp;agrave;m nhiều hơn để ngăn chặn vụ thảm s&amp;aacute;t 800.000 người Tutsi v&amp;agrave; Hutu &amp;ocirc;n h&amp;ograve;a tại Rwanda, theo Reuters.&lt;/p&gt;
&lt;p&gt;Thời điểm đ&amp;oacute;, tướng Romeo Dallaire, chỉ huy của lực lượng giữ g&amp;igrave;n h&amp;ograve;a b&amp;igrave;nh Li&amp;ecirc;n Hợp Quốc ở Rwanda, gửi điện khẩn cho Annan, th&amp;ocirc;ng b&amp;aacute;o những kẻ cực đoan Hutu đang chuẩn bị vũ kh&amp;iacute; v&amp;agrave; c&amp;oacute; những dấu hiệu cho thấy một cuộc giết người h&amp;agrave;ng loạt sắp xảy ra. Tướng Dallaire đề nghị lực lượng Li&amp;ecirc;n Hợp Quốc c&amp;oacute; những biện ph&amp;aacute;p can thiệp nhanh ch&amp;oacute;ng, nhưng Annan kh&amp;ocirc;ng đ&amp;aacute;p ứng y&amp;ecirc;u cầu n&amp;agrave;y. &lt;/p&gt;
&lt;p&gt;"Hồi ấy t&amp;ocirc;i nghĩ t&amp;ocirc;i đ&amp;atilde; l&amp;agrave;m hết sức m&amp;igrave;nh", Annan n&amp;oacute;i nhiều năm sau đ&amp;oacute;. "Nhưng sau vụ thảm s&amp;aacute;t t&amp;ocirc;i nhận ra rằng m&amp;igrave;nh c&amp;oacute; thể v&amp;agrave; đ&amp;aacute;ng lẽ phải l&amp;agrave;m nhiều hơn".&lt;/p&gt;
&lt;p&gt;&amp;Ocirc;ng cũng hứng chịu chỉ tr&amp;iacute;ch khi 8.000 người Hồi gi&amp;aacute;o bị lực lượng Serbia h&amp;agrave;nh quyết tại Bosnia năm 1995. Lực lượng g&amp;igrave;n giữ h&amp;ograve;a b&amp;igrave;nh qu&amp;aacute; mỏng đ&amp;atilde; kh&amp;ocirc;ng ngăn cản được vụ giết người.&lt;/p&gt;
&lt;p&gt;Annan giữ chức tổng thư k&amp;yacute; Li&amp;ecirc;n Hợp Quốc từ năm 1997 đến năm 2006. &amp;Ocirc;ng cho biết những khoảnh khắc tồi tệ nhất của m&amp;igrave;nh trong qu&amp;atilde;ng thời gian n&amp;agrave;y l&amp;agrave; kh&amp;ocirc;ng thể ngăn chặn được xung đột ở Darfur, Sudan bắt đầu từ năm 2003 khiến h&amp;agrave;ng trăm ngh&amp;igrave;n d&amp;acirc;n thường thiệt mạng, cũng như b&amp;ecirc; bối trong chương tr&amp;igrave;nh đổi dầu lấy lương thực v&amp;agrave; Chiến tranh Iraq 2003 - 2011.&lt;/p&gt;
&lt;p&gt;Đổi dầu lấy lương thực l&amp;agrave; chương tr&amp;igrave;nh được &amp;aacute;p dụng từ năm 1996 để b&amp;aacute;n lượng dầu hạn chế từ Iraq, nước hứng chịu nhiều lệnh trừng phạt quốc tế do đ&amp;atilde; tấn c&amp;ocirc;ng Kuwait, nhằm đổi lấy nhu yếu phẩm. Biện ph&amp;aacute;p n&amp;agrave;y nhằm giảm t&amp;aacute;c động của lệnh trừng phạt đối với d&amp;acirc;n thường Iraq. Năm 2004, chương tr&amp;igrave;nh bị phanh phui l&amp;agrave; c&amp;oacute; gian lận v&amp;agrave; tham nhũng để l&amp;agrave;m lợi cho một số quan chức Li&amp;ecirc;n Hợp Quốc v&amp;agrave; Iraq.&lt;/p&gt;
&lt;p&gt;Li&amp;ecirc;n Hợp Quốc bị chỉ tr&amp;iacute;ch l&amp;agrave; quản l&amp;yacute; lỏng lẻo. Mặc d&amp;ugrave; kết quả điều tra cho thấy Annan kh&amp;ocirc;ng c&amp;oacute; sai tr&amp;aacute;i, con trai duy nhất của &amp;ocirc;ng, Kojo, bị ph&amp;aacute;t hiện đ&amp;atilde; sử dụng c&amp;aacute;c mối quan hệ trong Li&amp;ecirc;n Hợp Quốc để tư lợi.&lt;/p&gt;
&lt;p&gt;Trong cuộc phỏng vấn hồi th&amp;aacute;ng 2/2013, Annan gọi cuộc chiến tranh do Mỹ ph&amp;aacute;t động nhắm v&amp;agrave;o Iraq l&amp;agrave; "thời điểm đen tối nhất cuộc đời" v&amp;agrave; chỉ tr&amp;iacute;ch h&amp;agrave;nh động n&amp;agrave;y của Washington l&amp;agrave; "phi ph&amp;aacute;p".&lt;/p&gt;
&lt;p&gt;Trước khi cuộc x&amp;acirc;m lược nổ ra, Mỹ khẳng định Iraq đang sở hữu v&amp;agrave; ph&amp;aacute;t triển vũ kh&amp;iacute; hủy diệt h&amp;agrave;ng loạt, nhưng Li&amp;ecirc;n Hợp Quốc đ&amp;atilde; kh&amp;ocirc;ng ph&amp;ecirc; chuẩn nghị quyết tấn c&amp;ocirc;ng v&amp;igrave; kh&amp;ocirc;ng c&amp;oacute; đủ chứng cứ. D&amp;ugrave; vậy, tổng thống Mỹ khi đ&amp;oacute; l&amp;agrave; George W. Bush vẫn quyết định ph&amp;aacute;t động Chiến tranh x&amp;acirc;m lược Iraq dựa tr&amp;ecirc;n c&amp;aacute;c c&amp;aacute;o buộc của m&amp;igrave;nh. Khoảng 151.000 - 600.000 người Iraq bị giết trong 3-4 năm đầu ti&amp;ecirc;n của cuộc xung đột.&lt;/p&gt;
&lt;p&gt;"Ch&amp;uacute;ng t&amp;ocirc;i đ&amp;atilde; kh&amp;ocirc;ng thể ngăn chặn n&amp;oacute;. T&amp;ocirc;i đ&amp;atilde; l&amp;agrave;m việc rất chăm chỉ - t&amp;ocirc;i đ&amp;atilde; li&amp;ecirc;n tục điện đ&amp;agrave;m, n&amp;oacute;i chuyện với c&amp;aacute;c l&amp;atilde;nh đạo tr&amp;ecirc;n khắp thế giới. Mỹ kh&amp;ocirc;ng nhận được sự ủng hộ của Hội đồng Bảo an Li&amp;ecirc;n Hợp Quốc", &amp;ocirc;ng nhấn mạnh.&lt;/p&gt;
&lt;p&gt;"Kofi tin rằng trong chiến tranh, ai cũng thua", Pierre Bertrand, cựu quan chức Li&amp;ecirc;n Hợp Quốc York n&amp;oacute;i. "&amp;Ocirc;ng ấy muốn l&amp;agrave;m mọi thứ c&amp;oacute; thể để ngăn chặn xung đột".&lt;/p&gt;
&lt;p&gt;Sự kiện đau đớn nhất trong sự nghiệp l&amp;agrave; vụ đ&amp;aacute;nh bom trụ sở Li&amp;ecirc;n Hiệp Quốc tại Baghdad ng&amp;agrave;y 19/8/2003 khiến 22 người thiệt mạng, sau khi Annan quyết định cho ph&amp;eacute;p nh&amp;acirc;n vi&amp;ecirc;n cao cấp của Li&amp;ecirc;n Hợp Quốc trở lại Iraq dưới sự hối th&amp;uacute;c của Mỹ. Trong số nạn nh&amp;acirc;n thiệt mạng trong vụ đ&amp;aacute;nh bom c&amp;oacute; ph&amp;aacute;i vi&amp;ecirc;n của &amp;ocirc;ng, Sergio Vieira de Mello.&lt;/p&gt;
&lt;p&gt;"T&amp;ocirc;i đau l&amp;ograve;ng như khi mất đi em g&amp;aacute;i sinh đ&amp;ocirc;i", Annan n&amp;oacute;i trong cuộc họp b&amp;aacute;o cuối c&amp;ugrave;ng với tư c&amp;aacute;ch tổng thư k&amp;yacute; Li&amp;ecirc;n Hợp Quốc. Em &amp;ocirc;ng, Efua Annan, qua đời v&amp;igrave; bệnh năm 1991.&lt;/p&gt;
&lt;p&gt;Khi đ&amp;oacute;n sinh nhật lần thứ 80 hồi th&amp;aacute;ng 4, Annan nhấn mạnh thế giới cần c&amp;aacute;c l&amp;atilde;nh đạo mạnh mẽ để gi&amp;uacute;p giải quyết khủng hoảng.&lt;/p&gt;
&lt;p&gt;"Ch&amp;uacute;ng ta đ&amp;atilde; c&amp;oacute; những kh&amp;oacute; khăn trong qu&amp;aacute; khứ nhưng trong một số trường hợp, c&amp;aacute;c l&amp;atilde;nh đạo đ&amp;atilde; tạo ra sự kh&amp;aacute;c biệt," &amp;ocirc;ng n&amp;oacute;i. "T&amp;ocirc;i l&amp;agrave; một người lạc quan bướng bỉnh, t&amp;ocirc;i sinh ra đ&amp;atilde; lạc quan v&amp;agrave; vẫn sẽ l&amp;agrave; người lạc quan. Nếu mất hy vọng th&amp;igrave; sẽ mất tất cả, t&amp;ocirc;i khuy&amp;ecirc;n c&amp;aacute;c bạn lu&amp;ocirc;n giữ hy vọng".&lt;/p&gt;
&lt;p&gt;Nguồn: H&amp;agrave; Linh (B&amp;aacute;o Tin tức) v&amp;agrave; Phương Vũ (Vnexpress.net)&lt;/p&gt;</content><category term="Inspiration"></category><category term="OTHER"></category></entry><entry><title>Giáo sư Nguyễn Xiển - nhà khoa học khí tượng hàng đầu</title><link href="/blog/other/2018/giao-su-nguyen-xien/" rel="alternate"></link><published>2018-08-21T13:18:30+00:00</published><updated>2018-08-21T13:18:30+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/other/2018/giao-su-nguyen-xien/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Gi&amp;aacute;o sư Nguyễn Xiển, nh&amp;agrave; khoa học kh&amp;iacute; tượng h&amp;agrave;ng đầu ở Việt Nam, kiến tr&amp;uacute;c sư của ng&amp;agrave;nh kh&amp;iacute; tượng c&amp;aacute;ch mạng Việt Nam, đ&amp;atilde; để lại cho ng&amp;agrave;nh kh&amp;iacute; tượng thủy văn nhiều c&amp;ocirc;ng tr&amp;igrave;nh khoa học gi&amp;aacute; trị. &amp;Ocirc;ng l&amp;agrave; gương s&amp;aacute;ng của một người l&amp;atilde;nh đạo s&amp;acirc;u s&amp;aacute;t, gương mẫu, một nh&amp;agrave; khoa học nghi&amp;ecirc;m t&amp;uacute;c, s&amp;aacute;ng tạo lu&amp;ocirc;n hướng khoa học phục vụ sản xuất v&amp;agrave; đời sống.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id="tri-thuc-yeu-nuoc"&gt;Tr&amp;iacute; thức y&amp;ecirc;u nước&lt;/h1&gt;
&lt;p&gt;Gi&amp;aacute;o sư Nguyễn Xiển sinh ng&amp;agrave;y 27/7/1907, tại th&amp;agrave;nh phố Vinh, Nghệ An trong một gia đ&amp;igrave;nh Nho học l&amp;acirc;u đời, c&amp;oacute; tiếng của xứ Nghệ.&lt;/p&gt;
&lt;p&gt;Cuộc đời v&amp;agrave; sự nghiệp của Gi&amp;aacute;o sư Nguyễn Xiển l&amp;agrave; sự kết tinh từ hai nền văn h&amp;oacute;a Đ&amp;ocirc;ng-T&amp;acirc;y, l&amp;agrave; h&amp;igrave;nh ảnh ti&amp;ecirc;u biểu của một bậc sĩ phu Bắc H&amp;agrave; thời hiện đại ở thế kỷ XX. Từ một tr&amp;iacute; thức T&amp;acirc;y học …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Gi&amp;aacute;o sư Nguyễn Xiển, nh&amp;agrave; khoa học kh&amp;iacute; tượng h&amp;agrave;ng đầu ở Việt Nam, kiến tr&amp;uacute;c sư của ng&amp;agrave;nh kh&amp;iacute; tượng c&amp;aacute;ch mạng Việt Nam, đ&amp;atilde; để lại cho ng&amp;agrave;nh kh&amp;iacute; tượng thủy văn nhiều c&amp;ocirc;ng tr&amp;igrave;nh khoa học gi&amp;aacute; trị. &amp;Ocirc;ng l&amp;agrave; gương s&amp;aacute;ng của một người l&amp;atilde;nh đạo s&amp;acirc;u s&amp;aacute;t, gương mẫu, một nh&amp;agrave; khoa học nghi&amp;ecirc;m t&amp;uacute;c, s&amp;aacute;ng tạo lu&amp;ocirc;n hướng khoa học phục vụ sản xuất v&amp;agrave; đời sống.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id="tri-thuc-yeu-nuoc"&gt;Tr&amp;iacute; thức y&amp;ecirc;u nước&lt;/h1&gt;
&lt;p&gt;Gi&amp;aacute;o sư Nguyễn Xiển sinh ng&amp;agrave;y 27/7/1907, tại th&amp;agrave;nh phố Vinh, Nghệ An trong một gia đ&amp;igrave;nh Nho học l&amp;acirc;u đời, c&amp;oacute; tiếng của xứ Nghệ.&lt;/p&gt;
&lt;p&gt;Cuộc đời v&amp;agrave; sự nghiệp của Gi&amp;aacute;o sư Nguyễn Xiển l&amp;agrave; sự kết tinh từ hai nền văn h&amp;oacute;a Đ&amp;ocirc;ng-T&amp;acirc;y, l&amp;agrave; h&amp;igrave;nh ảnh ti&amp;ecirc;u biểu của một bậc sĩ phu Bắc H&amp;agrave; thời hiện đại ở thế kỷ XX. Từ một tr&amp;iacute; thức T&amp;acirc;y học, Gi&amp;aacute;o sư đ&amp;atilde; được B&amp;aacute;c Hồ cảm h&amp;oacute;a trở th&amp;agrave;nh người c&amp;aacute;ch mạng, một l&amp;ograve;ng đi theo l&amp;yacute; tưởng của Người.&lt;/p&gt;
&lt;p&gt;Với tr&amp;iacute; th&amp;ocirc;ng minh vốn c&amp;oacute; v&amp;agrave; khả năng tự học hiếm thấy, &amp;ocirc;ng đỗ đầu kỳ thi t&amp;uacute; t&amp;agrave;i T&amp;acirc;y v&amp;agrave; c&amp;ugrave;ng Ho&amp;agrave;ng Xu&amp;acirc;n H&amp;atilde;n nhận học bổng của Hội Như T&amp;acirc;y sang Ph&amp;aacute;p du học.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gi&amp;aacute;o sư Nguyễn Xiển vinh dự được đứng c&amp;ugrave;ng Chủ tịch Hồ Ch&amp;iacute; Minh trong Đo&amp;agrave;n Chủ tịch kỳ họp Quốc hội năm 1957" src="https://media.baotintuc.vn/2017/07/25/19/57/GSNguyenxien1.jpg"/&gt;.&lt;/p&gt;
&lt;p&gt;Những năm ở Ph&amp;aacute;p, &amp;ocirc;ng theo học tại trường đại học Toulouse v&amp;agrave; đỗ cử nh&amp;acirc;n. D&amp;ugrave; chưa c&amp;oacute; xu hướng ch&amp;iacute;nh trị r&amp;otilde; rệt nhưng Nguyễn Xiển đ&amp;atilde; nghĩ rằng nếu học giỏi, nắm vững khoa học sẽ gi&amp;uacute;p c&amp;aacute;ch mạng nhiều hơn, c&amp;aacute;ch mạng kh&amp;ocirc;ng thể thiếu khoa học, x&amp;acirc;y dựng đất nước c&amp;agrave;ng cần đến khoa học. &amp;Ocirc;ng kh&amp;acirc;m phục l&amp;ograve;ng y&amp;ecirc;u nước nhiệt th&amp;agrave;nh của nh&amp;agrave; c&amp;aacute;ch mạng Nguyễn &amp;Aacute;i Quốc, gặp gỡ, quen biết, gần gũi những sinh vi&amp;ecirc;n cộng sản trẻ tuổi như Trần Văn Gi&amp;agrave;u, Phan Tư Nghĩa... tham gia c&amp;aacute;c hoạt động y&amp;ecirc;u nước, trong đ&amp;oacute; c&amp;oacute; cuộc m&amp;iacute;t tinh lớn chống thực d&amp;acirc;n Ph&amp;aacute;p đ&amp;agrave;n &amp;aacute;p phong tr&amp;agrave;o X&amp;ocirc; Viết - Nghệ Tĩnh.&lt;/p&gt;
&lt;p&gt;Khi về nước, Nguyễn Xiển rất băn khoăn trước lối đi cho ch&amp;iacute;nh cuộc đời m&amp;igrave;nh. &amp;Ocirc;ng quyết định ra H&amp;agrave; Nội, dạy học ở c&amp;aacute;c trường tư thục bởi &amp;ocirc;ng kh&amp;ocirc;ng muốn l&amp;agrave;m quan cho triều đ&amp;igrave;nh Huế.&lt;/p&gt;
&lt;p&gt;C&amp;aacute;ch mạng th&amp;aacute;ng T&amp;aacute;m th&amp;agrave;nh c&amp;ocirc;ng, l&amp;ograve;ng y&amp;ecirc;u nước của &amp;ocirc;ng chỉ ph&amp;aacute;t huy sau khi gặp Chủ tịch Hồ Ch&amp;iacute; Minh. Nếu kh&amp;ocirc;ng c&amp;oacute; Người, &amp;ocirc;ng m&amp;atilde;i m&amp;atilde;i chỉ l&amp;agrave; một tr&amp;iacute; thức T&amp;acirc;y học, cả đời đi t&amp;igrave;m l&amp;yacute; tưởng.&lt;/p&gt;
&lt;p&gt;Nguyễn Xiển đ&amp;atilde; c&amp;oacute; cuộc gặp gỡ kh&amp;ocirc;ng ngờ với Chủ tịch Hồ Ch&amp;iacute; Minh, được Người n&amp;oacute;i chuyện theo tinh thần &amp;ldquo;Quốc gia hưng vong, thất phu hữu tr&amp;aacute;ch&amp;rdquo;, ch&amp;iacute;nh B&amp;aacute;c đ&amp;atilde; thức tỉnh truyền thống y&amp;ecirc;u nước &amp;acirc;m ỉ trong l&amp;ograve;ng một người tr&amp;iacute; thức xứ Nghệ, khiến &amp;ocirc;ng mạnh bước theo c&amp;aacute;ch mạng, kh&amp;aacute;ng chiến... &amp;Ocirc;ng đ&amp;atilde; x&amp;aacute;c định r&amp;otilde; con đường m&amp;igrave;nh sẽ đi, to&amp;agrave;n t&amp;acirc;m theo c&amp;aacute;ch mạng.&lt;/p&gt;
&lt;p&gt;Gi&amp;aacute;o sư Nguyễn Xiển khẳng định rằng, l&amp;agrave; tr&amp;iacute; thức c&amp;oacute; l&amp;ograve;ng y&amp;ecirc;u nước tiềm ẩn, &amp;ocirc;ng đ&amp;atilde; đi theo con đường C&amp;aacute;ch mạng Th&amp;aacute;ng T&amp;aacute;m như một bản năng v&amp;agrave; &amp;ocirc;ng c&amp;oacute; l&amp;ograve;ng tin s&amp;acirc;u sắc ở Chủ tịch Hồ Ch&amp;iacute; Minh, con người ch&amp;iacute;nh trị văn h&amp;oacute;a vĩ đại v&amp;agrave; tinh tế, c&amp;oacute; sức tập hợp v&amp;agrave; l&amp;ocirc;i cuốn rộng r&amp;atilde;i c&amp;aacute;c tầng lớp nh&amp;acirc;n d&amp;acirc;n, đ&amp;atilde; chứng tỏ bản lĩnh dẫn dắt v&amp;agrave; ch&amp;egrave;o chống tuyệt vời của m&amp;igrave;nh trước những cơn s&amp;oacute;ng gi&amp;oacute; phức tạp, dồn dập của thời kỳ sau C&amp;aacute;ch mạng th&amp;aacute;ng T&amp;aacute;m khi ch&amp;iacute;nh quyền nh&amp;acirc;n d&amp;acirc;n c&amp;ograve;n trứng nước.&lt;/p&gt;
&lt;h1 id="nha-khoa-hoc-ve-khi-tuong-hang-dau-o-viet-nam"&gt;Nh&amp;agrave; khoa học về kh&amp;iacute; tượng h&amp;agrave;ng đầu ở Việt Nam&lt;/h1&gt;
&lt;p&gt;Nguyễn Xiển được B&amp;aacute;c giao giữ chức Chủ tịch Ủy ban H&amp;agrave;nh ch&amp;iacute;nh Bắc bộ ki&amp;ecirc;m Gi&amp;aacute;m đốc Nha kh&amp;iacute; tượng.&lt;/p&gt;
&lt;p&gt;L&amp;agrave; Chủ tịch Uỷ ban H&amp;agrave;nh ch&amp;iacute;nh Bắc Bộ, &amp;ocirc;ng đ&amp;atilde; giải quyết nhiều c&amp;ocirc;ng việc đang rất khẩn trương l&amp;uacute;c đ&amp;oacute; như tiếp quản bộ m&amp;aacute;y ch&amp;iacute;nh quyền, x&amp;acirc;y dựng c&amp;aacute;c ủy ban h&amp;agrave;nh ch&amp;iacute;nh từ tỉnh đến x&amp;atilde;, đặc biệt l&amp;agrave; việc sửa chữa đ&amp;ecirc; điều bị vỡ trong trận lụt lịch sử năm 1945... Trong trận lũ lịch sử ấy, một loạt đ&amp;ecirc; quan trọng như đ&amp;ecirc; s&amp;ocirc;ng Thao, s&amp;ocirc;ng L&amp;ocirc;, nhiều kh&amp;uacute;c đ&amp;ecirc; s&amp;ocirc;ng Hồng... bị vỡ. Nước lụt đ&amp;atilde; l&amp;agrave;m ngập h&amp;agrave;ng vạn mẫu ruộng. Việc cần l&amp;agrave;m ngay l&amp;agrave; hợp long những đoạn đ&amp;ecirc; vỡ để kịp thời bảo vệ vụ m&amp;ugrave;a năm đ&amp;oacute;; tiếp đến l&amp;agrave; phải đắp th&amp;ecirc;m nhiều đ&amp;ecirc; mới v&amp;ograve;ng quanh c&amp;aacute;c chỗ đ&amp;ecirc; vỡ, tu bổ những đoạn đ&amp;ecirc; xung yếu.&lt;/p&gt;
&lt;p&gt;&amp;Ocirc;ng phụ tr&amp;aacute;ch ng&amp;agrave;nh kh&amp;iacute; tượng v&amp;agrave; đ&amp;acirc;y l&amp;agrave; lĩnh vực l&amp;acirc;u nhất, xuy&amp;ecirc;n suốt cả cuộc đời, bắt đầu từ trước c&amp;aacute;ch mạng cho đến khi về hưu. &amp;Ocirc;ng kh&amp;ocirc;ng chỉ l&amp;agrave; người l&amp;atilde;nh đạo m&amp;agrave; &amp;ocirc;ng c&amp;ograve;n trực tiếp tham gia mọi c&amp;ocirc;ng việc, như một c&amp;aacute;n bộ kh&amp;iacute; tượng b&amp;igrave;nh thường kh&amp;aacute;c, từ quy tụ c&amp;aacute;n bộ, trực tiếp huấn luyện đ&amp;agrave;o tạo, đến t&amp;igrave;m kiếm địa điểm để x&amp;acirc;y dựng c&amp;aacute;c c&amp;ocirc;ng tr&amp;igrave;nh kh&amp;iacute; tượng.&lt;/p&gt;
&lt;p&gt;Trong mọi c&amp;ocirc;ng việc của ng&amp;agrave;nh, &amp;ocirc;ng lu&amp;ocirc;n s&amp;acirc;u s&amp;aacute;t đến từng chi tiết cụ thể. Điều đ&amp;oacute; thể hiện ở h&amp;igrave;nh ảnh vị gi&amp;aacute;m đốc đi đến hầu hết c&amp;aacute;c trạm quan trắc kh&amp;iacute; tượng ở miền bắc để kiểm tra, &amp;acirc;n cần nhắc nhở khi thấy lều quan trắc c&amp;ograve;n bẩn. Kh&amp;ocirc;ng tuần n&amp;agrave;o &amp;ocirc;ng kh&amp;ocirc;ng đến từng ph&amp;ograve;ng ban trong cơ quan để nắm t&amp;igrave;nh h&amp;igrave;nh, chỉ bảo nghi&amp;ecirc;m khắc khi thấy ai đ&amp;oacute; t&amp;aacute;m chuyện hoặc ngồi l&amp;agrave;m việc kh&amp;ocirc;ng đ&amp;uacute;ng tư thế. Khi c&amp;oacute; b&amp;atilde;o lũ, &amp;ocirc;ng thức trắng đ&amp;ecirc;m c&amp;ugrave;ng c&amp;aacute;c dự b&amp;aacute;o vi&amp;ecirc;n, theo d&amp;otilde;i chặt chẽ mọi diễn biến t&amp;igrave;nh h&amp;igrave;nh b&amp;atilde;o lũ, đề xuất kịp thời c&amp;aacute;c biện ph&amp;aacute;p ph&amp;ograve;ng tr&amp;aacute;nh. Những l&amp;uacute;c đ&amp;oacute;, &amp;ocirc;ng vừa l&amp;agrave; người l&amp;atilde;nh đạo vừa l&amp;agrave; một dự b&amp;aacute;o vi&amp;ecirc;n kh&amp;iacute; tượng gi&amp;agrave;u kinh nghiệm.&lt;/p&gt;
&lt;p&gt;Ng&amp;agrave;nh kh&amp;iacute; tượng do &amp;ocirc;ng l&amp;atilde;nh đạo đ&amp;atilde; duy tr&amp;igrave; tốt hoạt động trong chiến tranh chống Mỹ cứu nước để phục vụ tốt cho sản xuất v&amp;agrave; chiến đấu. Ng&amp;agrave;nh đ&amp;atilde; ph&amp;aacute;t động phong tr&amp;agrave;o chống chiến tranh kh&amp;iacute; tượng của đế quốc Mỹ. Rồi sau đ&amp;oacute; khi thống nhất đất nước, ng&amp;agrave;nh đ&amp;atilde; nhanh ch&amp;oacute;ng tổ chức mạng lưới hoạt động tr&amp;ecirc;n to&amp;agrave;n đất nước, từng bước hiện đại h&amp;oacute;a.&lt;/p&gt;
&lt;p&gt;Gi&amp;aacute;o sư Nguyễn Xiển l&amp;agrave; nh&amp;agrave; khoa học về kh&amp;iacute; tượng h&amp;agrave;ng đầu ở Việt Nam. C&amp;aacute;c c&amp;ocirc;ng tr&amp;igrave;nh khoa học của &amp;ocirc;ng hoặc do &amp;ocirc;ng chủ bi&amp;ecirc;n l&amp;agrave; những mẫu mực về t&amp;iacute;nh khoa học, t&amp;iacute;nh s&amp;aacute;ng tạo v&amp;agrave; t&amp;iacute;nh thực tiễn, l&amp;agrave; t&amp;agrave;i liệu cơ bản, được sử dụng l&amp;acirc;u d&amp;agrave;i ở nhiều ng&amp;agrave;nh, đặc biệt trong lĩnh vực n&amp;ocirc;ng nghiệp, kh&amp;iacute; hậu v&amp;agrave; m&amp;ocirc;i trường.&lt;/p&gt;
&lt;p&gt;Ngo&amp;agrave;i c&amp;ocirc;ng t&amp;aacute;c kh&amp;iacute; tượng thủy văn, &amp;ocirc;ng c&amp;ograve;n tham gia c&amp;ocirc;ng t&amp;aacute;c khoa học gi&amp;aacute;o dục. Ngay trong thời kỳ kh&amp;aacute;ng chiến chống Ph&amp;aacute;p gian khổ, &amp;ocirc;ng ho&amp;agrave;n th&amp;agrave;nh 2 c&amp;ocirc;ng tr&amp;igrave;nh: &amp;ldquo;To&amp;aacute;n học đại cương&amp;rdquo; v&amp;agrave; &amp;ldquo;Cơ học thuần l&amp;yacute;&amp;rdquo; bằng tiếng Việt, hợp t&amp;aacute;c với c&amp;aacute;c nh&amp;agrave; khoa học nổi tiếng, gi&amp;agrave;u t&amp;acirc;m huyết như tiến sĩ to&amp;aacute;n học cơ bản v&amp;agrave; trường sư phạm cao cấp l&amp;agrave; những trường đại học đầu ti&amp;ecirc;n đ&amp;agrave;o tạo nhiều c&amp;aacute;n bộ khoa học kỹ thuật, phục vụ cho sự nghiệp kh&amp;aacute;ng chiến kiến quốc.&lt;/p&gt;
&lt;p&gt;Ngo&amp;agrave;i ra, &amp;ocirc;ng c&amp;ograve;n c&amp;ugrave;ng nhiều nh&amp;agrave; tr&amp;iacute; thức ra tờ b&amp;aacute;o &amp;ldquo;Khoa học thường thức&amp;rdquo; (nay l&amp;agrave; b&amp;aacute;o Khoa học v&amp;agrave; Đời sống) để tuy&amp;ecirc;n truyền, phổ biến khoa học kỹ thuật. &amp;Ocirc;ng cũng l&amp;agrave; t&amp;aacute;c giả của nhiều b&amp;agrave;i b&amp;aacute;o về ch&amp;iacute;nh trị, x&amp;atilde; hội, văn h&amp;oacute;a v&amp;agrave; khoa học kỹ thuật. Năm 1959, gi&amp;aacute;o sư Nguyễn Xiển c&amp;ugrave;ng với một số nh&amp;agrave; tr&amp;iacute; thức c&amp;oacute; t&amp;acirc;m huyết tổ chức ra Ban vận động th&amp;agrave;nh lập Hội phổ biến khoa học v&amp;agrave; kỹ thuật Việt Nam do &amp;ocirc;ng l&amp;agrave;m trưởng ban.&lt;/p&gt;
&lt;p&gt;Gi&amp;aacute;o sư Nguyễn Xiển l&amp;agrave; nh&amp;agrave; hoạt động c&amp;aacute;ch mạng đa ng&amp;agrave;nh, đa lĩnh vực, đ&amp;atilde; để lại những dấu ấn đẹp cho c&amp;aacute;c thế hệ sau.&lt;/p&gt;
&lt;p&gt;Gi&amp;aacute;o sư đ&amp;atilde; được Đảng, Nh&amp;agrave; nước trao tặng nhiều phần thưởng cao qu&amp;yacute;, trong đ&amp;oacute; c&amp;oacute; Hu&amp;acirc;n chương Sao v&amp;agrave;ng Hu&amp;acirc;n chương Hồ Ch&amp;iacute; Minh v&amp;agrave; Giải thưởng Hồ Ch&amp;iacute; Minh.&lt;/p&gt;
&lt;p&gt;&amp;Ocirc;ng mất ng&amp;agrave;y 9/11/1997, hưởng thọ 90 tuổi.&lt;/p&gt;
&lt;p&gt;Nguồn: &lt;a href="https://baotintuc.vn/nhan-vat-su-kien/nguyen-xien-nha-khoa-hoc-khi-tuong-hang-dau-viet-nam-20170725195757945.htm"&gt;Hồng Quảng (TTXVN)&lt;/a&gt;&lt;/p&gt;</content><category term="OTHER"></category></entry><entry><title>Thiên tài không bằng cấp Michael Faraday</title><link href="/blog/other/2018/thien-tai-faraday/" rel="alternate"></link><published>2018-08-21T13:09:50+00:00</published><updated>2018-08-21T13:09:50+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/other/2018/thien-tai-faraday/</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;Michael Faraday l&amp;agrave; nh&amp;agrave; b&amp;aacute;c học được cả thế giới biết đến bởi &amp;ocirc;ng ch&amp;iacute;nh l&amp;agrave; người c&amp;oacute; c&amp;ocirc;ng lớn nhất trong việc biến từ th&amp;agrave;nh điện - nguồn năng lượng sạch v&amp;agrave; phổ biến nhất ng&amp;agrave;y nay.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id="tu-cau-be-dong-sach-ngheo-ham-hoc"&gt;Từ cậu b&amp;eacute; đ&amp;oacute;ng s&amp;aacute;ch ngh&amp;egrave;o ham học&lt;/h1&gt;
&lt;p&gt;Michael Faraday sinh ng&amp;agrave;y 22/9/1791 ở Newington Butts, ngoại &amp;ocirc; Lu&amp;acirc;n Đ&amp;ocirc;n, trong một gia đ&amp;igrave;nh ngh&amp;egrave;o c&amp;oacute; bố l&amp;agrave;m nghề thợ r&amp;egrave;n. Từ nhỏ, Faraday đ&amp;atilde; tỏ ra th&amp;ocirc;ng minh v&amp;agrave; hiếu học, nhưng phải sớm nghỉ học để phụ gi&amp;uacute;p gia đ&amp;igrave;nh.&lt;/p&gt;
&lt;p&gt;Khi đời sống của gia đ&amp;igrave;nh c&amp;agrave;ng kh&amp;oacute; khăn, năm 1804, khi mới 13 tuổi, Faraday đến xin việc tại &amp;ldquo;Hiệu b&amp;aacute;n s&amp;aacute;ch v&amp;agrave; đ&amp;oacute;ng s&amp;aacute;ch Rit&amp;ocirc;&amp;rdquo; ở Lu&amp;acirc;n Đ&amp;ocirc;n. Faraday vừa học nghề đ&amp;oacute;ng s&amp;aacute;ch, vừa tự học qua việc đọc s&amp;aacute;ch. &amp;Ocirc;ng đặc biệt ch&amp;uacute; &amp;yacute; đến c&amp;aacute;c quyển s&amp;aacute;ch về khoa học v&amp;agrave; c&amp;ograve;n tự l&amp;agrave;m …&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;Michael Faraday l&amp;agrave; nh&amp;agrave; b&amp;aacute;c học được cả thế giới biết đến bởi &amp;ocirc;ng ch&amp;iacute;nh l&amp;agrave; người c&amp;oacute; c&amp;ocirc;ng lớn nhất trong việc biến từ th&amp;agrave;nh điện - nguồn năng lượng sạch v&amp;agrave; phổ biến nhất ng&amp;agrave;y nay.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id="tu-cau-be-dong-sach-ngheo-ham-hoc"&gt;Từ cậu b&amp;eacute; đ&amp;oacute;ng s&amp;aacute;ch ngh&amp;egrave;o ham học&lt;/h1&gt;
&lt;p&gt;Michael Faraday sinh ng&amp;agrave;y 22/9/1791 ở Newington Butts, ngoại &amp;ocirc; Lu&amp;acirc;n Đ&amp;ocirc;n, trong một gia đ&amp;igrave;nh ngh&amp;egrave;o c&amp;oacute; bố l&amp;agrave;m nghề thợ r&amp;egrave;n. Từ nhỏ, Faraday đ&amp;atilde; tỏ ra th&amp;ocirc;ng minh v&amp;agrave; hiếu học, nhưng phải sớm nghỉ học để phụ gi&amp;uacute;p gia đ&amp;igrave;nh.&lt;/p&gt;
&lt;p&gt;Khi đời sống của gia đ&amp;igrave;nh c&amp;agrave;ng kh&amp;oacute; khăn, năm 1804, khi mới 13 tuổi, Faraday đến xin việc tại &amp;ldquo;Hiệu b&amp;aacute;n s&amp;aacute;ch v&amp;agrave; đ&amp;oacute;ng s&amp;aacute;ch Rit&amp;ocirc;&amp;rdquo; ở Lu&amp;acirc;n Đ&amp;ocirc;n. Faraday vừa học nghề đ&amp;oacute;ng s&amp;aacute;ch, vừa tự học qua việc đọc s&amp;aacute;ch. &amp;Ocirc;ng đặc biệt ch&amp;uacute; &amp;yacute; đến c&amp;aacute;c quyển s&amp;aacute;ch về khoa học v&amp;agrave; c&amp;ograve;n tự l&amp;agrave;m th&amp;iacute; nghiệm đơn giản để kiểm nghiệm lại những điều khẳng định trong s&amp;aacute;ch. Cứ thế trong thời gian rảnh rỗi, Faraday lại say m&amp;ecirc; c&amp;ugrave;ng những điều l&amp;yacute; th&amp;uacute; của khoa học, nhất l&amp;agrave; lĩnh vực điện năng. Cho đến một ng&amp;agrave;y, anh thợ đ&amp;oacute;ng s&amp;aacute;ch trẻ Faraday h&amp;aacute;o hức dự c&amp;aacute;c lớp học buổi tối do Hội Triết học tổ chức để trau dồi kiến thức, b&amp;ugrave; lại khoảng thời gian kh&amp;ocirc;ng được cắp s&amp;aacute;ch đến trường. Faraday ham học đến nỗi đ&amp;ecirc;m n&amp;agrave;o cũng đọc s&amp;aacute;ch tới khuya khiến cho nhiều lần ngủ gật trong giờ l&amp;agrave;m việc.&lt;/p&gt;
&lt;p&gt;Năm 1812, Faraday được người chủ hiệu s&amp;aacute;ch tặng cho một tấm v&amp;eacute; tham dự buổi thuyết giảng của gi&amp;aacute;o sư H&amp;oacute;a học Humphry Davy, hội vi&amp;ecirc;n Hội Khoa học Ho&amp;agrave;ng gia Lu&amp;acirc;n Đ&amp;ocirc;n. Faraday sau đ&amp;oacute; đ&amp;oacute;ng một quyển s&amp;aacute;ch c&amp;oacute; t&amp;ecirc;n &amp;ldquo;Ghi ch&amp;eacute;p về buổi diễn thuyết của tước sĩ Humphry Davy&amp;rdquo; v&amp;agrave; một bức thư tự tiến cử gửi tới gi&amp;aacute;o sư Davy. Nội dung ghi ch&amp;eacute;p c&amp;ugrave;ng những kiến giải ri&amp;ecirc;ng của Faraday thể hiện trong quyển s&amp;aacute;ch đ&amp;atilde; khiến gi&amp;aacute;o sư Davy hết sức ấn tượng.&lt;/p&gt;
&lt;h1 id="phu-ta-phong-thi-nghiem-voi-giao-su-davy"&gt;Phụ t&amp;aacute; ph&amp;ograve;ng th&amp;iacute; nghiệm với gi&amp;aacute;o sư Davy&lt;/h1&gt;
&lt;p&gt;Th&amp;aacute;ng 10/1812, cuộc đời Faraday đ&amp;atilde; bước hẳn sang một trang mới khi được nhận l&amp;agrave;m phụ t&amp;aacute; ở ph&amp;ograve;ng th&amp;iacute; nghiệm của gi&amp;aacute;o sư Davy. D&amp;ugrave; chỉ nhận được số lương &amp;iacute;t ỏi, Faraday vẫn hăng h&amp;aacute;i với c&amp;ocirc;ng việc. Faraday kh&amp;ocirc;ng những ghi ch&amp;eacute;p rất ch&amp;iacute;nh x&amp;aacute;c c&amp;aacute;c &amp;yacute; tưởng khoa học của Davy m&amp;agrave; c&amp;ograve;n tham gia đ&amp;oacute;ng g&amp;oacute;p &amp;yacute; kiến, ph&amp;acirc;n t&amp;iacute;ch c&amp;aacute;c số liệu thực nghiệm, nhận x&amp;eacute;t c&amp;aacute;c kết luận kh&amp;aacute;i qu&amp;aacute;t của nh&amp;agrave; b&amp;aacute;c học. Faraday được thăng chức trợ l&amp;yacute; khoa học v&amp;agrave;o th&amp;aacute;ng 3/1813.&lt;/p&gt;
&lt;p&gt;Từ năm 1813-1815, trong c&amp;aacute;c chuyến đi đến Ph&amp;aacute;p, &amp;Yacute; của gi&amp;aacute;o sư Davy, Faraday đều được đi c&amp;ugrave;ng v&amp;agrave; c&amp;oacute; cơ hội gặp gỡ, học hỏi từ nhiều nh&amp;agrave; b&amp;aacute;c học như Amp&amp;egrave;re, De la Rive... Faraday đ&amp;atilde; rất chịu kh&amp;oacute; ghi ch&amp;eacute;p v&amp;agrave; t&amp;iacute;ch lũy kiến thức trong suốt h&amp;agrave;nh tr&amp;igrave;nh.&lt;/p&gt;
&lt;p&gt;Lần đầu ti&amp;ecirc;n Faraday viết một luận văn khoa học l&amp;agrave; v&amp;agrave;o năm 1816 dưới sự chỉ đạo của gi&amp;aacute;o sư Davy. Chỉ trong 2 năm sau đ&amp;oacute;, &amp;ocirc;ng tiếp tục c&amp;oacute; tới 17 b&amp;agrave;i luận văn ph&amp;acirc;n t&amp;iacute;ch H&amp;oacute;a học.&lt;/p&gt;
&lt;p&gt;Từ năm 1818 đến năm 1823, trong qu&amp;aacute; tr&amp;igrave;nh nghi&amp;ecirc;n cứu để phục chế th&amp;eacute;p Faraday đ&amp;atilde; s&amp;aacute;ng tạo ra phương ph&amp;aacute;p ph&amp;acirc;n t&amp;iacute;ch kim loại.&lt;/p&gt;
&lt;p&gt;Năm 1821, Faraday cưới Sarah Barnard. Mặc d&amp;ugrave; đ&amp;atilde; lập gia đ&amp;igrave;nh, người phụ t&amp;aacute; của gi&amp;aacute;o sư Davy vẫn cần c&amp;ugrave; ng&amp;agrave;y hai buổi tới chuẩn bị b&amp;agrave;i giảng cho c&amp;aacute;c gi&amp;aacute;o sư của Hội khoa học Ho&amp;agrave;ng gia, v&amp;agrave; nhiều buổi trưa, buổi tối anh vẫn cặm cụi ở lại ph&amp;ograve;ng th&amp;iacute; nghiệm đọc nốt một chương s&amp;aacute;ch hay l&amp;agrave;m nốt một th&amp;iacute; nghiệm dở dang.&lt;/p&gt;
&lt;p&gt;Năm 1824, Faraday được bầu l&amp;agrave;m hội vi&amp;ecirc;n Hội Khoa học Ho&amp;agrave;ng gia Lu&amp;acirc;n Đ&amp;ocirc;n v&amp;agrave; bắt đầu giảng dạy tại Học viện Ho&amp;agrave;ng gia Anh.&lt;/p&gt;
&lt;p&gt;Năm 1825, Faraday bắt tay v&amp;agrave;o việc nghi&amp;ecirc;n cứu loại kh&amp;iacute; thể d&amp;ugrave;ng để chiếu s&amp;aacute;ng cho th&amp;agrave;nh phố Lu&amp;acirc;n Đ&amp;ocirc;n. Loại kh&amp;iacute; n&amp;agrave;y được đặt t&amp;ecirc;n l&amp;agrave; kh&amp;iacute; Clo. Cũng trong năm n&amp;agrave;y, Faraday được giao tr&amp;aacute;ch nhiệm chỉ đạo ph&amp;ograve;ng th&amp;iacute; nghiệm.&lt;/p&gt;
&lt;h1 id="den-nha-bac-hoc-thien-tai-khong-bang-cap"&gt;Đến nh&amp;agrave; b&amp;aacute;c học thi&amp;ecirc;n t&amp;agrave;i kh&amp;ocirc;ng bằng cấp&lt;/h1&gt;
&lt;p&gt;Giai đoạn năm 1830-1839 l&amp;agrave; thời kỳ m&amp;agrave; Faraday đạt được nhiều th&amp;agrave;nh quả khoa học nhất. &amp;Ocirc;ng bắt tay v&amp;agrave;o việc nghi&amp;ecirc;n cứu vật l&amp;yacute; m&amp;agrave; cơ bản l&amp;agrave; phần điện học hiện đại.&lt;/p&gt;
&lt;p&gt;Ng&amp;agrave;y 24/11/1831, Faraday b&amp;aacute;o c&amp;aacute;o trước Học viện Ho&amp;agrave;ng gia về vấn đề ph&amp;aacute;t hiện cảm ứng của điện từ. &amp;Ocirc;ng vinh dự được mời giữ chức chủ tịch Hội Khoa học Ho&amp;agrave;ng gia nhưng đ&amp;atilde; từ chối để chuy&amp;ecirc;n t&amp;acirc;m theo đuổi c&amp;aacute;c nghi&amp;ecirc;n cứu khoa học.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Th&amp;iacute; nghiệm của Faraday về cảm ứng điện từ năm 1931 - Ảnh: en.wikipedia.org" src="http://review.siu.edu.vn/Upload/Siu42/faraday-2.png"/&gt;&lt;/p&gt;
&lt;p&gt;Năm 1833, Faraday được cử l&amp;agrave;m gi&amp;aacute;o sư h&amp;oacute;a học ở Học viện Ho&amp;agrave;ng gia thay thế gi&amp;aacute;o sư Davy, cũng ch&amp;iacute;nh năm n&amp;agrave;y Faraday đưa ra l&amp;yacute; thuyết v&amp;agrave; hiện tượng điện ph&amp;acirc;n, đặt cơ sở l&amp;yacute; luận cho c&amp;aacute;c ng&amp;agrave;nh c&amp;ocirc;ng nghiệp điện - h&amp;oacute;a. &amp;Ocirc;ng ph&amp;aacute;t biểu về c&amp;aacute;c định luật định t&amp;iacute;nh, định lượng. Ch&amp;iacute;nh c&amp;aacute;c từ: "điện ph&amp;acirc;n", "điện cực", "Ion" l&amp;agrave; do &amp;ocirc;ng đặt ra.&lt;/p&gt;
&lt;p&gt;Năm 1838, Faraday tiếp tục n&amp;ecirc;u ra hai kh&amp;aacute;i niệm: từ trường điện, đường sức.&lt;/p&gt;
&lt;p&gt;Năm 1843, Faraday đưa ra l&amp;yacute; thuyết về sự nhiễm điện bằng cảm ứng.&lt;/p&gt;
&lt;p&gt;Năm 1844, Faraday được viện H&amp;agrave;n l&amp;acirc;m Khoa học Paris c&amp;ocirc;ng nhận l&amp;agrave; người kế tục Dalton trong số 8 th&amp;agrave;nh vi&amp;ecirc;n nước ngo&amp;agrave;i của Viện.&lt;/p&gt;
&lt;p&gt;Năm 1846, &amp;ocirc;ng kh&amp;aacute;m ph&amp;aacute; ra rằng năng lượng tĩnh điện được định vị trong c&amp;aacute;c chất điện m&amp;ocirc;i, kh&amp;aacute;m ph&amp;aacute; n&amp;agrave;y chuẩn bị cho sự xuất hiện l&amp;yacute; thyết điện tử của Maxwell sau n&amp;agrave;y. C&amp;ugrave;ng với kh&amp;aacute;m ph&amp;aacute; đ&amp;oacute;, Faraday t&amp;igrave;m ra "hằng số điện m&amp;ocirc;i".&lt;/p&gt;
&lt;p&gt;Để thưởng c&amp;ocirc;ng lao cho &amp;ocirc;ng, nữ ho&amp;agrave;ng Victoria đ&amp;atilde; tặng &amp;ocirc;ng ng&amp;ocirc;i nh&amp;agrave; ở Hampton Court v&amp;agrave; phong cho chức Hầu tước, &amp;ocirc;ng chỉ nhận nh&amp;agrave; với sự biết ơn, v&amp;agrave; từ chối tước vị.&lt;/p&gt;
&lt;p&gt;Ngo&amp;agrave;i thời gian d&amp;agrave;nh cho khoa học, Faraday c&amp;ograve;n l&amp;agrave; người thầy xuất sắc. &amp;Ocirc;ng c&amp;oacute; tr&amp;aacute;ch nhiệm cao v&amp;agrave; rất quan t&amp;acirc;m đến phương ph&amp;aacute;p giảng dạy phải thực nghiệm để x&amp;acirc;y dựng h&amp;igrave;nh tượng trực quan. Cho tới tận ng&amp;agrave;y nay, Học viện Ho&amp;agrave;ng gia Anh vẫn duy tr&amp;igrave; những nguy&amp;ecirc;n tắc giảng dạy m&amp;agrave; Faraday đ&amp;atilde; đề ra bằng kinh nghiệm v&amp;agrave; l&amp;ograve;ng tận t&amp;acirc;m với c&amp;ocirc;ng việc của &amp;ocirc;ng.&lt;/p&gt;
&lt;h1 id="nhung-nam-cuoi-cuoc-doi"&gt;Những năm cuối cuộc đời&lt;/h1&gt;
&lt;p&gt;Ng&amp;agrave;y 20/3/1862 l&amp;agrave; ng&amp;agrave;y cuối c&amp;ugrave;ng đ&amp;aacute;nh dấu c&amp;ocirc;ng việc nghi&amp;ecirc;n cứu của Michael Faraday. Trong cuốn sổ ghi kết quả nghi&amp;ecirc;n cứu của &amp;ocirc;ng người ta đọc được con số th&amp;iacute; nghiệm cuối c&amp;ugrave;ng của &amp;ocirc;ng: 16041.&lt;/p&gt;
&lt;p&gt;M&amp;ugrave;a h&amp;egrave; 1867, &amp;ocirc;ng bị điếc v&amp;agrave; mất tr&amp;iacute; nhớ, &amp;ocirc;ng qua đời ng&amp;agrave;y 25/8/1867 thọ 76 tuổi tại Hampton Court. Faraday đ&amp;atilde; để lại cho nh&amp;acirc;n loại những ph&amp;aacute;t minh vĩ đại.&lt;/p&gt;</content><category term="Inspiration"></category><category term="OTHER"></category></entry><entry><title>Nhà khoa học nữ gốc Việt trong top ảnh hưởng nhất thế giới</title><link href="/blog/other/2018/nha-nu-khoa-hoc-goc-viet/" rel="alternate"></link><published>2018-08-21T12:33:54+00:00</published><updated>2018-08-21T12:33:54+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/other/2018/nha-nu-khoa-hoc-goc-viet/</id><summary type="html">&lt;p&gt;Tuổi thơ theo mẹ đi khắp nơi để kiếm sống, sang Mỹ th&amp;igrave; bị bạn b&amp;egrave; ch&amp;ecirc; cười v&amp;igrave; kh&amp;ocirc;ng biết tiếng Anh, nhưng Nguyễn Thục Quy&amp;ecirc;n đ&amp;atilde; vượt qua tất cả v&amp;agrave; trở th&amp;agrave;nh một trong những nh&amp;agrave; khoa học ảnh hưởng nhất thế giới. 
Nh&amp;agrave; khoa học trong tốp 'ảnh hưởng nhất thế giới' kh&amp;ocirc;ng đủ can đảm cưới vợ&lt;/p&gt;
&lt;p&gt;Gi&amp;aacute;o sư Nguyễn Thục Quy&amp;ecirc;n sinh ra ở Bu&amp;ocirc;n Ma Thuật (Đắk Lắk) trong một gia đ&amp;igrave;nh thượng lưu gồm 5 anh chị em. Sau năm 1975, cha đi cải tạo, mẹ chị - một c&amp;ocirc; gi&amp;aacute;o dạy to&amp;aacute;n cấp 2, dẫn dắt đ&amp;agrave;n con đến c&amp;aacute;c v&amp;ugrave;ng kinh tế mới như Phước L&amp;acirc;m, Long Điền, Đất Đỏ, Phước Tỉnh v&amp;agrave; Vũng T&amp;agrave;u để sinh nhai. &lt;/p&gt;
&lt;p&gt;L&amp;uacute;c 5-6 tuổi, c&amp;ocirc; b&amp;eacute; Quy&amp;ecirc;n phải phụ gi&amp;uacute;p mẹ dọn dẹp nh&amp;agrave; cửa, kiếm củi nấu cơm, đ&amp;agrave;o khoai, c&amp;acirc;u c&amp;aacute;, g&amp;aacute;nh …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Tuổi thơ theo mẹ đi khắp nơi để kiếm sống, sang Mỹ th&amp;igrave; bị bạn b&amp;egrave; ch&amp;ecirc; cười v&amp;igrave; kh&amp;ocirc;ng biết tiếng Anh, nhưng Nguyễn Thục Quy&amp;ecirc;n đ&amp;atilde; vượt qua tất cả v&amp;agrave; trở th&amp;agrave;nh một trong những nh&amp;agrave; khoa học ảnh hưởng nhất thế giới. 
Nh&amp;agrave; khoa học trong tốp 'ảnh hưởng nhất thế giới' kh&amp;ocirc;ng đủ can đảm cưới vợ&lt;/p&gt;
&lt;p&gt;Gi&amp;aacute;o sư Nguyễn Thục Quy&amp;ecirc;n sinh ra ở Bu&amp;ocirc;n Ma Thuật (Đắk Lắk) trong một gia đ&amp;igrave;nh thượng lưu gồm 5 anh chị em. Sau năm 1975, cha đi cải tạo, mẹ chị - một c&amp;ocirc; gi&amp;aacute;o dạy to&amp;aacute;n cấp 2, dẫn dắt đ&amp;agrave;n con đến c&amp;aacute;c v&amp;ugrave;ng kinh tế mới như Phước L&amp;acirc;m, Long Điền, Đất Đỏ, Phước Tỉnh v&amp;agrave; Vũng T&amp;agrave;u để sinh nhai. &lt;/p&gt;
&lt;p&gt;L&amp;uacute;c 5-6 tuổi, c&amp;ocirc; b&amp;eacute; Quy&amp;ecirc;n phải phụ gi&amp;uacute;p mẹ dọn dẹp nh&amp;agrave; cửa, kiếm củi nấu cơm, đ&amp;agrave;o khoai, c&amp;acirc;u c&amp;aacute;, g&amp;aacute;nh nước... Cuộc sống cơm &amp;aacute;o, gạo tiền cứ đeo b&amp;aacute;m cho đến năm 1986 khi gia đ&amp;igrave;nh mở tiệm phở ở Bến Đ&amp;aacute; - Vũng T&amp;agrave;u, Quy&amp;ecirc;n mới được đi học ở trường Trung học Trần Nguy&amp;ecirc;n H&amp;atilde;n.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gi&amp;aacute;o sư Nguyễn Thục Quy&amp;ecirc;n. Ảnh do nh&amp;acirc;n vật cung cấp." src="https://vcdn-vnexpress.vnecdn.net/2016/02/05/thucquyen2-1917-1454574432-2161-1454677269.jpg"/&gt;&lt;/p&gt;
&lt;h1 id="nhoc-nhan-noi-xu-nguoi"&gt;Nhọc nhằn nơi xứ người&lt;/h1&gt;
&lt;p&gt;Th&amp;aacute;ng 7/1991, chị c&amp;ugrave;ng bố mẹ v&amp;agrave; 5 anh chị đến Mỹ định cư. Hai năm đầu, c&amp;aacute;c anh chị em của chị Quy&amp;ecirc;n cứ đ&amp;ograve;i về Việt Nam v&amp;igrave; kh&amp;ocirc;ng biết tiếng Anh v&amp;agrave; phong tục tập qu&amp;aacute;n Mỹ. Nhưng chị thấy ổn v&amp;igrave; được l&amp;agrave;m điều m&amp;igrave;nh th&amp;iacute;ch m&amp;agrave; kh&amp;ocirc;ng sợ người kh&amp;aacute;c dị nghị. &lt;/p&gt;
&lt;p&gt;"Khi c&amp;ograve;n ở Việt Nam, gia đ&amp;igrave;nh đ&amp;atilde; vất vả rồi, n&amp;ecirc;n khi sang Mỹ t&amp;ocirc;i phải cố gắng hơn rất nhiều để c&amp;oacute; được cuộc sống tốt hơn", chị Quy&amp;ecirc;n chia sẻ. Để tự khẳng định bản th&amp;acirc;n nơi đất kh&amp;aacute;ch qu&amp;ecirc; người, chị đ&amp;atilde; quyết t&amp;acirc;m học tiếng Anh thật nhanh bằng c&amp;aacute;ch đăng k&amp;yacute; ở ba trường trung học tại ba th&amp;agrave;nh phố. Ở Mỹ, tiếng Anh được học miễn ph&amp;iacute;.&lt;/p&gt;
&lt;p&gt;Vất vả với bao tủi nhục khi bị nhiều người coi thường c&amp;agrave;ng khiến chị c&amp;oacute; th&amp;ecirc;m động lực vươn l&amp;ecirc;n. "C&amp;oacute; gi&amp;aacute;o vi&amp;ecirc;n chế nhạo t&amp;ocirc;i trước cả lớp v&amp;igrave; khả năng n&amp;oacute;i tiếng Anh k&amp;eacute;m. Một &amp;ocirc;ng người Mỹ c&amp;ograve;n n&amp;oacute;i thẳng với t&amp;ocirc;i h&amp;atilde;y về nước của c&amp;ocirc; đi", chị nhớ lại v&amp;agrave; cho biết ở Mỹ vẫn c&amp;ograve;n một số người ph&amp;acirc;n biệt kỳ thị như vậy. "Thậm ch&amp;iacute; c&amp;oacute; đồng nghiệp l&amp;uacute;c ở trường kh&amp;ocirc;ng bao giờ n&amp;oacute;i chuyện với t&amp;ocirc;i mặc d&amp;ugrave; t&amp;ocirc;i đ&amp;atilde; cố gắng để n&amp;oacute;i chuyện với anh ta v&amp;agrave;i lần", nữ gi&amp;aacute;o sư n&amp;oacute;i.&lt;/p&gt;
&lt;p&gt;Th&amp;aacute;ng 9/1993, người c&amp;ocirc; họ cho chị ở c&amp;ugrave;ng nh&amp;agrave;, nhưng chị phải dọn dẹp, nấu nướng, đi chợ v&amp;agrave; chạy việc vặt cho c&amp;ocirc;. Thời gian n&amp;agrave;y, chị xin học ở Đại học Santa Monica nhưng kh&amp;ocirc;ng được nhận v&amp;igrave; tiếng Anh k&amp;eacute;m. Chị đ&amp;atilde; năn nỉ nh&amp;agrave; trường cho học thử một kỳ v&amp;agrave; hứa nếu kh&amp;ocirc;ng học được sẽ trở về trường trung học để học th&amp;ecirc;m tiếng Anh. Ban ng&amp;agrave;y đi học, ban đ&amp;ecirc;m chị t&amp;igrave;m lớp học th&amp;ecirc;m ở trung t&amp;acirc;m dạy tiếng Anh miễn ph&amp;iacute;. Với sự nỗ lực kh&amp;ocirc;ng ngừng nghỉ, cuối c&amp;ugrave;ng chị cũng được nhận v&amp;agrave;o học. &lt;/p&gt;
&lt;p&gt;Thấy bố mẹ vất vả l&amp;agrave;m trong nh&amp;agrave; h&amp;agrave;ng v&amp;agrave; ở h&amp;atilde;ng may, chị kh&amp;ocirc;ng cho ph&amp;eacute;p bản th&amp;acirc;n thất bại m&amp;agrave; cố gắng gấp đ&amp;ocirc;i, gấp ba so với những bạn c&amp;ugrave;ng trang lứa. Để c&amp;oacute; tiền học, chị xin l&amp;agrave;m th&amp;ecirc;m trong thư viện trường từ 17h đến 22h mỗi ng&amp;agrave;y, nhưng vẫn kh&amp;ocirc;ng đủ n&amp;ecirc;n phải vay th&amp;ecirc;m tiền của Ch&amp;iacute;nh phủ.&lt;/p&gt;
&lt;p&gt;Th&amp;aacute;ng 9/1995, chị xin chuyển l&amp;ecirc;n Đại học Califonia, Los Angeles v&amp;agrave; l&amp;agrave;m th&amp;ecirc;m trong ph&amp;ograve;ng th&amp;iacute; nghiệm với c&amp;ocirc;ng việc rửa dụng cụ. Chị xin l&amp;agrave;m nghi&amp;ecirc;n cứu nhưng kh&amp;ocirc;ng c&amp;oacute; ph&amp;ograve;ng th&amp;iacute; nghiệm n&amp;agrave;o nhận. Sau khi tốt nghiệp bằng đại học H&amp;oacute;a năm 1997, chị nộp đơn học cao học. Chỉ trong một năm chị đ&amp;atilde; c&amp;oacute; bằng thạc sĩ ng&amp;agrave;nh L&amp;yacute; - H&amp;oacute;a v&amp;agrave; quyết định học tiếp tiến sĩ. Thật bất ngờ, cuối năm của chương tr&amp;igrave;nh n&amp;agrave;y chị l&amp;agrave; một 7 nghi&amp;ecirc;n cứu sinh xuất sắc của Đại học Califonia, Los Angeles được trao học bổng.&lt;/p&gt;
&lt;p&gt;Th&amp;aacute;ng 6/2001, chị nhận bằng tiến sĩ v&amp;agrave; ra trường trước cả những sinh vi&amp;ecirc;n chị từng rửa dụng cụ th&amp;iacute; nghiệm cho họ trước đ&amp;acirc;y. Ra trường chị đạt giải thưởng xuất sắc ng&amp;agrave;nh L&amp;yacute; - H&amp;oacute;a. Th&amp;aacute;ng 9/2001, được giải thưởng của li&amp;ecirc;n bang đi tu nghiệp ở ph&amp;ograve;ng th&amp;iacute; nghiệm quốc gia nhưng chị từ chối v&amp;agrave; đến l&amp;agrave;m ở Đại học Columbia, New York.&lt;/p&gt;
&lt;p&gt;Ba năm sau chị bắt đầu l&amp;agrave;m việc ở Đại học California, Santa Barbara v&amp;agrave; mất hơn hai năm x&amp;acirc;y dựng hai ph&amp;ograve;ng th&amp;iacute; nghiệm ri&amp;ecirc;ng. Sau 11 năm, chị đ&amp;atilde; c&amp;oacute; 7 ph&amp;ograve;ng th&amp;iacute; nghiệm ri&amp;ecirc;ng cho nh&amp;oacute;m nghi&amp;ecirc;n cứu. Chị c&amp;ograve;n xin hơn 10 triệu USD cho những dự &amp;aacute;n nghi&amp;ecirc;n cứu, được mời tới hơn 200 địa điểm tr&amp;ecirc;n thế giới để thuyết tr&amp;igrave;nh cũng như nhận nhiều giải thưởng lớn cho c&amp;ocirc;ng tr&amp;igrave;nh nghi&amp;ecirc;n cứu.&lt;/p&gt;
&lt;p&gt;"Bạn b&amp;egrave; t&amp;ocirc;i ở Việt Nam vẫn thường bảo hồi ở qu&amp;ecirc; học dốt thế m&amp;agrave; sao qua Mỹ học giỏi gh&amp;ecirc; thế. T&amp;ocirc;i trả lời rằng ng&amp;agrave;y xưa l&amp;agrave;m g&amp;igrave; c&amp;oacute; thời gian để học v&amp;igrave; c&amp;ograve;n phải phụ gi&amp;uacute;p gia đ&amp;igrave;nh", vị gi&amp;aacute;o sư n&amp;oacute;i.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Pin năng lượng mặt trời l&amp;agrave;m từ chất nhựa dẫn điện Nghi&amp;ecirc;n cứu trong ph&amp;ograve;ng th&amp;iacute; nghiệm của gi&amp;aacute;o sư Nguyễn Thục Quy&amp;ecirc;n. Ảnh do nh&amp;acirc;n vật cung cấp." src="https://vcdn-vnexpress.vnecdn.net/2016/02/04/thucquyen1-9266-1454574433.jpg"/&gt;&lt;/p&gt;
&lt;h1 id="nhung-vat-va-cua-phu-nu-khi-lam-khoa-hoc"&gt;Những vất vả của phụ nữ khi l&amp;agrave;m khoa học&lt;/h1&gt;
&lt;p&gt;Gi&amp;aacute;o sư Quy&amp;ecirc;n t&amp;acirc;m sự, c&amp;oacute; được ng&amp;agrave;y h&amp;ocirc;m nay l&amp;agrave; nhờ sự d&amp;igrave;u dắt của mẹ v&amp;agrave; người cậu ruột khi hướng cho chị đi theo con đường tốt nhất c&amp;oacute; thể. Trong khi bố cho rằng, con g&amp;aacute;i th&amp;igrave; n&amp;ecirc;n lấy chồng, kh&amp;ocirc;ng cần học, th&amp;igrave; mẹ ngược lại. Chị c&amp;ograve;n nhớ ng&amp;agrave;y học xong lớp 12, chị đ&amp;atilde; x&amp;aacute;c định sẽ ở nh&amp;agrave; v&amp;agrave; t&amp;iacute;nh chuyện lấy chồng, nhưng mẹ vẫn đưa chị l&amp;ecirc;n S&amp;agrave;i G&amp;ograve;n để thi đại học. "Mẹ đưa t&amp;ocirc;i l&amp;ecirc;n S&amp;agrave;i G&amp;ograve;n ở nh&amp;agrave; b&amp;agrave; ngoại để thi đại học, nhưng t&amp;ocirc;i kh&amp;ocirc;ng muốn. Lớn l&amp;ecirc;n v&amp;agrave; học ở trường l&amp;agrave;ng t&amp;ocirc;i thấy ở tuổi 18 người ta đ&amp;atilde; lấy chồng v&amp;agrave; c&amp;oacute; con rồi", chị n&amp;oacute;i.&lt;/p&gt;
&lt;p&gt;Người cậu đ&amp;atilde; gọi chị đến n&amp;oacute;i chuyện hơn hai giờ, với mục đ&amp;iacute;ch khuy&amp;ecirc;n chị đi thi v&amp;agrave; cố gắng v&amp;agrave;o đại học. "Tại sao c&amp;oacute; cơ hội như vậy m&amp;agrave; ch&amp;aacute;u lại từ chối. Học đại học sau n&amp;agrave;y ch&amp;aacute;u sẽ c&amp;oacute; c&amp;ocirc;ng ăn việc l&amp;agrave;m ổn định, c&amp;oacute; sự nghiệp, nếu lấy được người tốt th&amp;igrave; kh&amp;ocirc;ng sao...", chị kể lại lời &amp;ocirc;ng cậu. &lt;/p&gt;
&lt;p&gt;Lớn l&amp;ecirc;n, người chị h&amp;acirc;m mộ đ&amp;oacute; l&amp;agrave; b&amp;agrave; Marie Curie, bởi thời đ&amp;oacute; khoa học gia l&amp;agrave; nữ rất &amp;iacute;t. B&amp;agrave; ch&amp;iacute;nh l&amp;agrave; tấm gương vượt kh&amp;oacute; để chị tiếp tục cố gắng cho nghi&amp;ecirc;n cứu khoa học.&lt;/p&gt;
&lt;p&gt;Hơn 11 năm l&amp;agrave;m việc ở Đại học California, Santa Barbara, chị l&amp;agrave;m khoảng thời gian 15 tiếng mỗi ng&amp;agrave;y. B&amp;ecirc;n cạnh việc giảng dạy, chị c&amp;ograve;n l&amp;agrave;m nhiều c&amp;ocirc;ng việc kh&amp;aacute;c như bi&amp;ecirc;n tập b&amp;aacute;o khoa học, tổ chức hội nghị khoa học quốc tế, xin tiền dự &amp;aacute;n nghi&amp;ecirc;n cứu trả lương, học ph&amp;iacute;, v&amp;agrave; bảo hiểm y tế cho sinh vi&amp;ecirc;n (mỗi nghi&amp;ecirc;n cứu sinh tốn khoảng 100.000 đ&amp;ocirc;la mỗi năm), hướng dẫn sinh vi&amp;ecirc;n l&amp;agrave;m nghi&amp;ecirc;n cứu, gi&amp;uacute;p sinh vi&amp;ecirc;n viết b&amp;agrave;i đăng b&amp;aacute;o, l&amp;agrave;m trong ban x&amp;eacute;t l&amp;ecirc;n lương v&amp;agrave; l&amp;ecirc;n chức cho tất cả gi&amp;aacute;o sư trong trường, ban tuyển dụng gi&amp;aacute;o sư... &lt;/p&gt;
&lt;p&gt;Chị chia sẻ, l&amp;agrave;m khoa học đ&amp;atilde; kh&amp;oacute; nhưng phụ nữ trong lĩnh vực n&amp;agrave;y c&amp;agrave;ng vất vả hơn, bởi ngo&amp;agrave;i sự nghiệp, họ c&amp;ograve;n phải lo cho gia đ&amp;igrave;nh. Ngay bản th&amp;acirc;n chị, d&amp;ugrave; đ&amp;atilde; cố gắng rất nhiều nhưng đ&amp;ocirc;i khi vẫn kh&amp;ocirc;ng nhận được sự t&amp;ocirc;n trọng của đồng nghiệp nam giới. "Cũng may t&amp;ocirc;i c&amp;oacute; người chồng t&amp;acirc;m l&amp;yacute; v&amp;agrave; th&amp;ocirc;ng cảm, anh dạy h&amp;oacute;a hữu cơ c&amp;ugrave;ng trường, lu&amp;ocirc;n hỗ trợ n&amp;ecirc;n t&amp;ocirc;i c&amp;oacute; th&amp;ecirc;m động lực để giảng dạy v&amp;agrave; nghi&amp;ecirc;n cứu", nữ gi&amp;aacute;o sư n&amp;oacute;i.&lt;/p&gt;
&lt;p&gt;"Phần đ&amp;ocirc;ng mọi người nghĩ con g&amp;aacute;i th&amp;igrave; n&amp;ecirc;n lo cho chồng con, dọn dẹp nh&amp;agrave; cửa v&amp;agrave; kh&amp;ocirc;ng n&amp;ecirc;n c&amp;oacute; sự nghiệp ri&amp;ecirc;ng. T&amp;ocirc;i muốn cho những người phụ nữ kh&amp;aacute;c biết l&amp;agrave; họ c&amp;oacute; thể l&amp;agrave;m cả hai. T&amp;ocirc;i muốn l&amp;agrave;m những điều hữu &amp;iacute;ch cho x&amp;atilde; hội", chị n&amp;oacute;i.&lt;/p&gt;
&lt;p&gt;Chị vẫn c&amp;ograve;n nhớ như in thời điểm bắt đầu v&amp;agrave;o học trong trường. L&amp;uacute;c đ&amp;oacute; chị xin v&amp;agrave;o ph&amp;ograve;ng th&amp;iacute; nghiệm nhưng kh&amp;ocirc;ng được v&amp;igrave; nhiều người nghĩ chị kh&amp;ocirc;ng thể l&amp;agrave;m được điều g&amp;igrave; v&amp;agrave; khuy&amp;ecirc;n rằng &amp;ldquo;nghi&amp;ecirc;n cứu kh&amp;ocirc;ng dễ d&amp;agrave;ng v&amp;agrave; kh&amp;ocirc;ng phải ai cũng l&amp;agrave;m được. Bạn n&amp;ecirc;n tập trung để học tiếng Anh đi&amp;rdquo;. M&amp;atilde;i sau n&amp;agrave;y, c&amp;oacute; vị gi&amp;aacute;o sư thấy chị c&amp;oacute; những c&amp;acirc;u hỏi hay trong lớp n&amp;ecirc;n khuyến kh&amp;iacute;ch theo đường nghi&amp;ecirc;n cứu. Biết được tin n&amp;agrave;y chị rất vui v&amp;igrave; từ b&amp;eacute; đ&amp;atilde; th&amp;iacute;ch t&amp;igrave;m t&amp;ograve;i những điều mới.&lt;/p&gt;
&lt;p&gt;Đầu năm 2004, chị đi phỏng vấn ng&amp;agrave;nh h&amp;oacute;a ở một số trường đại học. Chị cũng rất sợ v&amp;igrave; những trường n&amp;agrave;y ng&amp;agrave;nh h&amp;oacute;a rất &amp;iacute;t hoặc kh&amp;ocirc;ng c&amp;oacute; nữ gi&amp;aacute;o sư. "Con đường đi đến th&amp;agrave;nh c&amp;ocirc;ng ở Mỹ kh&amp;ocirc;ng phải dễ d&amp;agrave;ng v&amp;igrave; quốc gia n&amp;agrave;y thường thu h&amp;uacute;t nh&amp;agrave; khoa học h&amp;agrave;ng đầu tr&amp;ecirc;n thế giới nhưng bản chất người Việt Nam th&amp;ocirc;ng minh v&amp;agrave; chăm chỉ", vị gi&amp;aacute;o sư n&amp;oacute;i v&amp;agrave; cho rằng c&amp;oacute; c&amp;ocirc;ng mai sắt c&amp;oacute; ng&amp;agrave;y n&amp;ecirc;n kim. &lt;/p&gt;
&lt;h1 id="thich-ve-viet-nam"&gt;Th&amp;iacute;ch về Việt Nam&lt;/h1&gt;
&lt;p&gt;"T&amp;ocirc;i nhớ Việt Nam lắm. Nếu c&amp;oacute; thời gian l&amp;agrave; t&amp;ocirc;i về ngay, bởi hiện nay anh em họ h&amp;agrave;ng, nhất l&amp;agrave; &amp;ocirc;ng cậu - người đặt vi&amp;ecirc;n gạch đầu ti&amp;ecirc;n trong cuộc đời khoa học của t&amp;ocirc;i vẫn ở qu&amp;ecirc; hương", nữ gi&amp;aacute;o sư t&amp;acirc;m sự.&lt;/p&gt;
&lt;p&gt;Lần đầu ti&amp;ecirc;n chị v&amp;agrave; mẹ về Việt Nam l&amp;agrave; năm 1999 để thăm b&amp;agrave; ngoại trong 3 tuần. 9 lần về nước ngo&amp;agrave;i dự hội nghị khoa học, chị d&amp;agrave;nh thời gian để thăm gia đ&amp;igrave;nh.&lt;/p&gt;
&lt;p&gt;Chị cho biết, thời gian 21 năm sống ở Việt Nam, chị nhớ m&amp;oacute;n ăn thuần t&amp;uacute;y Việt Nam v&amp;agrave; c&amp;aacute;c b&amp;agrave;i h&amp;aacute;t Việt, n&amp;ecirc;n lần n&amp;agrave;o về nước chị cũng nhờ cậu mợ dẫn đi xem ca nhạc. "T&amp;ocirc;i th&amp;iacute;ch nhạc d&amp;acirc;n ca như b&amp;agrave;i Qu&amp;ecirc; hương, Ai đưa con s&amp;aacute;o sang s&amp;ocirc;ng", chị n&amp;oacute;i. &lt;/p&gt;
&lt;p&gt;Khi hỏi &amp;yacute; định về Việt Nam sinh sống, chị n&amp;oacute;i: "C&amp;oacute; lẽ khi n&amp;agrave;o về hưu t&amp;ocirc;i mới về nước, v&amp;igrave; qu&amp;ecirc; hương vẫn chưa c&amp;oacute; đủ cơ sở vật chất điều kiện tốt để t&amp;ocirc;i c&amp;oacute; thể nghi&amp;ecirc;n cứu", chị n&amp;oacute;i v&amp;agrave; cho biết 7 ph&amp;ograve;ng th&amp;iacute; nghiệm ri&amp;ecirc;ng của chị trị gi&amp;aacute; khoảng 4 triệu đ&amp;ocirc;la.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;B&amp;ecirc;n cạnh giải thưởng l&amp;agrave; một trong những nh&amp;agrave; khoa học ảnh hưởng nhất thế giới ng&amp;agrave;nh khoa học vật liệu, chị c&amp;ograve;n nhận nhiều giải thưởng kh&amp;aacute;c như: Giải thưởng Nghi&amp;ecirc;n cứu khoa học Alexander von Humboldt-Foundation của Đức năm 2015; Giải thưởng Nghi&amp;ecirc;n cứu khoa học của Quỹ Khoa học Quốc gia Mỹ 2010, Giải thưởng Nghi&amp;ecirc;n cứu khoa học của Alfred P. Sloan Foundation 2009; Giải thưởng Nghi&amp;ecirc;n cứu khoa học của Camille Dreyfus Foundation 2008; Giải thưởng Nghi&amp;ecirc;n cứu khoa học Harold J. Plous Memorial Award and Lectureship 2007.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Nguồn: &lt;a href="https://vnexpress.net/tin-tuc/khoa-hoc/trong-nuoc/nha-khoa-hoc-nu-goc-viet-trong-top-anh-huong-nhat-the-gioi-3352788.html"&gt;Vnexpress.net&lt;/a&gt;&lt;/p&gt;</content><category term="OTHER"></category></entry><entry><title>Nigerian immigrant dreams of finding cures for infectious diseases</title><link href="/blog/other/2018/infectious-disease-american-dream/" rel="alternate"></link><published>2018-08-21T12:10:13+00:00</published><updated>2018-08-21T12:10:13+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/other/2018/infectious-disease-american-dream/</id><summary type="html">&lt;h1 id="nigerian-born-chidiebere-akusobi-has-notched-many-impressive-academic-achievements-in-his-short-life"&gt;Nigerian-born Chidiebere Akusobi has notched many impressive academic achievements in his short life.&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The 25-year old studied ecology and evolutionary biology as an undergraduate at Yale, then earned his master's in biochemistry from the University of Cambridge. Now he's three years into a joint PhD/MD program researching cures for infectious diseases at Harvard and MIT.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;But if you ask him, he'll tell you that the biggest academic hurdle he ever had to overcome was in the fifth grade.&lt;/p&gt;
&lt;p&gt;That's when Akusobi, who had moved from Nigeria to the impoverished New York City neighborhood of the South Bronx when he was two years old, was accepted into the rigorous New York City Prep for Prep program.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chidi his mother, father and sister Ijeoma outside their South Bronx apartment building in 1994" src="https://i2.cdn.turner.com/money/dam/assets/160524133705-chidiebere-akusobi-1-780x439.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;The program is an educational boot camp that selects roughly 225 promising students a year from the poorest New York City neighborhoods and grooms them for scholarships to attend the city's top private schools …&lt;/p&gt;</summary><content type="html">&lt;h1 id="nigerian-born-chidiebere-akusobi-has-notched-many-impressive-academic-achievements-in-his-short-life"&gt;Nigerian-born Chidiebere Akusobi has notched many impressive academic achievements in his short life.&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The 25-year old studied ecology and evolutionary biology as an undergraduate at Yale, then earned his master's in biochemistry from the University of Cambridge. Now he's three years into a joint PhD/MD program researching cures for infectious diseases at Harvard and MIT.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;But if you ask him, he'll tell you that the biggest academic hurdle he ever had to overcome was in the fifth grade.&lt;/p&gt;
&lt;p&gt;That's when Akusobi, who had moved from Nigeria to the impoverished New York City neighborhood of the South Bronx when he was two years old, was accepted into the rigorous New York City Prep for Prep program.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chidi his mother, father and sister Ijeoma outside their South Bronx apartment building in 1994" src="https://i2.cdn.turner.com/money/dam/assets/160524133705-chidiebere-akusobi-1-780x439.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;The program is an educational boot camp that selects roughly 225 promising students a year from the poorest New York City neighborhoods and grooms them for scholarships to attend the city's top private schools.&lt;/p&gt;
&lt;p&gt;For 14 months, students were assigned six hours of homework a day -- on top of their normal workload -- and they were expected to read one book a week, he said.&lt;/p&gt;
&lt;p&gt;"I remember July 4th, 2001, everyone was outside and there were fireworks. I was inside and my mom was keeping me awake as I read," he said.&lt;/p&gt;
&lt;p&gt;But Akusobi was determined to complete the program.&lt;/p&gt;
&lt;p&gt;"I was taught that [education] was our shot of the American Dream," he said.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Akusobi's third grade picture" src="https://i2.cdn.turner.com/money/dam/assets/160524135208-chidiebere-akusobi-7-340xa.jpg"/&gt;.&lt;/p&gt;
&lt;p&gt;When he was done, he had won a full academic scholarship to Horace Mann, one of the most prestigious prep schools in New York City.&lt;/p&gt;
&lt;p&gt;Once Akusobi enrolled, finding his place in the school's rarefied halls became his next big challenge.&lt;/p&gt;
&lt;p&gt;Related: My American Dream - Offering legal help to other immigrants&lt;/p&gt;
&lt;p&gt;He was only 12 and the stark contrast between he and the other mostly white, wealthy students was striking.&lt;/p&gt;
&lt;p&gt;"I realized that there are people that go on vacations every summer or have summer houses or drivers," he said. "It's funny to go to a school where kids have trust funds and then to take the bus home to the South Bronx."&lt;/p&gt;
&lt;p&gt;At the time, Akusobi's father was working three jobs while also studying to become a nurse. His mother, who was also pursuing a nursing degree, worked as a home health aide.&lt;/p&gt;
&lt;p&gt;"They were working to support me and my three younger siblings. That's the reason they left Nigeria, to provide a better life," he said.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chidi (far right) and his siblings at his dad's graduation from City College of New York, where he graduated with a degree in nursing" src="https://i2.cdn.turner.com/money/dam/assets/160524134226-chidiebere-akusobi-2-780x439.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;Akusobi took full advantage of what Horace Mann had to offer. He became head of the dance team and even wrote and acted in a one act show.&lt;/p&gt;
&lt;p&gt;"I just took advantage of all the opportunities that I could and did well enough that I got into Yale," he said.&lt;/p&gt;
&lt;p&gt;But his true passion was medicine.&lt;/p&gt;
&lt;p&gt;Related: I want to bring health care to undocumented immigrants&lt;/p&gt;
&lt;p&gt;Even though he had left Nigeria at a young age, Akusobi remained close to family members who still live in the country. "When I go to Nigeria there's a sense of being home because that's where my folks grew up," he said.&lt;/p&gt;
&lt;p&gt;But the attachments have come with heartache each time he receives news of a family member or friend who has passed away from an infectious disease, like malaria or HIV.&lt;/p&gt;
&lt;p&gt;"It's shocking the toll that infectious diseases have. I could work on fixing that. There's real impact that has to be made," he said.&lt;/p&gt;
&lt;p&gt;And Akusobi is getting closer to that goal. Recently, he was granted one of the Paul and Daisy Soros Fellowships for New Americans, which will pay up to $90,000 for his joint PhD/MD program at Harvard and MIT.&lt;/p&gt;
&lt;p&gt;Besides his research at Harvard/MIT, Akusobi has advocated on a variety of issues, especially those dealing with racial equality and diversity in medicine.&lt;/p&gt;
&lt;p&gt;He helped organize the WhiteCoat4BlackLives movement on Harvard Medical School's campus to commemorate Eric Garner and Michael Brown, two black men whose deaths at the hands of the police spurred a national movement against police brutality and highlighted the issue of racism in America.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Akusobi with a friend he did Prep for Prep with" src="https://i2.cdn.turner.com/money/dam/assets/160524135500-chidiebere-akusobi-10-780x439.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;He has also taken a leadership role at the Student National Medical Association, which seeks to help get more minorities like Akusobi involved in the practice of medicine.&lt;/p&gt;
&lt;p&gt;"From what I've seen there are so many students that have potential," he said. "But there is systemic injustice and institutionalized racism that doesn't allow people to get to where they need to be."&lt;/p&gt;
&lt;p&gt;Related: Daughter of immigrants challenges the American Dream&lt;/p&gt;
&lt;p&gt;While he believes in the American Dream, Akusobi says he realizes it isn't a reality for many people, especially those who didn't get the opportunities he did.&lt;/p&gt;
&lt;p&gt;"The American Dream for a lot of people is a fantasy. I have experienced sub par schools with sub par teachers," he said. "An elementary school student attending those schools and living in a neighborhood without quality food or after-school opportunities and surrounded by people in that situation. For a kid in that situation it's easy to see how they might feel like the American Dream doesn't exist."&lt;/p&gt;</content><category term="Inspiration"></category><category term="OTHER"></category></entry><entry><title>One immigrant's path from cleaning houses to Stanford professor</title><link href="/blog/other/2018/chinese-immigrant-professor/" rel="alternate"></link><published>2018-08-21T11:53:45+00:00</published><updated>2018-08-21T11:53:45+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/other/2018/chinese-immigrant-professor/</id><summary type="html">&lt;p&gt;&lt;strong&gt; House cleaning. Working the cash register at a Chinese restaurant. Walking dogs. Running a dry cleaner.
Fei-Fei Li arrived in the U.S. from China at age 16 with many big dreams. And it took many odd jobs to help her achieve them. &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Luckily, she was smart -- and extremely driven. And today, she's the director of Stanford University's artificial intelligence lab.&lt;/p&gt;
&lt;p&gt;"As one of the leaders in the world for A.I., I feel tremendous excitement and responsibility to create the most awesome and benevolent technology for society and to educate the most awesome and benevolent technologists -- that's my calling," Li said.&lt;/p&gt;
&lt;p&gt;She is also a staunch advocate for diversity in the tech industry.&lt;/p&gt;
&lt;p&gt;"I see extremely talented Stanford PhD students struggling with their visas and I find it unthinkable that we create so many hurdles for the talents of the world," Li said.&lt;/p&gt;
&lt;p&gt;fei fei li american success
Fei …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt; House cleaning. Working the cash register at a Chinese restaurant. Walking dogs. Running a dry cleaner.
Fei-Fei Li arrived in the U.S. from China at age 16 with many big dreams. And it took many odd jobs to help her achieve them. &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Luckily, she was smart -- and extremely driven. And today, she's the director of Stanford University's artificial intelligence lab.&lt;/p&gt;
&lt;p&gt;"As one of the leaders in the world for A.I., I feel tremendous excitement and responsibility to create the most awesome and benevolent technology for society and to educate the most awesome and benevolent technologists -- that's my calling," Li said.&lt;/p&gt;
&lt;p&gt;She is also a staunch advocate for diversity in the tech industry.&lt;/p&gt;
&lt;p&gt;"I see extremely talented Stanford PhD students struggling with their visas and I find it unthinkable that we create so many hurdles for the talents of the world," Li said.&lt;/p&gt;
&lt;p&gt;fei fei li american success
Fei Fei Li delivering a TED Talk.
As an immigrant herself, she knows their journey requires sacrifice and determination.&lt;/p&gt;
&lt;p&gt;While Li was in college at Princeton, she borrowed money from friends -- and even her high school math teacher -- to buy a dry cleaning business for her parents in order to help them get by. Li attended classes during the week and worked at the business on the weekends.&lt;/p&gt;
&lt;p&gt;Then, when Li was in graduate school, her mom developed cancer and had a stroke. It was difficult to keep moving ahead while all of this was happening. "The real existential challenge is to live up to your fullest potential, live up to your sense of responsibility and to be honest to yourself about your dreams while doing it," she said.&lt;/p&gt;
&lt;p&gt;Li was recently named a Great Immigrant of 2016 by the Carnegie Corporation, the nation's oldest grant making foundation which honors roughly 40 naturalized U.S. citizens each year. Her graduate studies were supported by the Paul &amp;amp; Daisy Soros Fellowships for New Americans.&lt;/p&gt;
&lt;p&gt;Here is her American success story:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Li and her parents while they were still living in China.
" src="https://i2.cdn.turner.com/money/dam/assets/160711111924-fei-fei-li-ted-age-two-780x439.jpg"/&gt;&lt;/p&gt;
&lt;h1 id="what-was-life-like-for-you-as-a-child"&gt;What was life like for you as a child?&lt;/h1&gt;
&lt;p&gt;My parents were educated, but didn't speak English. So instead of being engineers and scientists, my father did camera repair and my mother worked as a cashier.&lt;/p&gt;
&lt;p&gt;We didn't have money so on top of all my studies, I did all types of jobs. I wasn't upset about doing that because my parents were working equally hard. We were just trying to survive as a family.&lt;/p&gt;
&lt;p&gt;I had to learn English from scratch, but I was making good grades -- especially in math and science.&lt;/p&gt;
&lt;p&gt;Parsippany High School was a middle ranking New Jersey high school. But I met a couple of teachers there that were extremely kind to me and helped me to make it through this tough experience. I graduated number six in my class.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Li outside of her New Jersey high school." src="https://i2.cdn.turner.com/money/dam/assets/160711111908-fei-fei-li-ted-high-school-780x439.jpg"/&gt;&lt;/p&gt;
&lt;h1 id="did-you-feel-like-a-part-of-the-community-in-parsippany"&gt;Did you feel like a part of the community in Parsippany?&lt;/h1&gt;
&lt;p&gt;We had some immigrant friends, but everyone was busy and we were just in survival mode.&lt;/p&gt;
&lt;p&gt;I didn't make a lot of friends in high school. It's a cruel time and I was very geeky. But my high school teacher was a white American and his kindness really helped.&lt;/p&gt;
&lt;p&gt;I'm really thankful for the teachers at my mediocre New Jersey high school for helping me. I was a nobody, just an immigrant kid and I didn't speak any English.&lt;/p&gt;
&lt;h1 id="what-was-your-biggest-challenge"&gt;What was your biggest challenge?&lt;/h1&gt;
&lt;p&gt;Seeking knowledge and truth was in my blood. I wanted to understand the universe and I wanted that kind of intellectualism in my life.&lt;/p&gt;
&lt;p&gt;I applied to a bunch of colleges, but Princeton gave me a nearly full scholarship.&lt;/p&gt;
&lt;p&gt;&lt;img alt="An early write-up of Li's story in the local newspaper." src="https://i2.cdn.turner.com/money/dam/assets/160711112002-fei-fei-li-newspaper-780x439.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;For the first two years of my immigrant life, it was all Chinese restaurants and cleaning houses and then I was [at Princeton] with all these wonderful intellectuals. I loved Princeton.&lt;/p&gt;
&lt;p&gt;However, my family in Parsippany was still struggling.&lt;/p&gt;
&lt;p&gt;I decided to buy a dry cleaner for them to work in and make money. Every day after class, I was on the phone to help them.&lt;/p&gt;
&lt;p&gt;For me it was A Tale of Two Cities: Parsippany and Princeton. So Monday to Friday, I was a physics student at Princeton. And on weekends, I'd go back to Parsippany [to work at the dry cleaning business] -- that's when everyone drops off or picks up their laundry.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Li followed her dream of studying in Tibet" src="https://i2.cdn.turner.com/money/dam/assets/160711111837-fei-fei-li-tibet-780x439.jpg"/&gt;.
I graduated in 1999 during a huge bull market. We were all getting offers from Wall Street, but my dream was to go to Tibet to do a year of research on Tibetan medicine, which was crazy.&lt;/p&gt;
&lt;p&gt;Then my dream was to get a PhD, in which you get paid nothing.&lt;/p&gt;
&lt;p&gt;As a Chinese daughter, I have a responsibility to take care of my parents. When Goldman Sachs offers you a salary, it becomes a distraction because I could take the job and it would relieve [my parents' difficulties]. I was invited for interviews by a number of investment banks and management consulting companies, but I didn't go. A couple of years later, McKinsey did offer me a job, but I didn't take it.&lt;/p&gt;
&lt;p&gt;The real existential challenge is to live up to your fullest potential, along with living up to your intense sense of responsibility and to be honest to yourself about what you want.&lt;/p&gt;
&lt;p&gt;My parents were very supportive of my dreams. They came to this country to pursue a dream, then I should be able to pursue my dreams.&lt;/p&gt;
&lt;p&gt;I went to graduate school at the California Institute of Technology. I was studying artificial intelligence and computational neuroscience. Graduate school was hard and my mom had cancer and a stroke and we went through a lot of difficulties and we all survived together.&lt;/p&gt;
&lt;p&gt;The backdrop here is to do all this while navigating a completely new society or culture. I don't think I would be able to do it again.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Li graduating from Princeton" src="https://i2.cdn.turner.com/money/dam/assets/160711111940-fei-fei-li-princeton-780x439.jpg"/&gt;&lt;/p&gt;
&lt;h1 id="what-helped-you-to-keep-it-together"&gt;What helped you to keep it together?&lt;/h1&gt;
&lt;p&gt;I'm a go-getter. It's in my DNA. If I spend a lot of time lamenting on the difficulties then it could be distracting. There were always people who have wanted to support me - my parents, my teachers.&lt;/p&gt;
&lt;p&gt;It doesn't take 500 people. It just takes a couple and that makes a huge difference.&lt;/p&gt;
&lt;h1 id="what-do-you-hope-to-leave-for-your-children"&gt;What do you hope to leave for your children?&lt;/h1&gt;
&lt;p&gt;That's a very cosmic question. I would rather do the best job to make a better world and our children will live in that world.&lt;/p&gt;
&lt;h1 id="is-there-something-that-you-do-every-day-that-helps-you"&gt;Is there something that you do every day that helps you?&lt;/h1&gt;
&lt;p&gt;It's watching and playing with my kids. My field is intelligence. They shed so much light to my understanding on what it means to be intelligent and what it means to be a "being." It's so much fun to see how these kids grow and become humanly intelligent.&lt;/p&gt;
&lt;p&gt;I know that's geeky, but I believe that the ultimate power is love. As a technologist, you need to keep that in mind.&lt;/p&gt;
&lt;p&gt;Source: &lt;a href="https://money.cnn.com/2016/07/21/news/economy/chinese-immigrant-stanford-professor/index.html"&gt;CNN&lt;/a&gt;&lt;/p&gt;</content><category term="OTHER"></category></entry><entry><title>Bí quyết thành công của Michael Phelps</title><link href="/blog/other/2018/bi-quyet-thanh-cong-cua-michael-phelps/" rel="alternate"></link><published>2018-08-21T11:34:51+00:00</published><updated>2018-08-21T11:34:51+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/other/2018/bi-quyet-thanh-cong-cua-michael-phelps/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Vận động vi&amp;ecirc;n nhiều huy chương nhất mọi thời đại vừa th&amp;ecirc;m v&amp;agrave;o bộ sưu tập khổng lồ của m&amp;igrave;nh chiếc huy chương v&amp;agrave;ng thứ 21 tại nội dung 4x200m tự do tiếp sức ( 3 chiếc trong 3 ng&amp;agrave;y tại Rio). Một kỉ lục c&amp;oacute; lẽ sẽ đứng vững trong bảng th&amp;agrave;nh t&amp;iacute;ch lịch sử.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sinh ra với nền tảng thể chất ho&amp;agrave;n hảo nhất cho bơi lội, nhưng những th&amp;agrave;nh c&amp;ocirc;ng của anh c&amp;oacute; được lại nhờ phần lớn v&amp;agrave;o c&amp;aacute;c yếu tố kh&amp;aacute;c, c&amp;aacute;ch anh đặt ra những mục ti&amp;ecirc;u ngắn hạn d&amp;agrave;i hạn cũng như chuẩn bị cho mọi ho&amp;agrave;n cảnh c&amp;oacute; thể xảy ra...&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cafebiz.cafebizcdn.vn/thumb_w/600/2016/photo-4-1470820025283-crop-1470820394575.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;Dưới đ&amp;acirc;y l&amp;agrave; 6 th&amp;oacute;i quen đ&amp;atilde; gi&amp;uacute;p Phelps trở th&amp;agrave;nh vận động vi&amp;ecirc;n vĩ đại nhất, m&amp;agrave; bạn ho&amp;agrave;n to&amp;agrave;n c&amp;oacute; thể học hỏi v&amp;agrave; &amp;aacute;p dụng ch&amp;uacute;ng trong cuộc sống v&amp;agrave; c&amp;ocirc;ng việc của m&amp;igrave;nh.&lt;/p&gt;
&lt;h1 id="dat-ra-muc-tieu-ro-rang"&gt;Đặt ra mục ti&amp;ecirc;u r&amp;otilde; …&lt;/h1&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Vận động vi&amp;ecirc;n nhiều huy chương nhất mọi thời đại vừa th&amp;ecirc;m v&amp;agrave;o bộ sưu tập khổng lồ của m&amp;igrave;nh chiếc huy chương v&amp;agrave;ng thứ 21 tại nội dung 4x200m tự do tiếp sức ( 3 chiếc trong 3 ng&amp;agrave;y tại Rio). Một kỉ lục c&amp;oacute; lẽ sẽ đứng vững trong bảng th&amp;agrave;nh t&amp;iacute;ch lịch sử.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sinh ra với nền tảng thể chất ho&amp;agrave;n hảo nhất cho bơi lội, nhưng những th&amp;agrave;nh c&amp;ocirc;ng của anh c&amp;oacute; được lại nhờ phần lớn v&amp;agrave;o c&amp;aacute;c yếu tố kh&amp;aacute;c, c&amp;aacute;ch anh đặt ra những mục ti&amp;ecirc;u ngắn hạn d&amp;agrave;i hạn cũng như chuẩn bị cho mọi ho&amp;agrave;n cảnh c&amp;oacute; thể xảy ra...&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cafebiz.cafebizcdn.vn/thumb_w/600/2016/photo-4-1470820025283-crop-1470820394575.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;Dưới đ&amp;acirc;y l&amp;agrave; 6 th&amp;oacute;i quen đ&amp;atilde; gi&amp;uacute;p Phelps trở th&amp;agrave;nh vận động vi&amp;ecirc;n vĩ đại nhất, m&amp;agrave; bạn ho&amp;agrave;n to&amp;agrave;n c&amp;oacute; thể học hỏi v&amp;agrave; &amp;aacute;p dụng ch&amp;uacute;ng trong cuộc sống v&amp;agrave; c&amp;ocirc;ng việc của m&amp;igrave;nh.&lt;/p&gt;
&lt;h1 id="dat-ra-muc-tieu-ro-rang"&gt;Đặt ra mục ti&amp;ecirc;u r&amp;otilde; r&amp;agrave;ng&lt;/h1&gt;
&lt;p&gt;Kể từ 8 tuổi, Phelps đ&amp;atilde; đặt ra mục ti&amp;ecirc;u v&amp;agrave; mường tượng về tương lai của m&amp;igrave;nh tại Olympics, đ&amp;acirc;y l&amp;agrave; "bảng thiết lập mục ti&amp;ecirc;u" m&amp;agrave; anh đ&amp;atilde; viết khi c&amp;ograve;n l&amp;agrave; một đứa trẻ&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://cafebiz.cafebizcdn.vn/k:thumb_w/640/2016/photo-1-1470820024634/biquyetthanhcongcuamichaelphelpsnhavodichsohuu21huychuongvangolympics.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;Giờ đ&amp;acirc;y, với tổng cộng 25 huy chương v&amp;agrave; c&amp;oacute; thể sẽ c&amp;ograve;n hơn thế nữa, Phelps v&amp;agrave; những th&amp;agrave;nh c&amp;ocirc;ng của anh ch&amp;iacute;nh l&amp;agrave; th&amp;agrave;nh quả của việc thiết lập mục ti&amp;ecirc;u cực kỳ chi tiết cho cả ngắn hạn v&amp;agrave; d&amp;agrave;i hạn. Anh viết từng khung giờ phải đạt được cho từng v&amp;ograve;ng đua trong từng ng&amp;agrave;y luyện tập v&amp;agrave; kiểm tra lại ch&amp;uacute;ng hằng ng&amp;agrave;y.&lt;/p&gt;
&lt;p&gt;"T&amp;ocirc;i để bảng mục ti&amp;ecirc;u của m&amp;igrave;nh ở chỗ t&amp;ocirc;i c&amp;oacute; thể nh&amp;igrave;n được mọi l&amp;uacute;c, vậy n&amp;ecirc;n khi ngủ dậy t&amp;ocirc;i sẽ biết ngay ng&amp;agrave;y h&amp;ocirc;m nay m&amp;igrave;nh sẽ cần phải l&amp;agrave;m g&amp;igrave; để đạt được những g&amp;igrave;", Phelps n&amp;oacute;i.&lt;/p&gt;
&lt;p&gt;Những mục ti&amp;ecirc;u l&amp;agrave;m bạn cho&amp;aacute;ng ngợp, tạo động lực hay &amp;eacute;p bạn ra khỏi 'v&amp;ugrave;ng an to&amp;agrave;n' của bản th&amp;acirc;n sẽ l&amp;agrave; những mục ti&amp;ecirc;u th&amp;uacute;c đẩy giới hạn của bạn đi xa hơn mỗi ng&amp;agrave;y.&lt;/p&gt;
&lt;h1 id="hinh-dung-moi-thu"&gt;H&amp;igrave;nh dung mọi thứ&lt;/h1&gt;
&lt;p&gt;Từ hồi c&amp;ograve;n l&amp;agrave; thiếu ni&amp;ecirc;n, huấn luyện vi&amp;ecirc;n của Phelps - Bob Bowman lu&amp;ocirc;n nhắc anh sau mỗi buổi tập l&amp;agrave; h&amp;atilde;y về nh&amp;agrave; v&amp;agrave; 'mường tượng'. H&amp;atilde;y mường tượng bản th&amp;acirc;n m&amp;igrave;nh bơi một cuộc đua ho&amp;agrave;n hảo trước khi đi ngủ v&amp;agrave; ngay sau khi thức dậy, h&amp;igrave;nh dung ra cả c&amp;aacute;c đối thủ xung quanh v&amp;agrave; cảm nhận cả những tiểu tiết như từng nhịp thở hay từng giọt nước siết qua m&amp;ocirc;i m&amp;igrave;nh như thế n&amp;agrave;o, từng đ&amp;ecirc;m v&amp;agrave; từng s&amp;aacute;ng như vậy đ&amp;atilde; gi&amp;uacute;p anh đạt được th&amp;agrave;nh c&amp;ocirc;ng.
&lt;img alt="" src="http://cafebiz.cafebizcdn.vn/k:thumb_w/640/2016/photo-2-1470820025862/biquyetthanhcongcuamichaelphelpsnhavodichsohuu21huychuongvangolympics.jpg"/&gt;
Đ&amp;ecirc;m trước mỗi một cuộc đua, Phelps sẽ h&amp;igrave;nh dung m&amp;igrave;nh tại nh&amp;agrave; thi đấu dưới g&amp;oacute;c nh&amp;igrave;n của bản th&amp;acirc;n v&amp;agrave; dưới cả g&amp;oacute;c quan s&amp;aacute;t của những người tr&amp;ecirc;n kh&amp;aacute;n đ&amp;agrave;i nữa.&lt;/p&gt;
&lt;h1 id="chuan-bi-cho-moi-tinh-huong"&gt;Chuẩn bị cho mọi t&amp;igrave;nh huống&lt;/h1&gt;
&lt;p&gt;Kh&amp;ocirc;ng chỉ h&amp;igrave;nh dung một cuộc đua ho&amp;agrave;n hảo, Phelps c&amp;ograve;n chuẩn bị v&amp;agrave; đặt bản th&amp;acirc;n v&amp;agrave;o bất kỳ t&amp;igrave;nh huống n&amp;agrave;o c&amp;oacute; thể xảy ra trong một cuộc đua d&amp;ugrave; với khả năng nhỏ nhất. Đặt ra mọi kế hoạch xử l&amp;yacute; nếu đồ bơi của anh bị r&amp;aacute;ch hay k&amp;iacute;nh bị d&amp;ograve; nước giữa cuộc đua.&lt;/p&gt;
&lt;p&gt;V&amp;agrave; mọi thứ sẽ kh&amp;ocirc;ng bao giờ ho&amp;agrave;n hảo. Trong chung kết 200m bơi bướm tại Bắc Kinh 2008, k&amp;iacute;nh bơi của Phelps đ&amp;atilde; bị vỡ v&amp;agrave; anh ho&amp;agrave;n to&amp;agrave;n kh&amp;ocirc;ng nh&amp;igrave;n thấy g&amp;igrave; dưới nước.
&lt;img alt="" src="http://cafebiz.cafebizcdn.vn/k:thumb_w/640/2016/photo-3-1470820026025/biquyetthanhcongcuamichaelphelpsnhavodichsohuu21huychuongvangolympics.jpg"/&gt;
Mọi người c&amp;oacute; thể cho rằng mất tầm nh&amp;igrave;n giữa một cuộc đua ở Olympics th&amp;igrave; ai c&amp;oacute; thể giữ được b&amp;igrave;nh tĩnh, nhưng bởi anh đ&amp;atilde; chuẩn bị cho t&amp;igrave;nh huống n&amp;agrave;y rồi, kế hoạch đ&amp;atilde; c&amp;oacute; v&amp;agrave; Phelps ho&amp;agrave;n th&amp;agrave;nh cuộc đua với một huy chương v&amp;agrave;ng c&amp;ugrave;ng với một kỉ lục thế giới mới được x&amp;aacute;c lập.&lt;/p&gt;
&lt;h1 id="khong-duoc-chim-dam-vao-that-bai"&gt;Kh&amp;ocirc;ng được ch&amp;igrave;m đắm v&amp;agrave;o thất bại&lt;/h1&gt;
&lt;p&gt;Miễn l&amp;agrave; Phelps biết m&amp;igrave;nh đ&amp;atilde; cố gắng hết sức - anh đ&amp;atilde; luyện tập, chuẩn bị mọi thứ ở mức tối đa - anh sẽ kh&amp;ocirc;ng bị ảnh hưởng t&amp;acirc;m l&amp;yacute; khi thua một cuộc đua n&amp;agrave;o. Huấn luyện vi&amp;ecirc;n của anh lu&amp;ocirc;n nhắc nhở c&amp;aacute;c vận động vi&amp;ecirc;n kh&amp;ocirc;ng n&amp;ecirc;n tập trung v&amp;agrave;o kết quả thi đấu, m&amp;agrave; l&amp;agrave; qu&amp;aacute; tr&amp;igrave;nh luyện tập.&lt;/p&gt;
&lt;p&gt;Từ đ&amp;oacute; Phelps cũng lu&amp;ocirc;n l&amp;agrave; nguồn cảm hứng bất tận cho những người kh&amp;aacute;c, "D&amp;ugrave; thắng hay thua, miễn l&amp;agrave; bạn đang theo đuổi giấc mơ của m&amp;igrave;nh với tất cả những g&amp;igrave; bạn c&amp;oacute; th&amp;igrave; chắc chắn bạn sẽ th&amp;agrave;nh c&amp;ocirc;ng".
&lt;img alt="" src="http://cafebiz.cafebizcdn.vn/k:thumb_w/640/2016/photo-4-1470820025283/biquyetthanhcongcuamichaelphelpsnhavodichsohuu21huychuongvangolympics.jpg"/&gt;&lt;/p&gt;
&lt;h1 id="rut-kinh-nghiem-tu-loi-lam-cua-minh"&gt;R&amp;uacute;t kinh nghiệm từ lỗi lầm của m&amp;igrave;nh&lt;/h1&gt;
&lt;p&gt;Tại thế vận hội London 2012, chuỗi chiến thắng của Phelps bị cắt đoạn bởi Chad le Clos, vận động vi&amp;ecirc;n Nam Phi đ&amp;atilde; đ&amp;aacute;nh bại Phelps bằng 0.05 gi&amp;acirc;y tại nội dung 200m bơi bướm.&lt;/p&gt;
&lt;p&gt;Tất cả chỉ l&amp;agrave;m Phelps th&amp;ecirc;m quyết t&amp;acirc;m, anh đ&amp;atilde; xem lại rất nhiều lần đoạn video đ&amp;oacute; v&amp;agrave; nhận ra lỗi m&amp;igrave;nh mắc phải v&amp;agrave; cần thay đổi những g&amp;igrave;.&lt;/p&gt;
&lt;p&gt;V&amp;agrave; anh đ&amp;atilde; phục th&amp;ugrave; khi vừa d&amp;agrave;nh chiến thắng trước Le Clos tại Rio 2016 trong ch&amp;iacute;nh nội dung 200m bơi bướm, đem lại cho m&amp;igrave;nh chiếc huy chương v&amp;agrave;ng thứ 20 trong sự nghiệp.
&lt;img alt="" src="http://cafebiz.cafebizcdn.vn/k:thumb_w/640/2016/photo-5-1470820027610/biquyetthanhcongcuamichaelphelpsnhavodichsohuu21huychuongvangolympics.jpg"/&gt;&lt;/p&gt;
&lt;h1 id="no-luc-luyen-tap"&gt;Nỗ lực luyện tập&lt;/h1&gt;
&lt;p&gt;Kh&amp;ocirc;ng ai c&amp;oacute; thể đạt được sự tuyệt hảo m&amp;agrave; kh&amp;ocirc;ng cần d&amp;agrave;nh thời gian v&amp;agrave; nỗ lực v&amp;agrave;o luyện tập. Phelps c&amp;oacute; thể được ban tặng cho một th&amp;acirc;n h&amp;igrave;nh bơi lội ho&amp;agrave;n mỹ, nhưng anh chắc chắn kh&amp;ocirc;ng thể d&amp;agrave;nh được những vinh quang h&amp;ocirc;m nay nếu kh&amp;ocirc;ng ki&amp;ecirc;n tr&amp;igrave; v&amp;agrave; ph&amp;aacute; vỡ giới hạn của bản th&amp;acirc;n mội ng&amp;agrave;y.
&lt;img alt="" src="http://cafebiz.cafebizcdn.vn/k:thumb_w/640/2016/photo-6-1470820027164/biquyetthanhcongcuamichaelphelpsnhavodichsohuu21huychuongvangolympics.jpg"/&gt;
Trước thềm thế vận hội Athen 2004, anh luyện tập 365 ng&amp;agrave;y/năm trong li&amp;ecirc;n tục 6 năm, d&amp;ugrave; l&amp;agrave; gi&amp;aacute;ng sinh hay sinh nhật. Anh n&amp;oacute;i: "T&amp;ocirc;i kh&amp;ocirc;ng d&amp;agrave;nh ra chủ nhật để nghỉ bởi nếu t&amp;ocirc;i tập cả v&amp;agrave;o chủ nhật m&amp;agrave; những người kh&amp;aacute;c kh&amp;ocirc;ng, t&amp;ocirc;i sẽ c&amp;oacute; th&amp;ecirc;m 52 buổi tập mỗi năm so với họ".&lt;/p&gt;
&lt;p&gt;Nguồn: &lt;a href="http://cafebiz.vn/bi-quyet-thanh-cong-cua-michael-phelps-nha-vo-dich-so-huu-21-huy-chuong-vang-olympics-20160810161446211.chn"&gt;Cafebiz&lt;/a&gt;.&lt;/p&gt;</content><category term="Inspiration"></category><category term="OTHER"></category></entry><entry><title>The evolution of image classification</title><link href="/blog/paper/2018/the-evolution-of-image-classification/" rel="alternate"></link><published>2018-08-21T11:14:24+00:00</published><updated>2018-08-21T11:14:24+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-21:/blog/paper/2018/the-evolution-of-image-classification/</id><summary type="html">&lt;p&gt;In this blog post, we will talk about the evolution of image classification from a high-level perspective. The goal here is to try to understand the key changes that were brought along the years, and why they succeeded in solving our problems.&lt;/p&gt;
&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;h2 id="why-it-matters"&gt;Why it matters&lt;/h2&gt;
&lt;p&gt;Recent research in deep learning has been largely inspired by the way our brain works. When you think of it, it is fascinating to know that with a given input, our brain processes features that say let us know of the world that surrounds us.&lt;/p&gt;
&lt;p&gt;As a result, architectures are crucial for us, not only because many challenges rely on the tasks we can perform with them. In fact, the design of the networks themselves points us out to the representation that researchers were looking for, in order to better learn from the data.&lt;/p&gt;
&lt;h1 id="lenet_1"&gt;LeNet&lt;/h1&gt;
&lt;h2 id="pioneering-work"&gt;Pioneering work&lt;/h2&gt;
&lt;p&gt;Before starting, let's note that we would …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this blog post, we will talk about the evolution of image classification from a high-level perspective. The goal here is to try to understand the key changes that were brought along the years, and why they succeeded in solving our problems.&lt;/p&gt;
&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;h2 id="why-it-matters"&gt;Why it matters&lt;/h2&gt;
&lt;p&gt;Recent research in deep learning has been largely inspired by the way our brain works. When you think of it, it is fascinating to know that with a given input, our brain processes features that say let us know of the world that surrounds us.&lt;/p&gt;
&lt;p&gt;As a result, architectures are crucial for us, not only because many challenges rely on the tasks we can perform with them. In fact, the design of the networks themselves points us out to the representation that researchers were looking for, in order to better learn from the data.&lt;/p&gt;
&lt;h1 id="lenet_1"&gt;LeNet&lt;/h1&gt;
&lt;h2 id="pioneering-work"&gt;Pioneering work&lt;/h2&gt;
&lt;p&gt;Before starting, let's note that we would not have been successful if we simply used a raw multi-layer perceptron connected to each pixel of an image. On top of becoming quickly intractable, this direct operation is not very efficient as pixels are spatially correlated.&lt;/p&gt;
&lt;p&gt;Therefore, we first need to extract&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;meaningful and&lt;/li&gt;
&lt;li&gt;low-dimensional features that we can work on.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And that's where convolutional neural networks come in the game!&lt;/p&gt;
&lt;p&gt;To tackle this issue, Yann Le Cun's idea proceeds in multiple steps.
&lt;img alt="" src="https://stanford.edu/~shervine/images/LeNet.png"/&gt;
Source: LeCun et al., 1998&lt;/p&gt;
&lt;p&gt;First, an input image is fed to the network. Filters of a given size scan the image and perform convolutions. The obtained features then go through an activation function. Then, the output goes through a succession of pooling and other convolution operations.&lt;/p&gt;
&lt;p&gt;As you can see, features are reduced in dimension as the network goes on.&lt;/p&gt;
&lt;p&gt;At the end, high-level features are flattened and fed to fully connected layers, which will eventually yield class probabilities through a softmax layer.&lt;/p&gt;
&lt;p&gt;During training time, the network learns how to recognize the features that make a sample belong to a given class through backpropagation.&lt;/p&gt;
&lt;p&gt;To give an example of what such a network can 'see': let's say we have an image of a horse. The first filters may focus on the animal's overall shape. And then as we go deeper, we can reach a higher level of abstraction where details like eyes and ears can be captured.&lt;/p&gt;
&lt;p&gt;That way, ConvNets appear as a way to construct features that we would have had to handcraft ourselves otherwise.&lt;/p&gt;
&lt;h1 id="alexnet_1"&gt;AlexNet&lt;/h1&gt;
&lt;p&gt;Convolution's rise to fame
Then you could wonder, why have ConvNets not been trendy since 1998? The short answer is: we had not leveraged their full potential back then.
&lt;img alt="" src="https://stanford.edu/~shervine/images/AlexNet.png"/&gt;
Source: Krizhevsky et al., 2009&lt;/p&gt;
&lt;p&gt;Here, AlexNet takes the same top-down approach, where successive filters are designed to capture more and more subtle features. But here, his work explored several crucial details.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First, Krizhevsky introduced better non-linearity in the network with the ReLU activation, whose derivative is 0 if the feature is below 0 and 1 for positive values. This proved to be efficient for gradient propagation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second, his paper introduced the concept of dropout as regularization. From a representation point of view, you force the network to forget things at random, so that it can see your next input data from a better perspective.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Just to give an example, after you finish reading this post, you will have most probably forgotten parts of it. And yet this is OK, because you will have only kept in mind what was essential.&lt;/p&gt;
&lt;p&gt;Well, hopefully.&lt;/p&gt;
&lt;p&gt;The same happens for neural networks, and leads the model to be more robust.&lt;/p&gt;
&lt;p&gt;Also, it introduced data augmentation. When fed to the network, images are shown with random translation, rotation, crop. That way, it forces the network to be more aware of the attributes of the images, rather than the images themselves.
Finally, another trick used by AlexNet is to be deeper. You can see here that they stacked more convolutional layers before pooling operations. The representation captures consequently finer features that reveal to be useful for classification.&lt;/p&gt;
&lt;p&gt;This network largely outperformed what was state-of-the-art back in 2012, with a 15.4% top-5 error on the ImageNet dataset.&lt;/p&gt;
&lt;h1 id="vggnet"&gt;VGGNet&lt;/h1&gt;
&lt;h2 id="deeper-is-better"&gt;Deeper is better&lt;/h2&gt;
&lt;p&gt;The next big milestone of image classification further explored the last point that I mentioned: going deeper.&lt;/p&gt;
&lt;p&gt;And it works. This suggests that such networks can achieve a better hierarchical representation of visual data with more layers.
&lt;img alt="" src="https://stanford.edu/~shervine/images/VGGNet.png"/&gt;&lt;/p&gt;
&lt;p&gt;Source: Simonyan et al., 2014
As you can see, something else is very special on this network. It contains almost exclusively 3 by 3 convolutions. This is curious, isn't?&lt;/p&gt;
&lt;p&gt;In fact, the authors were driven by three main reasons to do so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First, using small filters induces more non-linearity, which means more degrees of freedom for the network.&lt;/li&gt;
&lt;li&gt;Second, the fact of stacking these layers together enables the network to see more things than it looks like. For example, with two of these, the network in fact sees a 5x5 receptive field. And when you stack 3 of these filters, you have in fact a 7x7 receptive field! Therefore, the same feature extraction capabilities as in the previous examples can be achieved on this architecture as well.&lt;/li&gt;
&lt;li&gt;Third, using only small filters also limits the number of parameters, which is good when you want to go that deep.
Quantitatively speaking, this architecture achieved a 7.3% top-5 error on ImageNet.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="googlenet_1"&gt;GoogLeNet&lt;/h1&gt;
&lt;h2 id="time-for-inception"&gt;Time for inception&lt;/h2&gt;
&lt;p&gt;Next, GoogLeNet came in the game. It bases its success on its inception modules.
&lt;img alt="" src="https://stanford.edu/~shervine/images/GoogLeNet.png"/&gt;
Source: Szegedy et al., 2015
As you can see, convolutions with different filter sizes are processed on the same input, and then concatenated together.&lt;/p&gt;
&lt;p&gt;From a representation point of view, this allows the model to take advantage of multi-level feature extraction at each step. For example, general features can be extracted by the 5x5 filters at the same time that more local features are captured by the 3x3 convolutions.&lt;/p&gt;
&lt;p&gt;But then, you could tell me. Well that's great. But isn't that insanely expensive to compute?&lt;/p&gt;
&lt;p&gt;And I would say: very good remark! Actually, the Google team had a brilliant solution for this: 1x1 convolutions.&lt;/p&gt;
&lt;p&gt;On the one hand, it reduces the dimensionality of your features.
On the other, it combines feature maps in a way that can be beneficial from a representation perspective.
Then you could ask, why is it called inception? Well, you can see all of those modules as being networks stacked one over another inside a bigger network.&lt;/p&gt;
&lt;p&gt;And for the record, the best GoogLeNet ensemble achieved a 6.7% error on ImageNet.&lt;/p&gt;
&lt;h1 id="resnet_1"&gt;ResNet&lt;/h1&gt;
&lt;h2 id="connect-the-layers"&gt;Connect the layers&lt;/h2&gt;
&lt;p&gt;So all these networks we talked about earlier followed the same trend: going deeper. But at some point, we realize that stacking more layers does not lead to better performance. In fact, the exact opposite occurs. But why is that?&lt;/p&gt;
&lt;p&gt;In one word: the gradient, ladies and gentlemen.&lt;/p&gt;
&lt;p&gt;But don't worry, researchers found a trick to counter this effect. Here, the key concept developed by ResNet is residual learning.
&lt;img alt="" src="https://stanford.edu/~shervine/images/ResNet.png"/&gt;
Source: He et al., 2015
As you can see, every two layers, there is an identity mapping via an element-wise addition. This proved to be very helpful for gradient propagation, as the error can be backpropagated through multiple paths.&lt;/p&gt;
&lt;p&gt;Also, from a representation point of view, this helps to combine different levels of features at each step of the network, just like we saw it with the inception modules.&lt;/p&gt;
&lt;p&gt;It is to this date one of the best performing network on ImageNet, with a 3.6% top-5 error rate.&lt;/p&gt;
&lt;h1 id="densenet_1"&gt;DenseNet&lt;/h1&gt;
&lt;h2 id="connect-more"&gt;Connect more!&lt;/h2&gt;
&lt;p&gt;An extension of this reasoning has been later proposed. DenseNet proposes entire blocks of layers connected to one another.
&lt;img alt="" src="https://stanford.edu/~shervine/images/DenseNet.png"/&gt;
Source: Huang et al., 2016
This contributes to &lt;strong&gt;diversifying&lt;/strong&gt; a lot more the &lt;strong&gt;features&lt;/strong&gt; within those blocks.&lt;/p&gt;
&lt;h1 id="conclusion_1"&gt;Conclusion&lt;/h1&gt;
&lt;h2 id="global-trends"&gt;Global trends&lt;/h2&gt;
&lt;p&gt;A major pattern observed overall is that &lt;strong&gt;networks&lt;/strong&gt; are designed to be &lt;strong&gt;deeper and deeper&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Computational tricks&lt;/strong&gt; (ReLU, dropout, batch normalization) have been also introduced alongside them and had a significant impact in improving performance.&lt;/p&gt;
&lt;p&gt;We have also seen the apparition of &lt;strong&gt;modules&lt;/strong&gt; that are able to capture rich features at each step of the network.&lt;/p&gt;
&lt;p&gt;Finally, another major point is the increasing use of &lt;strong&gt;connections between the layers&lt;/strong&gt; of the network, which helps for producing diverse features and revealed to be useful for gradient propagation.&lt;/p&gt;
&lt;h2 id="disclaimer"&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;This article is written by Shervine, and the original post can be found &lt;a href="https://stanford.edu/~shervine/blog/evolution-image-classification-explained.html"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="Image Classification"></category><category term="CNN"></category><category term="2015"></category><category term="PAPER"></category></entry><entry><title>An example of how to generate your data in parallel with PyTorch</title><link href="/blog/tutorial/2018/an-example-of-how-to-generate-your-data-in-parallel-with-pytorch/" rel="alternate"></link><published>2018-08-21T00:02:01+00:00</published><updated>2018-08-21T00:02:01+00:00</updated><author><name>Shervine</name></author><id>tag:None,2018-08-21:/blog/tutorial/2018/an-example-of-how-to-generate-your-data-in-parallel-with-pytorch/</id><summary type="html">&lt;h1 id="motivation"&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Have you ever had to load a dataset that was so memory consuming that you wished a magic trick could seamlessly take care of that? Large datasets are increasingly becoming part of our lives, as we are able to harness an ever-growing quantity of data.&lt;/p&gt;
&lt;p&gt;We have to keep in mind that in some cases, even the most state-of-the-art configuration won't have enough memory space to process the data the way we used to do it. That is the reason why we need to find other ways to do that task efficiently. In this blog post, we are going to show you &lt;strong&gt;how to generate your data on multiple cores in real time&lt;/strong&gt; and feed it right away to your deep learning model.&lt;/p&gt;
&lt;p&gt;This tutorial will show you how to do so on the GPU-friendly framework PyTorch, where &lt;strong&gt;an efficient data generation scheme&lt;/strong&gt; is crucial to leverage the full …&lt;/p&gt;</summary><content type="html">&lt;h1 id="motivation"&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Have you ever had to load a dataset that was so memory consuming that you wished a magic trick could seamlessly take care of that? Large datasets are increasingly becoming part of our lives, as we are able to harness an ever-growing quantity of data.&lt;/p&gt;
&lt;p&gt;We have to keep in mind that in some cases, even the most state-of-the-art configuration won't have enough memory space to process the data the way we used to do it. That is the reason why we need to find other ways to do that task efficiently. In this blog post, we are going to show you &lt;strong&gt;how to generate your data on multiple cores in real time&lt;/strong&gt; and feed it right away to your deep learning model.&lt;/p&gt;
&lt;p&gt;This tutorial will show you how to do so on the GPU-friendly framework PyTorch, where &lt;strong&gt;an efficient data generation scheme&lt;/strong&gt; is crucial to leverage the full potential of your GPU during the training process.&lt;/p&gt;
&lt;h1 id="tutorial"&gt;Tutorial&lt;/h1&gt;
&lt;h2 id="previous-situation"&gt;Previous situation&lt;/h2&gt;
&lt;p&gt;Before reading this article, your PyTorch script probably looked like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Load entire dataset&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'some_training_set_with_labels.pt'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Train model&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Local batches and labels&lt;/span&gt;
        &lt;span class="n"&gt;local_X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;local_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;:(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;,],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;:(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;n_batches&lt;/span&gt;&lt;span class="p"&gt;,]&lt;/span&gt;

        &lt;span class="c1"&gt;# Your model&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or even this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Unoptimized generator&lt;/span&gt;
&lt;span class="n"&gt;training_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SomeSingleCoreGenerator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'some_training_set_with_labels.pt'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Train model&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;local_X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;local_y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;training_generator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Your model&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This article is about optimizing the entire data generation process, so that it does not become a bottleneck in the training procedure.&lt;/p&gt;
&lt;p&gt;In order to do so, let's dive into a step by step recipe that builds a parallelizable data generator suited for this situation. By the way, the following code is a good skeleton to use for your own project; you can copy/paste the following pieces of code and fill the blanks accordingly.&lt;/p&gt;
&lt;h1 id="notations_1"&gt;Notations&lt;/h1&gt;
&lt;p&gt;Before getting started, let's go through a few organizational tips that are particularly useful when dealing with large datasets.&lt;/p&gt;
&lt;p&gt;Let &lt;code&gt;ID&lt;/code&gt; be the Python string that identifies a given sample of the dataset. A good way to keep track of samples and their labels is to adopt the following framework:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a dictionary called &lt;code&gt;partition&lt;/code&gt; where you gather:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in &lt;code&gt;partition['train']&lt;/code&gt; a list of training IDs&lt;/li&gt;
&lt;li&gt;in &lt;code&gt;partition['validation']&lt;/code&gt; a list of validation IDs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a dictionary called &lt;code&gt;labels&lt;/code&gt; where for each &lt;code&gt;ID&lt;/code&gt; of the dataset, the associated label is given by &lt;code&gt;labels[ID]&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, let's say that our training set contains &lt;code&gt;id-1&lt;/code&gt;, &lt;code&gt;id-2&lt;/code&gt; and &lt;code&gt;id-3&lt;/code&gt; with respective labels &lt;code&gt;0&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;2&lt;/code&gt;, with a validation set containing &lt;code&gt;id-4&lt;/code&gt; with label &lt;code&gt;1&lt;/code&gt;. In that case, the Python variables partition and labels look like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt;&amp;gt;&amp;gt; partition
&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'train'&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'id-1'&lt;/span&gt;, &lt;span class="s1"&gt;'id-2'&lt;/span&gt;, &lt;span class="s1"&gt;'id-3'&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;, &lt;span class="s1"&gt;'validation'&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'id-4'&lt;/span&gt;&lt;span class="o"&gt;]}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt;&amp;gt;&amp;gt; labels
&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'id-1'&lt;/span&gt;: &lt;span class="m"&gt;0&lt;/span&gt;, &lt;span class="s1"&gt;'id-2'&lt;/span&gt;: &lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="s1"&gt;'id-3'&lt;/span&gt;: &lt;span class="m"&gt;2&lt;/span&gt;, &lt;span class="s1"&gt;'id-4'&lt;/span&gt;: &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also, for the sake of modularity, we will write PyTorch code and customized classes in separate files, so that your folder looks like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;folder/
├── my_classes.py
├── pytorch_script.py
└── data/
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where &lt;code&gt;data/&lt;/code&gt; is assumed to be the folder containing your dataset.&lt;/p&gt;
&lt;p&gt;Finally, it is good to note that the code in this tutorial is aimed at being general and minimal, so that you can easily adapt it for your own dataset.&lt;/p&gt;
&lt;h1 id="dataset"&gt;Dataset&lt;/h1&gt;
&lt;p&gt;Now, let's go through the details of how to set the Python class &lt;code&gt;Dataset&lt;/code&gt;, which will characterize the key features of the dataset you want to generate.&lt;/p&gt;
&lt;p&gt;First, let's write the initialization function of the class. We make the latter inherit the properties of &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt; so that we can later leverage nice functionalities such as multiprocessing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;list_IDs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s1"&gt;'Initialization'&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;list_IDs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;list_IDs&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There, we store important information such as labels and the list of IDs that we wish to generate at each pass.&lt;/p&gt;
&lt;p&gt;Each call requests a sample index for which the upperbound is specified in the &lt;code&gt;__len__&lt;/code&gt; method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__len__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s1"&gt;'Denotes the total number of samples'&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;list_IDs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, when the sample corresponding to a given index is called, the generator executes the &lt;code&gt;__getitem__&lt;/code&gt; method to generate it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__getitem__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s1"&gt;'Generates one sample of data'&lt;/span&gt;
    &lt;span class="c1"&gt;# Select sample&lt;/span&gt;
    &lt;span class="n"&gt;ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;list_IDs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# Load data and get label&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'data/'&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ID&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;'.pt'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ID&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;During data generation, this method reads the Torch tensor of a given example from its corresponding file ID.pt. Since our code is designed to be multicore-friendly, note that you can do more complex operations instead (e.g. computations from source files) without worrying that data generation becomes a bottleneck in the training process.&lt;/p&gt;
&lt;p&gt;The complete code corresponding to the steps that we described in this section is shown below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dataset&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="s1"&gt;'Characterizes a dataset for PyTorch'&lt;/span&gt;
  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;list_IDs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s1"&gt;'Initialization'&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;list_IDs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;list_IDs&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__len__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s1"&gt;'Denotes the total number of samples'&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;list_IDs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__getitem__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s1"&gt;'Generates one sample of data'&lt;/span&gt;
        &lt;span class="c1"&gt;# Select sample&lt;/span&gt;
        &lt;span class="n"&gt;ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;list_IDs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="c1"&gt;# Load data and get label&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'data/'&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ID&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;'.pt'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ID&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="pytorch-script"&gt;PyTorch script&lt;/h1&gt;
&lt;p&gt;Now, we have to modify our PyTorch script accordingly so that it accepts the generator that we just created. In order to do so, we use PyTorch's &lt;code&gt;DataLoader&lt;/code&gt; class, which in addition to our &lt;code&gt;Dataset&lt;/code&gt; class, also takes in the following important arguments:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;* `batch_size`, which denotes the number of samples contained in each generated batch.
* `shuffle`. If set to `True`, we will get a new order of exploration at each pass (or just keep a linear exploration scheme otherwise). Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.
* `num_workers`, which denotes the number of processes that generate batches in parallel. A high enough number of workers assures that CPU computations are efficiently managed, i.e. that the bottleneck is indeed the neural network's forward and backward operations on the GPU (and not data generation).
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A proposition of code template that you can write in your script is shown below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;my_classes&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Dataset&lt;/span&gt;


&lt;span class="c1"&gt;# Parameters&lt;/span&gt;
&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'batch_size'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;'shuffle'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="s1"&gt;'num_workers'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;max_epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;

&lt;span class="c1"&gt;# Datasets&lt;/span&gt;
&lt;span class="n"&gt;partition&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="c1"&gt;# IDs&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="c1"&gt;# Labels&lt;/span&gt;

&lt;span class="c1"&gt;# Generators&lt;/span&gt;
&lt;span class="n"&gt;training_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;partition&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;Dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;partition&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'validation'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;training_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;validation_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Training process&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;local_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;local_labels&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;training_generator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Your model&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="c1"&gt;# Validation process&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;local_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;local_labels&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;validation_generator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# Your model&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This is it! You can now run your PyTorch script with the command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python3 pytorch_script.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and you will see that during the training phase, &lt;strong&gt;data is generated in parallel by the CPU&lt;/strong&gt;, which can then be &lt;strong&gt;fed to the GPU&lt;/strong&gt; for neural network computations.&lt;/p&gt;
&lt;h1 id="disclaimer"&gt;Disclaimer&lt;/h1&gt;
&lt;p&gt;This article is written by Shervine which you can find the original blog post &lt;a href="https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel.html"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="Data Parallel"></category><category term="Data Loader"></category><category term="Pytorch"></category><category term="TUTORIAL"></category></entry><entry><title>Cheatsheet in ML and DL</title><link href="/blog/tutorial/2018/cheatsheet-in-ml-and-dl/" rel="alternate"></link><published>2018-08-20T23:31:44+00:00</published><updated>2018-08-20T23:31:44+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-20:/blog/tutorial/2018/cheatsheet-in-ml-and-dl/</id><summary type="html">&lt;h2 id="machine-learning-cheatsheet"&gt;Machine Learning Cheatsheet&lt;/h2&gt;
&lt;p&gt;https://ml-cheatsheet.readthedocs.io/&lt;/p&gt;
&lt;h2 id="deep-learning-cheatsheet"&gt;Deep Learning Cheatsheet&lt;/h2&gt;
&lt;h2 id="probability-cheatsheet"&gt;Probability Cheatsheet&lt;/h2&gt;
&lt;p&gt;https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-probability.html&lt;/p&gt;
&lt;h2 id="statistics-cheeatsheet"&gt;Statistics Cheeatsheet&lt;/h2&gt;
&lt;p&gt;https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-statistics.html&lt;/p&gt;</summary><content type="html">&lt;h2 id="machine-learning-cheatsheet"&gt;Machine Learning Cheatsheet&lt;/h2&gt;
&lt;p&gt;https://ml-cheatsheet.readthedocs.io/&lt;/p&gt;
&lt;h2 id="deep-learning-cheatsheet"&gt;Deep Learning Cheatsheet&lt;/h2&gt;
&lt;h2 id="probability-cheatsheet"&gt;Probability Cheatsheet&lt;/h2&gt;
&lt;p&gt;https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-probability.html&lt;/p&gt;
&lt;h2 id="statistics-cheeatsheet"&gt;Statistics Cheeatsheet&lt;/h2&gt;
&lt;p&gt;https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-statistics.html&lt;/p&gt;</content><category term="Cheatsheet"></category><category term="TUTORIAL"></category></entry><entry><title>Bash Commands</title><link href="/blog/tutorial/2018/bash-commands/" rel="alternate"></link><published>2018-08-17T12:22:35+00:00</published><updated>2018-08-17T12:22:35+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-17:/blog/tutorial/2018/bash-commands/</id><summary type="html">&lt;h1 id="the-most-common-commands"&gt;The most common commands&lt;/h1&gt;
&lt;p&gt;Below are some most popular commands in Linux Command Line (CLI).&lt;/p&gt;
&lt;h1 id="terminal-navigation-commands"&gt;Terminal Navigation Commands&lt;/h1&gt;
&lt;h2 id="cd-change-the-directory"&gt;cd: Change the directory&lt;/h2&gt;
&lt;p&gt;Change/move the directory to current path:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; path/to/directory
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="ls-list-all-the-files-and-folders-in-the-current-directory"&gt;ls: List all the files and folders in the current directory&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ls

&lt;span class="c1"&gt;# list all in long description&lt;/span&gt;
ls -l

&lt;span class="c1"&gt;# list all including the hidden files&lt;/span&gt;
ls -h
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="pwd-print-the-path-of-directories"&gt;pwd: Print the path of directories&lt;/h2&gt;
&lt;p&gt;Print the path to current directory&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="_1"&gt;&amp;amp;&amp;amp;&lt;/h2&gt;
&lt;p&gt;This one is so basic that it&amp;rsquo;s not even technically a command. If you ever want to run multiple commands in sequential order, just stick this in between each one. For example, [command1] &amp;amp;&amp;amp; [command2] will first run [command1] then immediately follow it with [command2]. You can chain as many commands as you want.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;comand1&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;command2&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="clear-clear-the-screen"&gt;clear: Clear the screen&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;clear
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="man-the-manual-command"&gt;man: The manual command&lt;/h2&gt;
&lt;p&gt;The man command is used to show the manual of the inputted …&lt;/p&gt;</summary><content type="html">&lt;h1 id="the-most-common-commands"&gt;The most common commands&lt;/h1&gt;
&lt;p&gt;Below are some most popular commands in Linux Command Line (CLI).&lt;/p&gt;
&lt;h1 id="terminal-navigation-commands"&gt;Terminal Navigation Commands&lt;/h1&gt;
&lt;h2 id="cd-change-the-directory"&gt;cd: Change the directory&lt;/h2&gt;
&lt;p&gt;Change/move the directory to current path:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; path/to/directory
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="ls-list-all-the-files-and-folders-in-the-current-directory"&gt;ls: List all the files and folders in the current directory&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ls

&lt;span class="c1"&gt;# list all in long description&lt;/span&gt;
ls -l

&lt;span class="c1"&gt;# list all including the hidden files&lt;/span&gt;
ls -h
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="pwd-print-the-path-of-directories"&gt;pwd: Print the path of directories&lt;/h2&gt;
&lt;p&gt;Print the path to current directory&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="_1"&gt;&amp;amp;&amp;amp;&lt;/h2&gt;
&lt;p&gt;This one is so basic that it&amp;rsquo;s not even technically a command. If you ever want to run multiple commands in sequential order, just stick this in between each one. For example, [command1] &amp;amp;&amp;amp; [command2] will first run [command1] then immediately follow it with [command2]. You can chain as many commands as you want.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;comand1&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;command2&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="clear-clear-the-screen"&gt;clear: Clear the screen&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;clear
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="man-the-manual-command"&gt;man: The manual command&lt;/h2&gt;
&lt;p&gt;The man command is used to show the manual of the inputted command.
Example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;man &lt;span class="nb"&gt;cd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="help-check-help-for-a-specific-command"&gt;help: Check help for a specific command&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;help&lt;/span&gt; &lt;span class="nb"&gt;command&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="history-view-the-command-history-in-terminal"&gt;history: View the command history in terminal&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;history&lt;/span&gt;
&lt;span class="c1"&gt;# View with more lines in history&lt;/span&gt;
&lt;span class="c1"&gt;# Replace N with a number&lt;/span&gt;
&lt;span class="nb"&gt;history&lt;/span&gt; N
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="file-management-commands_1"&gt;File Management Commands&lt;/h1&gt;
&lt;h2 id="find-search-files"&gt;find: Search files&lt;/h2&gt;
&lt;p&gt;Search a specific directory to find files that match given set of criteria including filename, filetype, filesize, permissions, owners, date created, date modified, etc.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;find file_name
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="grep-search-where-is-the-text-in-files"&gt;grep: Search where is the text in files&lt;/h2&gt;
&lt;p&gt;Searches a specific file or set of files to see if a given string of text exists, and if it does, tells you where the text exists in those files. This command is extremely flexible (e.g. use wildcards to search all files of a given type) and particularly useful for programmers (to find specific lines of code).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;grep 
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="rename"&gt;rename&lt;/h2&gt;
&lt;p&gt;https://ss64.com/bash/rename.html&lt;/p&gt;
&lt;h2 id="mkdir-create-a-directory"&gt;mkdir: Create a directory&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;mkdir&lt;/code&gt; - make directory - command allows the user to make a new directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir dir_name
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="mkdir-p-create-a-directory-tree"&gt;mkdir -p: Create a directory tree&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir -p dir0/dir1/dir2/dirk
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="mv-move-files_1"&gt;mv: Move files&lt;/h2&gt;
&lt;p&gt;The mv command - move - allows a user to move a file to another folder or directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mv file.txt /path/new/dir/
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="mv-change-the-files-name"&gt;mv: Change the file's name&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mv old_file_name new_file_name
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="cp-copy-files-and-folders"&gt;cp: Copy files and folders&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cp
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="rm-delete-files"&gt;rm: Delete files&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;rm&lt;/code&gt; command - remove - like the &lt;code&gt;rmdir&lt;/code&gt; command is meant to remove files from your Linux OS. Whereas the &lt;code&gt;rmdir&lt;/code&gt; command will remove directories and files held within, the &lt;code&gt;rm&lt;/code&gt; command will delete created files. An example of the &lt;code&gt;rm&lt;/code&gt; command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rm file.txt
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please be careful with the below command if somebody tells you.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rm -rf/
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Do not do it. The &lt;code&gt;rm -rf/&lt;/code&gt; command means remove (&lt;code&gt;rm&lt;/code&gt;) - recursive (&lt;code&gt;r&lt;/code&gt;) force (&lt;code&gt;f&lt;/code&gt;) home (&lt;code&gt;/&lt;/code&gt;). Spelled out logically, the rm -rf/ command will delete every folder, file and directory within your Linux OS. It is the equivalent of wiping your entire hard drive clean. Use the &lt;code&gt;rm-rf/&lt;/code&gt;command at your own peril. &lt;/p&gt;
&lt;h2 id="rmdir-remove-directory"&gt;rmdir: Remove Directory&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;rmdir&lt;/code&gt; - remove directory - command allows the user to remove an existing command using the Linux CLI. An example of the &lt;code&gt;rmdir&lt;/code&gt; command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rmdir dir_name
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="cat-view-a-file-in-terminal"&gt;cat: View a file in terminal&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cat file
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="touch-create-a-file-in-terminal"&gt;touch: Create a file in terminal&lt;/h2&gt;
&lt;p&gt;Create a file with &lt;code&gt;touch&lt;/code&gt;. The touch command - a.k.a. the make file command - allows users to make files using the Linux CLI. Just as the mkdir command makes directories, the touch command makes files. Just as you would make a .doc or a .txt using a PC desktop, the touch command makes empty files. An example of the touch command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;touch file
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="nano-create-and-open-a-file-in-terminal"&gt;nano: Create and open a file in terminal&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create or open a file&lt;/span&gt;
nano file
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="nvidia-smi-check-gpus-status"&gt;nvidia-smi: Check GPU's status&lt;/h2&gt;
&lt;p&gt;We use &lt;code&gt;watch -1&lt;/code&gt; to track the GPU's status every second. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;watch -1 nvidia-smi
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="kill-kill-a-process"&gt;kill: Kill a process&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;kill&lt;/span&gt; pid
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="echo-write"&gt;echo: Write&lt;/h2&gt;
&lt;h1 id="commands-with-git_1"&gt;Commands with git&lt;/h1&gt;
&lt;p&gt;If the current directory or files are under &lt;code&gt;git&lt;/code&gt; repository, it is useful to use &lt;code&gt;mv&lt;/code&gt; and &lt;code&gt;rm&lt;/code&gt; with &lt;code&gt;git&lt;/code&gt; command for further tracking:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git mv old_file_name new_file_name
&lt;span class="c1"&gt;# Change the name of a file or dir&lt;/span&gt;

git rm file
&lt;span class="c1"&gt;# Remove a file&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="shortcuts"&gt;Shortcuts&lt;/h1&gt;
&lt;h2 id="code"&gt;code&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Open the current folder&lt;/span&gt;
code .
&lt;span class="c1"&gt;# Open or create the file&lt;/span&gt;
code file
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="chrome"&gt;chrome&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="exit"&gt;Exit&lt;/h2&gt;
&lt;p&gt;We can press &lt;code&gt;Ctrl + C&lt;/code&gt; or sometimes &lt;code&gt;Ctrl + D&lt;/code&gt; to exit the current program in terminal.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;exit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="some-useful-commands_1"&gt;Some useful commands&lt;/h1&gt;
&lt;h2 id="ps-list-process-by-username"&gt;ps: List process by username&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ps -u ubuntu
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="htop-task-manager"&gt;htop: Task manager&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;htop
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="kill-a-process-on-a-specific-port"&gt;kill a process on a specific port&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# find the pid running on the port &lt;/span&gt;
lsof -i :port_number
&lt;span class="c1"&gt;# kill the process with this port&lt;/span&gt;
&lt;span class="nb"&gt;kill&lt;/span&gt; pid
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="list-all-the-process-running-gpus-and-kill-it"&gt;List all the process running GPUs and kill it.&lt;/h2&gt;
&lt;p&gt;Apart from nvidia-smi, on Linux you can check which processes might be using the GPU using the command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo fuser -v /dev/nvidia*
kill pid
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="install-pillow-simd-for-fast-augmentation"&gt;Install pillow-simd for fast augmentation&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip uninstall pillow
pip install pillow-simd
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="warning-remote-port-forwarding-failed-for-listen-port-52698"&gt;Warning: remote port forwarding failed for listen port 52698&lt;/h2&gt;
&lt;p&gt;This bug happens when rmate from server can&amp;rsquo;t forward to Visual Code in client machine. The reason is some sshd sessions aren&amp;rsquo;t dettached correctly.&lt;/p&gt;
&lt;p&gt;Solution&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ps -u username
$ &lt;span class="nb"&gt;kill&lt;/span&gt; &lt;span class="sb"&gt;`&lt;/span&gt;PID_of_sshd&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or we can do it as below: First, we need to install killall from psmisc package&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo apt-get install psmisc
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then, start killing all sshd connection.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo killall -u YOURUSER -r sshd
$ sudo killall -u ubuntu -r sshd
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="scp-copy-files-from-remote-server-to-local"&gt;scp: Copy files from remote server to local&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;scp -i /path/to/private_key/ username@ip:/path/to/file/on/server /path/to/location/on/local

&lt;span class="c1"&gt;# Example&lt;/span&gt;
scp -i ~/.ssh/.quang ubuntu@ip:/home/ubuntu/ .
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="exit-the-ssh-session-in-bashshell-script"&gt;Exit the ssh session in bash/shell script&lt;/h2&gt;
&lt;p&gt;Normally, we can use exit statement in terminal prompt to terminate the ssh session. However, this way doesn&amp;rsquo;t work properly when we perform exit in bash script file.sh.&lt;/p&gt;
&lt;p&gt;Solution&lt;/p&gt;
&lt;p&gt;In order to execute the bash script file.sh and terminate the ssh session later: Just put exit outside the script file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ bash &lt;span class="sb"&gt;`&lt;/span&gt;file.sh&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="fail-to-load-the-interpreter-of-python-for-virtual-environment"&gt;Fail to load the interpreter of Python for virtual environment&lt;/h2&gt;
&lt;p&gt;Instead of running python from virtual environment, it calls python of base environment. We can check which version of python is running by using:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;source&lt;/span&gt; activate VIRTUAL_ENV
$ which python
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Solution&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t need to run source activate &lt;code&gt;VIRTUAL_ENV&lt;/code&gt;, call directly &lt;code&gt;python&lt;/code&gt; from that &lt;code&gt;VIRTUAL_ENV&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ /path/to/anaconda/envs/VIRTUAL_ENV/bin/python filename.py
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="zip-and-unzip-files-and-folders"&gt;zip and unzip files and folders.&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;zip -r myfiles.zip mydir
&lt;/pre&gt;&lt;/div&gt;</content><category term="Bash"></category><category term="TUTORIAL"></category></entry><entry><title>Check Result of Face Detection</title><link href="/blog/tutorial/2018/check-result-of-face-detection/" rel="alternate"></link><published>2018-08-17T11:54:56+00:00</published><updated>2018-08-17T11:54:56+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-17:/blog/tutorial/2018/check-result-of-face-detection/</id><summary type="html">&lt;style type="text/css"&gt;/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future …&lt;/style&gt;</summary><content type="html">&lt;style type="text/css"&gt;/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell &gt; div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area &gt; div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area &gt; div.highlight &gt; pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the &lt;head&gt; if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev &lt;Maniac@SoftwareManiacs.Org&gt;
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph &gt; img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell &gt; div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
&lt;/style&gt;
&lt;style type="text/css"&gt;.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */&lt;/style&gt;
&lt;style type="text/css"&gt;
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
&lt;/style&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Import-packages"&gt;Import packages&lt;a class="anchor-link" href="#Import-packages"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[12]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;PIL&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ImageDraw&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os.path&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;osp&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;argparse&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Get-home_path"&gt;Get home_path&lt;a class="anchor-link" href="#Get-home_path"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We get &lt;code&gt;home_path&lt;/code&gt; on different machines&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[5]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;home_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'~'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;home_path&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[5]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;'/Users/quanguet'&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Compute-the-IoU"&gt;Compute the IoU&lt;a class="anchor-link" href="#Compute-the-IoU"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We define the function to compute the IoU of a predicted box with a list of ground truth boxes.&lt;/p&gt;
&lt;p&gt;A box is represented by a list &lt;code&gt;[xmin, ymin, xmax, ymax]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A list ground truth boxes is a list of boxes &lt;code&gt;[box1, box2, box3]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;All are &lt;code&gt;np.array&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[7]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_iou&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_img_box&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gt_img_boxes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Compute the area of all boxes including the predicted box&lt;/span&gt;
    &lt;span class="n"&gt;pred_box_area&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_img_box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;pred_img_box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# area = w&lt;/span&gt;
    &lt;span class="n"&gt;pred_box_area&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_img_box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;pred_img_box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# area = w * h&lt;/span&gt;
    &lt;span class="n"&gt;gt_boxes_area&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gt_img_boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;gt_img_boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# area = w&lt;/span&gt;
    &lt;span class="n"&gt;gt_boxes_area&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gt_img_boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;gt_img_boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# area = w * h&lt;/span&gt;

    &lt;span class="n"&gt;xx1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_img_box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;gt_img_boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;yy1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_img_box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;gt_img_boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;xx2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_img_box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;gt_img_boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;yy2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_img_box&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;gt_img_boxes&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="c1"&gt;# Compute the area of intersection&lt;/span&gt;
    &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xx2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;xx1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yy2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;yy1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;inter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;
    &lt;span class="c1"&gt;# get IoU all boxes with the box of highest conf&lt;/span&gt;
    &lt;span class="n"&gt;iou&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inter&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_box_area&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;gt_boxes_area&lt;/span&gt;&lt;span class="p"&gt;[:]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;inter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;iou&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Check-whether-a-predict-box-is-a-ground-truth"&gt;Check whether a predict box is a ground truth&lt;a class="anchor-link" href="#Check-whether-a-predict-box-is-a-ground-truth"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[8]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pred_box_is_gt_box&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_img_box&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gt_img_boxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;iou&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_iou&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred_img_box&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gt_img_boxes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iou&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Get-the-prediction-from-a-file"&gt;Get the prediction from a file&lt;a class="anchor-link" href="#Get-the-prediction-from-a-file"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[15]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# args = argparse.ArgumentParser().parse_args()&lt;/span&gt;

&lt;span class="n"&gt;result_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Dropbox/DeepFPNResnet/Resnet101/afw_Deepresnet101_val.txt'&lt;/span&gt;
&lt;span class="n"&gt;data_root&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Datasets/AFW'&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_pred_data&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# file_name score &lt;/span&gt;
    &lt;span class="n"&gt;boxes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;img_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;img_paths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;home_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;result_path&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;"r"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;readlines&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;elems&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;" "&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;img_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;elems&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;'.jpg'&lt;/span&gt;
        &lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;elems&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;box&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;elems&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
        &lt;span class="n"&gt;box&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;img_name&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;img_names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="n"&gt;img_names&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;img_paths&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;home_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data_root&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
            &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;box&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;final_boxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;final_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;img_name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;img_names&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;final_boxes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="n"&gt;final_scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;img_name&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;img_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img_paths&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;final_boxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;final_scores&lt;/span&gt;

&lt;span class="n"&gt;img_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img_paths&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;final_boxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;final_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_pred_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[17]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;img_names&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[17]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;['2353849.jpg',
 '2201628776.jpg',
 '406798473.jpg',
 '70037463.jpg',
 '2030653815.jpg']&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[18]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;img_paths&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[18]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;['/Users/quanguet/Datasets/AFW/2353849.jpg',
 '/Users/quanguet/Datasets/AFW/2201628776.jpg',
 '/Users/quanguet/Datasets/AFW/406798473.jpg',
 '/Users/quanguet/Datasets/AFW/70037463.jpg',
 '/Users/quanguet/Datasets/AFW/2030653815.jpg']&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[19]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;final_boxes&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[19]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;[array([[ 277. ,  478. ,  471. ,  791.3],
        [ 971.3,  444.7, 1115.3,  631.7],
        [ 678.7,  894.2,  783.6, 1030.6]]),
 array([[576. , 338.7, 755.3, 590.7],
        [730.1, 103. , 742.1, 119. ],
        [719.2, 132.6, 739.8, 156.6]]),
 array([[517.7, 370.7, 649.3, 545.7],
        [312. , 346.3, 446. , 513. ],
        [818.3, 375.3, 966.3, 572. ]]),
 array([[ 470. ,  193.3,  681.3,  517.7],
        [1011. ,  406.7, 1259.3,  733.3]]),
 array([[724.3, 435. , 914. , 681. ],
        [529. , 459.8, 636.2, 659.2]])]&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[20]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;final_scores&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[20]:&lt;/div&gt;
&lt;div class="output_text output_subarea output_execute_result"&gt;
&lt;pre&gt;[array([1.   , 1.   , 0.998]),
 array([1.   , 0.883, 0.637]),
 array([1., 1., 1.]),
 array([1., 1.]),
 array([1.   , 0.496])]&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="TUTORIAL"></category></entry><entry><title>Maxout ICML 2013</title><link href="/blog/paper/2018/maxout-icml-2013/" rel="alternate"></link><published>2018-08-15T22:48:34+00:00</published><updated>2018-08-15T22:48:34+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-15:/blog/paper/2018/maxout-icml-2013/</id><summary type="html">&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Link to paper, which appears on ICML 2013.&lt;/li&gt;
&lt;li&gt;Maxout network (maxout - its output is the max of a set of inputs)&lt;/li&gt;
&lt;li&gt;Design to both facilitate optimization by dropout and improve the accuracy.&lt;/li&gt;
&lt;li&gt;The SOTA results on MNIST, CIFAR-10, CIFAR-100, SVHN.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dropout - an inexpensive and simple means for:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;training a large ensemble of models that share params.&lt;/li&gt;
&lt;li&gt;approximately averaging together these models&amp;rsquo; predictions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Dropout is applied in MLP, CNN and get the SOTA.&lt;/p&gt;
&lt;p&gt;This paper argues that: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The best  performance may be obtained by directly designing a model that enhances dropout&amp;rsquo;s abilities as a model averaging technique.&lt;/li&gt;
&lt;li&gt;They introduce &lt;code&gt;maxout&lt;/code&gt; layer with that purpose.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="review-of-dropout"&gt;Review of dropout&lt;/h2&gt;
&lt;p&gt;Dropout is a technique applied in deterministic feedforward networks: predict output &lt;code&gt;y&lt;/code&gt; given vector &lt;code&gt;x&lt;/code&gt; with a series of hidden layers &lt;span class="math"&gt;\(\textbf{h} = \{h^{(1)},...,h^{(L)}\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Dropout trains an ensemble of models consisting of the set of all models that …&lt;/p&gt;</summary><content type="html">&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Link to paper, which appears on ICML 2013.&lt;/li&gt;
&lt;li&gt;Maxout network (maxout - its output is the max of a set of inputs)&lt;/li&gt;
&lt;li&gt;Design to both facilitate optimization by dropout and improve the accuracy.&lt;/li&gt;
&lt;li&gt;The SOTA results on MNIST, CIFAR-10, CIFAR-100, SVHN.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dropout - an inexpensive and simple means for:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;training a large ensemble of models that share params.&lt;/li&gt;
&lt;li&gt;approximately averaging together these models&amp;rsquo; predictions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Dropout is applied in MLP, CNN and get the SOTA.&lt;/p&gt;
&lt;p&gt;This paper argues that: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The best  performance may be obtained by directly designing a model that enhances dropout&amp;rsquo;s abilities as a model averaging technique.&lt;/li&gt;
&lt;li&gt;They introduce &lt;code&gt;maxout&lt;/code&gt; layer with that purpose.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="review-of-dropout"&gt;Review of dropout&lt;/h2&gt;
&lt;p&gt;Dropout is a technique applied in deterministic feedforward networks: predict output &lt;code&gt;y&lt;/code&gt; given vector &lt;code&gt;x&lt;/code&gt; with a series of hidden layers &lt;span class="math"&gt;\(\textbf{h} = \{h^{(1)},...,h^{(L)}\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Dropout trains an ensemble of models consisting of the set of all models that contain a subset of the variables in both &lt;code&gt;v&lt;/code&gt; and h. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The same set of parameters &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is used to parameterize a family of distributions &lt;span class="math"&gt;\(p(y|v; \theta, \mu)\)&lt;/span&gt; where &lt;span class="math"&gt;\(\mu \in M\)&lt;/span&gt; is a binary mask determining which variables to include in the model.&lt;/li&gt;
&lt;li&gt;On each presentation of a training example, we train a different sub-model by following the gradient of &lt;span class="math"&gt;\(logp(y|v;\theta, \mu)\)&lt;/span&gt; for a different randomly sampled &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;For many parameterization of &lt;span class="math"&gt;\(p\)&lt;/span&gt;, the initiation of different sub-models &lt;span class="math"&gt;\(p(y|v; \theta, \mu)\)&lt;/span&gt; can be obtained by elem-wise multiplication of v and h with the mask &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dropout and bagging:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Similar: Dropout training is similar to bagging where many different models are trained on different subsets of the data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Different: In dropout &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Each model is trained for only 1 step&lt;/li&gt;
&lt;li&gt;All the models share parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This training procedure behave as if: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is training an ensemble rather than a single model (an ensemble with bagging under &lt;code&gt;parameter sharing constraint&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Each update must have a large effect (unlike SGD makes a steady progress with small steps), so that it makes the sub-model induced by that &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; fit the current input v well.&lt;/li&gt;
&lt;li&gt;Each update step is considered as an update to different model on a different subset of training set.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In prediction phase&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bagging averages with the arithmetic mean &amp;larr; impossible with exponentially many models using dropout.&lt;/li&gt;
&lt;li&gt;Fortunately, some model families (such as deep non-linear network) yield an inexpensive geometric mean.&lt;/li&gt;
&lt;li&gt;If &lt;span class="math"&gt;\(p(y|v;\theta) = softmax(v^TW + b)\)&lt;/span&gt;, then the geometric mean of &lt;span class="math"&gt;\(p(y|v; \theta, \mu)\)&lt;/span&gt; where &lt;span class="math"&gt;\(\mu \in M\)&lt;/span&gt; over &lt;span class="math"&gt;\(M\)&lt;/span&gt; is simply &lt;span class="math"&gt;\(softmax(v^TW/2 + b)\)&lt;/span&gt;. &lt;/li&gt;
&lt;li&gt;The average prediction of exponentially many sub-models can be computed simply by running the full model with the weights divided by 2.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="description-of-maxout"&gt;Description of Maxout&lt;/h2&gt;
&lt;p&gt;The maxout model is simply uses a new type of activation function: the maxout unit.&lt;/p&gt;
&lt;p&gt;Given the input &lt;span class="math"&gt;\(x \in R^d\)&lt;/span&gt; (x may be a vector v, or may be a hidden layer&amp;rsquo;s state), a maxout hidden layer implements the function:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(h_i(x) = max_{j \in [1,k]}z_{i,j}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(z_{ij} = x^TW_{..ij} + b_{ij}\)&lt;/span&gt; and &lt;span class="math"&gt;\(W \in R^{d \times m\times k}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\in R^{m \times k}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In CNN, a maxout feature map can be constructed by taking the maximum across k affine feature maps (pool across channels). &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When training with dropout, we perform the elem-wise multiplication with the dropout mask immediately before the multiplication by the weights in all cases.&lt;/li&gt;
&lt;li&gt;A single maxout unit can be interpreted as making a piecewise linear approx to an arbitrary convex function (see Fig.1).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" src="https://d2mxuefqeaa7sj.cloudfront.net/s_181B4C9A1CF08268454C195FA358E6ED2E4EEEFD261BA3D2C40C18FCCF458838_1531408560500_image.png"/&gt;&lt;/p&gt;
&lt;p&gt;Characteristics of maxout:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The representation it produces is not sparse though the gradient is highly sparse and dropout will artificially sparsify the effective representation during training.&lt;/li&gt;
&lt;li&gt;maxout is locally linear almost everywhere while the other activation functions have significant cuvature.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&amp;rarr; They are very robust and easy to train with dropout.&lt;/p&gt;
&lt;h2 id="maxout-is-a-universal-approximator"&gt;Maxout is a universal approximator&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A standard MLP with enough hidden units is a universal approximator. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="" src="https://d2mxuefqeaa7sj.cloudfront.net/s_181B4C9A1CF08268454C195FA358E6ED2E4EEEFD261BA3D2C40C18FCCF458838_1531410910906_image.png"/&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;maxout networks are universal approximators as well.
&lt;img alt="" src="https://d2mxuefqeaa7sj.cloudfront.net/s_181B4C9A1CF08268454C195FA358E6ED2E4EEEFD261BA3D2C40C18FCCF458838_1531411300798_image.png"/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example, they show that a  maxout model with just 2 hidden units can approx any continous function of &lt;span class="math"&gt;\(v \in R^n\)&lt;/span&gt; if each maxout unit may have many affine components.&lt;/p&gt;
&lt;h2 id="experimental-results"&gt;Experimental Results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MNIST&lt;/li&gt;
&lt;li&gt;CIFAR-10&lt;/li&gt;
&lt;li&gt;CIFAR-100&lt;/li&gt;
&lt;li&gt;SVHN&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="model-averaging"&gt;Model Averaging&lt;/h2&gt;
&lt;h2 id="notes"&gt;Notes&lt;/h2&gt;
&lt;p&gt;Deterministic algorithm is an algorithm which, given a particular input, will always produce the same output, with the underlying machine always passing through the same sequence of states.&lt;/p&gt;
&lt;p&gt;Deterministic model: model x&amp;aacute;c định (tất định).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.simon-hohberg.de/"&gt;Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs231n.github.io/neural-networks-1/"&gt;Explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Duncanswilson/maxout-pytorch/blob/master/maxout_pytorch.ipynb"&gt;Code in Pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "0em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Maxout"></category><category term="ICML"></category><category term="Dropout"></category><category term="2013"></category><category term="PAPER"></category></entry><entry><title>How to use models in slim</title><link href="/blog/tutorial/2018/how-to-use-models-in-slim/" rel="alternate"></link><published>2018-08-15T22:13:53+00:00</published><updated>2018-08-15T22:13:53+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-15:/blog/tutorial/2018/how-to-use-models-in-slim/</id><summary type="html">&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This guide to use &lt;code&gt;slim&lt;/code&gt; for Image Classification and Image Annotation and Segmentation. Find this tutorial here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;slim&lt;/code&gt; library was released with a set of standard models like ResNet-v1, VGG, Inception-Resnet-v2, Resnet-v2, Inception and so on.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The pretrained models are supported by Google &amp;rarr; much better. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;slim&lt;/code&gt; is very clean and lightweight wrapper around Tensorflow. &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="setup"&gt;Setup&lt;/h1&gt;
&lt;h2 id="clone-the-slim"&gt;Clone the slim&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/tensorflow/models
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="add-the-path-to-the-slim-library-in-order-to-use-datasets-or-some-modules"&gt;Add the path to the slim library in order to use &lt;code&gt;datasets&lt;/code&gt; or some modules.&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os.path&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;osp&lt;/span&gt;
&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'path/to/models/research/slim/'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="download-the-models_1"&gt;Download the models&lt;/h1&gt;
&lt;p&gt;Download the pretrained models from here. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;dataset_utils&lt;/span&gt;
&lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz"&lt;/span&gt;

&lt;span class="n"&gt;checkpoints_dir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'/content/pretrained_models/'&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoints_dir&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;makedirs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoint_dir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dataset_utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download_and_uncompress_tarball&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoints_dir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="preprocess-the-image"&gt;Preprocess the image&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Subtract image by mean&lt;/li&gt;
&lt;li&gt;expand 1 image into a batch …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This guide to use &lt;code&gt;slim&lt;/code&gt; for Image Classification and Image Annotation and Segmentation. Find this tutorial here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;slim&lt;/code&gt; library was released with a set of standard models like ResNet-v1, VGG, Inception-Resnet-v2, Resnet-v2, Inception and so on.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The pretrained models are supported by Google &amp;rarr; much better. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;slim&lt;/code&gt; is very clean and lightweight wrapper around Tensorflow. &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="setup"&gt;Setup&lt;/h1&gt;
&lt;h2 id="clone-the-slim"&gt;Clone the slim&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/tensorflow/models
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="add-the-path-to-the-slim-library-in-order-to-use-datasets-or-some-modules"&gt;Add the path to the slim library in order to use &lt;code&gt;datasets&lt;/code&gt; or some modules.&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os.path&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;osp&lt;/span&gt;
&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'path/to/models/research/slim/'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="download-the-models_1"&gt;Download the models&lt;/h1&gt;
&lt;p&gt;Download the pretrained models from here. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;dataset_utils&lt;/span&gt;
&lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz"&lt;/span&gt;

&lt;span class="n"&gt;checkpoints_dir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'/content/pretrained_models/'&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoints_dir&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;makedirs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoint_dir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dataset_utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download_and_uncompress_tarball&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoints_dir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="preprocess-the-image"&gt;Preprocess the image&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Subtract image by mean&lt;/li&gt;
&lt;li&gt;expand 1 image into a batch of 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

&lt;span class="n"&gt;image_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'url/image.jpg'&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;320&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;320&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;offline&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;offline&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;img_raw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_raw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;offline&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;WIDTH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HEIGHT&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;_R_MEAN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_G_MEAN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_B_MEAN&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="define-the-model-and-get-the-prediction"&gt;Define the model and get the prediction&lt;/h1&gt;
&lt;h2 id="create-the-graph"&gt;Create the graph&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;HOME_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"~/"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;IMG_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;HOME_PATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Pictures/Hien.jpg'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;RESNET_50_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;HOME_PATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Result/pretrained-Resnet-Tensorflow/resnet_v1_50.ckpt'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resnet_v1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet_arg_scope&lt;/span&gt;&lt;span class="p"&gt;()):&lt;/span&gt;
    &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end_points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;resnet_v1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet_v1_50&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;global_pool&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;is_training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="load-the-pretrained-weights"&gt;Load the pretrained weights&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;saver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Saver&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RESNET_50_PATH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="make-a-prediction"&gt;Make a prediction&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'Placeholder:0'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="n"&gt;sorted_indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                    &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sorted_indices&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="decode-the-name-of-the-labels-in-1000-classes"&gt;Decode the name of the labels in 1000 classes&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Decode the labels name&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;decode_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sorted_idx&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imagenet&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create_readable_names_for_imagenet_labels&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sorted_idx&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Probability &lt;/span&gt;&lt;span class="si"&gt;%0.2f&lt;/span&gt;&lt;span class="s1"&gt; =&amp;gt; [&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;]'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="test-with-an-example-image_1"&gt;Test with an example image&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'/Users/quanguet/Pictures/cat.jpg'&lt;/span&gt;
&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;offline&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'True'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;decode_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="all-in-one-program"&gt;All in one program&lt;/h1&gt;
&lt;p&gt;The full code is below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os.path&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;osp&lt;/span&gt;
&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'path/to/models/research/slim/'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;dataset_utils&lt;/span&gt;
&lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz"&lt;/span&gt;

&lt;span class="c1"&gt;# Download the model&lt;/span&gt;
&lt;span class="n"&gt;checkpoints_dir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'/content/pretrained_models/'&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoints_dir&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;makedirs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;checkpoint_dir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dataset_utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download_and_uncompress_tarball&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;checkpoints_dir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Preprocess the images&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

&lt;span class="n"&gt;image_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'url/image.jpg'&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;320&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;320&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;offline&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;offline&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;img_raw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_raw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;offline&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resize&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;WIDTH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HEIGHT&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;_R_MEAN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_G_MEAN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_B_MEAN&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;

&lt;span class="c1"&gt;# Create the graph&lt;/span&gt;
&lt;span class="n"&gt;HOME_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"~/"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;IMG_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;HOME_PATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Pictures/Hien.jpg'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;RESNET_50_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;osp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;HOME_PATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'Result/pretrained-Resnet-Tensorflow/resnet_v1_50.ckpt'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resnet_v1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet_arg_scope&lt;/span&gt;&lt;span class="p"&gt;()):&lt;/span&gt;
    &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end_points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;resnet_v1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resnet_v1_50&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;global_pool&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;is_training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Load the pretrained weights&lt;/span&gt;
&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;saver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Saver&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;saver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RESNET_50_PATH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Make a prediction&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'Placeholder:0'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="n"&gt;sorted_indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                    &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sorted_indices&lt;/span&gt;


&lt;span class="c1"&gt;# Decode the labels name&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;decode_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sorted_idx&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imagenet&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create_readable_names_for_imagenet_labels&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sorted_idx&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Probability &lt;/span&gt;&lt;span class="si"&gt;%0.2f&lt;/span&gt;&lt;span class="s1"&gt; =&amp;gt; [&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;]'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Tensorflow"></category><category term="Slim"></category><category term="Fine-tuning"></category><category term="TUTORIAL"></category></entry><entry><title>Alexnet NIPS 2012</title><link href="/blog/paper/2018/alexnet-nips-2012/" rel="alternate"></link><published>2018-08-15T18:06:40+00:00</published><updated>2018-08-15T18:06:40+00:00</updated><author><name>Quang Nguyen</name></author><id>tag:None,2018-08-15:/blog/paper/2018/alexnet-nips-2012/</id><summary type="html">&lt;h1 id="abstract"&gt;Abstract&lt;/h1&gt;
&lt;hr/&gt;
&lt;ul&gt;
&lt;li&gt;Published on NIPS 2012. &lt;/li&gt;
&lt;li&gt;Pdf version can be found in dropbox here. &lt;/li&gt;
&lt;li&gt;SOTA in ImageNet 2012 with top-5 error rate 15.3%.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;dropout&lt;/code&gt;, &lt;code&gt;relu&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li&gt;CNN: &lt;code&gt;conv&lt;/code&gt; layers have much fewer connections &amp;amp; params, and so easier to train.&lt;/li&gt;
&lt;li&gt;Dataset: Imagenet contains enough labeled examples &amp;rarr; can train without severe overfitting.&lt;/li&gt;
&lt;li&gt;Train Alexnet takes 5-6 days with 2 GPUs 3GB. 
Dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;hr/&gt;
&lt;ul&gt;
&lt;li&gt;ImageNet 1.2 million high-resolution images with 1000 classes.&lt;/li&gt;
&lt;li&gt;Two error rates:&lt;/li&gt;
&lt;li&gt;Top-5 error rate&lt;/li&gt;
&lt;li&gt;Top-1 error rate&lt;/li&gt;
&lt;li&gt;The size of the images to be trained: smaller size &amp;rarr; 256, then crop the center patch: 256x256.
Architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;ul&gt;
&lt;li&gt;60 million params with 650,000 neurons.&lt;/li&gt;
&lt;li&gt;5 &lt;code&gt;conv&lt;/code&gt; layers, 3 &lt;code&gt;fc&lt;/code&gt; layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Error rate w.r.t epochs on CIFAR10. The dash line is of tanh while the solid line is of relu." src="https://d2mxuefqeaa7sj.cloudfront.net/s_6EF601BE07E17A98BF97AB239E543B35D29C3B4B9D1871E11707DB6D0C2C3533_1533070511862_image.png"/&gt;&lt;/p&gt;
&lt;p&gt;Relu&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;non-saturating nonlinearity &amp;rarr; converge faster&lt;/li&gt;
&lt;li&gt;easy to train - no exponential computations.&lt;/li&gt;
&lt;li&gt;one of the best choice of activation function.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Local Reponse Normalization&lt;/p&gt;
&lt;h1 id="specific-notes"&gt;Specific notes&lt;/h1&gt;
&lt;hr/&gt;
&lt;h1 id="tldr-code"&gt;TLDR: Code&lt;/h1&gt;
&lt;p&gt;Below are two version implemented in Pytorch and Tensorflow.&lt;/p&gt;
&lt;h2 id="the-pytorch-code-of-alexnet"&gt;The Pytorch code of AlexNet …&lt;/h2&gt;</summary><content type="html">&lt;h1 id="abstract"&gt;Abstract&lt;/h1&gt;
&lt;hr/&gt;
&lt;ul&gt;
&lt;li&gt;Published on NIPS 2012. &lt;/li&gt;
&lt;li&gt;Pdf version can be found in dropbox here. &lt;/li&gt;
&lt;li&gt;SOTA in ImageNet 2012 with top-5 error rate 15.3%.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;dropout&lt;/code&gt;, &lt;code&gt;relu&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li&gt;CNN: &lt;code&gt;conv&lt;/code&gt; layers have much fewer connections &amp;amp; params, and so easier to train.&lt;/li&gt;
&lt;li&gt;Dataset: Imagenet contains enough labeled examples &amp;rarr; can train without severe overfitting.&lt;/li&gt;
&lt;li&gt;Train Alexnet takes 5-6 days with 2 GPUs 3GB. 
Dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;hr/&gt;
&lt;ul&gt;
&lt;li&gt;ImageNet 1.2 million high-resolution images with 1000 classes.&lt;/li&gt;
&lt;li&gt;Two error rates:&lt;/li&gt;
&lt;li&gt;Top-5 error rate&lt;/li&gt;
&lt;li&gt;Top-1 error rate&lt;/li&gt;
&lt;li&gt;The size of the images to be trained: smaller size &amp;rarr; 256, then crop the center patch: 256x256.
Architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;ul&gt;
&lt;li&gt;60 million params with 650,000 neurons.&lt;/li&gt;
&lt;li&gt;5 &lt;code&gt;conv&lt;/code&gt; layers, 3 &lt;code&gt;fc&lt;/code&gt; layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Error rate w.r.t epochs on CIFAR10. The dash line is of tanh while the solid line is of relu." src="https://d2mxuefqeaa7sj.cloudfront.net/s_6EF601BE07E17A98BF97AB239E543B35D29C3B4B9D1871E11707DB6D0C2C3533_1533070511862_image.png"/&gt;&lt;/p&gt;
&lt;p&gt;Relu&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;non-saturating nonlinearity &amp;rarr; converge faster&lt;/li&gt;
&lt;li&gt;easy to train - no exponential computations.&lt;/li&gt;
&lt;li&gt;one of the best choice of activation function.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Local Reponse Normalization&lt;/p&gt;
&lt;h1 id="specific-notes"&gt;Specific notes&lt;/h1&gt;
&lt;hr/&gt;
&lt;h1 id="tldr-code"&gt;TLDR: Code&lt;/h1&gt;
&lt;p&gt;Below are two version implemented in Pytorch and Tensorflow.&lt;/p&gt;
&lt;h2 id="the-pytorch-code-of-alexnet"&gt;The Pytorch code of AlexNet.&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The dropout_rate = 0.5, it&amp;rsquo;s the default value in Pytorch implementation of &lt;code&gt;dropout&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kr"&gt;class&lt;/span&gt; &lt;span class="nx"&gt;AlexNet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="kr"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;AlexNet&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;192&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;192&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;384&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;384&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;nn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="tensorflow-implementation-with-slim"&gt;Tensorflow Implementation with slim&lt;/h2&gt;
&lt;div class="alert alert-success" role="alert"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;All the fully connected layers have been transformed to conv2d layers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To use in classification mode, resize input to &lt;code&gt;224x224&lt;/code&gt; or set &lt;code&gt;global_pool = True&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LRN layers have been removed &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change the initializers from &lt;code&gt;random_normal_initializer&lt;/code&gt; to &lt;code&gt;xavier_initializer&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;

&lt;span class="n"&gt;slim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contrib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;slim&lt;/span&gt;
&lt;span class="n"&gt;trunc_normal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal_initializer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;alexnet_v2_arg_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight_decay&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0005&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg_scope&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                        &lt;span class="n"&gt;activation_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;biases_initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant_initializer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                        &lt;span class="n"&gt;weights_regularizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;l2_regularizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight_decay&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg_scope&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'SAME'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg_scope&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'VALID'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;arg_sc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;arg_sc&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;alexnet_v2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;is_training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;dropout_keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;spatial_squeeze&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'alexnet_v2'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;global_pool&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""AlexNet version 2.&lt;/span&gt;
&lt;span class="sd"&gt;    Described in: http://arxiv.org/pdf/1404.5997v2.pdf&lt;/span&gt;
&lt;span class="sd"&gt;    Parameters from:&lt;/span&gt;
&lt;span class="sd"&gt;    github.com/akrizhevsky/cuda-convnet2/blob/master/layers/&lt;/span&gt;
&lt;span class="sd"&gt;    layers-imagenet-1gpu.cfg&lt;/span&gt;
&lt;span class="sd"&gt;    Note: All the fully_connected layers have been transformed to conv2d layers.&lt;/span&gt;
&lt;span class="sd"&gt;        To use in classification mode, resize input to 224x224 or set&lt;/span&gt;
&lt;span class="sd"&gt;        global_pool=True. To use in fully convolutional mode, set&lt;/span&gt;
&lt;span class="sd"&gt;        spatial_squeeze to false.&lt;/span&gt;
&lt;span class="sd"&gt;        The LRN layers have been removed and change the initializers from&lt;/span&gt;
&lt;span class="sd"&gt;        random_normal_initializer to xavier_initializer.&lt;/span&gt;
&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;    inputs: a tensor of size [batch_size, height, width, channels].&lt;/span&gt;
&lt;span class="sd"&gt;    num_classes: the number of predicted classes. If 0 or None, the logits layer&lt;/span&gt;
&lt;span class="sd"&gt;    is omitted and the input features to the logits layer are returned instead.&lt;/span&gt;
&lt;span class="sd"&gt;    is_training: whether or not the model is being trained.&lt;/span&gt;
&lt;span class="sd"&gt;    dropout_keep_prob: the probability that activations are kept in the dropout&lt;/span&gt;
&lt;span class="sd"&gt;        layers during training.&lt;/span&gt;
&lt;span class="sd"&gt;    spatial_squeeze: whether or not should squeeze the spatial dimensions of the&lt;/span&gt;
&lt;span class="sd"&gt;        logits. Useful to remove unnecessary dimensions for classification.&lt;/span&gt;
&lt;span class="sd"&gt;    scope: Optional scope for the variables.&lt;/span&gt;
&lt;span class="sd"&gt;    global_pool: Optional boolean flag. If True, the input to the classification&lt;/span&gt;
&lt;span class="sd"&gt;        layer is avgpooled to size 1x1, for any input size. (This is not part&lt;/span&gt;
&lt;span class="sd"&gt;        of the original AlexNet.)&lt;/span&gt;
&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;    net: the output of the logits layer (if num_classes is a non-zero integer),&lt;/span&gt;
&lt;span class="sd"&gt;        or the non-dropped-out input to the logits layer (if num_classes is 0&lt;/span&gt;
&lt;span class="sd"&gt;        or None).&lt;/span&gt;
&lt;span class="sd"&gt;    end_points: a dict of tensors with intermediate activations.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variable_scope&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'alexnet_v2'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;end_points_collection&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_name_scope&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;'_end_points'&lt;/span&gt;
    &lt;span class="c1"&gt;# Collect outputs for conv2d, fully_connected and max_pool2d.&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg_scope&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fully_connected&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                        &lt;span class="n"&gt;outputs_collections&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;end_points_collection&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'VALID'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'conv1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'pool1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;192&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'conv2'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'pool2'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;384&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'conv3'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;384&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'conv4'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'conv5'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'pool5'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Use conv2d instead of fully_connected layers.&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg_scope&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                            &lt;span class="n"&gt;weights_initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;trunc_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.005&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                            &lt;span class="n"&gt;biases_initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant_initializer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'VALID'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'fc6'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_keep_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;is_training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;is_training&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dropout6'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'fc7'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Convert end_points_collection into a end_point dict.&lt;/span&gt;
        &lt;span class="n"&gt;end_points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_collection_to_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;end_points_collection&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;global_pool&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;keep_dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'global_pool'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;end_points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'global_pool'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_keep_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;is_training&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;is_training&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'dropout7'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                            &lt;span class="n"&gt;activation_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;normalizer_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;biases_initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_initializer&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
                            &lt;span class="n"&gt;scope&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'fc8'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;spatial_squeeze&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'fc8/squeezed'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;end_points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;'/fc8'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end_points&lt;/span&gt;
&lt;span class="n"&gt;alexnet_v2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;default_image_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;224&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Image Recognition"></category><category term="Alexnet"></category><category term="CNN"></category><category term="ImageNet"></category><category term="2012"></category><category term="PAPER"></category></entry></feed>