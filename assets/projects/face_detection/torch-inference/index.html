<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Do inference from the trained model - Face Detection Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Do inference from the trained model";
    var mkdocs_page_input_path = "torch-inference.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Face Detection Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <span class="caption-text">Pytorch Object Detection</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../torch-getting-started/">Getting started</a>
                </li>
                <li class="">
                    
    <a class="" href="../torch-environment-setup/">Environment setup</a>
                </li>
                <li class="">
                    
    <a class="" href="../torch-prepare-dataset/">Prepare the dataset</a>
                </li>
                <li class="">
                    
    <a class="" href="../torch-encoding/">How to encode the grouth truth boxes</a>
                </li>
                <li class="">
                    
    <a class="" href="../torch-build-model/">Build some popular model</a>
                </li>
                <li class="">
                    
    <a class="" href="../torch-extract-model.md">Extract the smaller model from pretrained model</a>
                </li>
                <li class="">
                    
    <a class="" href="../torch-train-model/">Train the model</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Do inference from the trained model</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#inference-for-object-detection">Inference for object detection</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#1-build-and-load-the-model">1. Build and load the model</a></li>
        
            <li><a class="toctree-l4" href="#2-create-a-boxcoder-for-decoding-the-detection-results">2. Create a BoxCoder for decoding the detection results.</a></li>
        
            <li><a class="toctree-l4" href="#3-load-the-image-and-make-detections">3. Load the image and make detections</a></li>
        
            <li><a class="toctree-l4" href="#4-draw-the-detected-bounding-boxes-and-scores-and-show-the-result">4. Draw the detected bounding boxes and scores and show the result.</a></li>
        
            <li><a class="toctree-l4" href="#other-detect-methods">Other detect methods</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../torch-truncated-model/">Truncate the first detection layers from models</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Face Detection Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Pytorch Object Detection &raquo;</li>
        
      
    
    <li>Do inference from the trained model</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="inference-for-object-detection">Inference for object detection</h1>
<h3 id="1-build-and-load-the-model">1. Build and load the model</h3>
<pre><code class="python">import torch
import torch_object_detection

# Build the model
net = torch_object_detection.models.FaceNet(num_classes=2, 
                                            base=torch_object_detection.models.Base101())
# Load the trained weights
net.load_state_dict(torch.load('path/to/trained_model.pkl'))                                                                              
</code></pre>

<h3 id="2-create-a-boxcoder-for-decoding-the-detection-results">2. Create a <code>BoxCoder</code> for decoding the detection results.</h3>
<ul>
<li>
<p>If we make detections on in-variant size images (The image size is the same for all images), it
 is strongly recommended to define the <code>BoxCoder</code> for decoding the detection results. It is 
 because we only need to define a <code>BoxCoder</code> once and use it many times (which saves a lot of 
 time when creating a <code>BoxCoder</code>).</p>
</li>
<li>
<p>If the input images' size is variant, we can skip creating a <code>BoxCoder</code>. Insider, <code>detect</code> 
method, it already supports creating a <code>BoxCoder</code> by itself. However, this will make detection 
time longer.</p>
</li>
</ul>
<p>In the first case, we need to provide values to three arguments:</p>
<ul>
<li>
<p><code>fm_sizes</code>: The feature map sizes of each detection layer obtained when we pass an image of 
size (Height, Width). <strong>These values can be different from the training procedure.</strong></p>
</li>
<li>
<p><code>aspect_ratios</code>: The aspect ratios of the anchors at each detection layer. <strong>These values must be
 the same as the configuration setting during training.</strong></p>
</li>
<li>
<p><code>anchor_sizes</code>: The referenced size of anchors at each detection layer. <strong>These values must be
 the same as the configuration setting during training.</strong></p>
</li>
</ul>
<p>So, basically, we only need to determine the <code>fm_sizes</code> for different input image size (Height, 
Width). In case you don't know exactly how to compute it, please use the trick code below:</p>
<pre><code class="python"># The simulated tensor - the same size as our input image.
input_tensor = torch.randn(1, 3, Height, Width)
input_tensor = input_tensor.to(DEVIDE)
loc_dets, cls_dets, fm_sizes = net(input_tensor, fm_sizes_return=True)
print(fm_sizes)
</code></pre>

<pre><code class="python">fm_sizes = [[160, 160],
            [80, 80],
            [40, 40],
            [20, 20],
            [10, 10],
            [5, 5]]
# The referenced size of anchors at each detection layer
anchor_sizes = (16, 32, 64, 128, 256, 512),

# The aspect ratios of the anchors at each detection layer
aspect_ratios = [[1, ], [1, ], [1, ], [1, ], [1, ], [1, ]]

box_coder = BoxCoder(img_size=(HEIGHT, WIDTH),
                     fm_sizes=fm_sizes,
                     anchor_sizes=anchor_sizes,
                     aspect_ratios=aspect_ratios)
</code></pre>

<h3 id="3-load-the-image-and-make-detections">3. Load the image and make detections</h3>
<pre><code class="python"># Load image and preprocess the image.
image = torch_object_detection.utils.load_image('path/to/img', height=640, width=640)
tensor = torch_object_detection.utils.preprocess_image(image)

# Make detections
# If you don't create the the box_coder, just leave `box_coder=None`. 
det_boxes, det_scores = torch_object_detection.utils.detect(tensor, net,
                                                           box_coder=box_coder,
                                                           device='cpu',
                                                           score_thresh=0.3,
                                                           nms_thresh=0.3)
</code></pre>

<h3 id="4-draw-the-detected-bounding-boxes-and-scores-and-show-the-result">4. Draw the detected bounding boxes and scores and show the result.</h3>
<pre><code class="python">torch_object_detection.utils.draw_detections(img, det_boxes, det_scores)

</code></pre>

<h3 id="other-detect-methods">Other detect methods</h3>
<p>For making detections of image with different scales..., I make several variants of <code>detect</code> 
formats. The source code can be found at <code>torch_object_detection/utils/detect_utils.py</code>:</p>
<ul>
<li><code>detect</code>: The method that we use above.</li>
<li><code>flip_detect</code>: The detection method for detecting the horizontal flipped of the input image.</li>
<li><code>resize_detect</code>: The detection method for detecting the image of different size. Resize the one 
of the side of image to specific value and keep the aspect ratio unchanged.</li>
<li><code>scale_detect</code>: The detection method for detecting the image of different scale. For example, if
 we have an image of size (300, 500), we can use this method to detect for scaled image of (600, 
 1000) if <code>scale = 2</code>.</li>
</ul>
<blockquote>
<p>All the detection methods except <code>detect</code> require the original image (PIL Image) as the input 
while <code>detect</code> uses <code>tensor</code> of the image as the input.</p>
</blockquote>
<p>More information and details of arguments for those methods, please visit source code.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../torch-truncated-model/" class="btn btn-neutral float-right" title="Truncate the first detection layers from models">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../torch-train-model/" class="btn btn-neutral" title="Train the model"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../torch-train-model/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../torch-truncated-model/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
