{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Object Detection Documentation For full documentation visit mkdocs.org . Install mkdocs $ pip install mkdocs Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#object-detection-documentation","text":"For full documentation visit mkdocs.org .","title":"Object Detection Documentation"},{"location":"#install-mkdocs","text":"$ pip install mkdocs","title":"Install mkdocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"torch-build-model/","text":"How to build a single-stage model for object detection and face detection In this part, I'm going to present how to build a single-stage object detector like SSD, SFD and so on. Contents The general architecture of a single-stage object detector Building SSD7: The simple example of object detector Building S3FD: Single scale-invariant face detector FaceNet: A single-stage face detector using ResNet backbone 1. The general architecture of a single-stage object detector The general architecture for a single-state (single-shot or one-stage or one-shot so on) object detector usually consists of: 1.1. The feature extractor First, we build a base network as a feature extractor. Second, we extract some layers in the feature extractor as the detection layers. Detection layer i (of size ( Mi x Ni x Ki )) is a set of Ki feature maps of size ( Mi x Ni ). Ki can be considered as the number of channels of this layer. The detection layer i divides the image into Mi x Ni spatially equal cell. Each cell may have 1 or more than 1 anchors. For more details about encoding, please visit How to encoding ground truth boxes and labels to anchors . In the example above, let say, we extract 3 detection layers from layers i , j , k . Later, we pass these layers to detection blocks to get the detection of total_num_anchors generated by our model. K = num_anchors_per_cells total_num_anchors = K[i]*(Mi*Ni) + K[j]*(Mj*Nj) + K[k]*(Mk*Nk) 1.2. The detection block The outputs of detection blocks will tell us whether a anchor belongs to a certain object class or not. If yes, then where is the detected bounding box by computing the offsets of coordinates of the detected boxes with this anchor. The detection block can be divided into 2 sub-blocks: classification block, and regression block. The detection block at each layer take the corresponding detection layer as the input and return the set of loc_dets and cls_dets . cls_dets is the set of detection classification scores for each class - the probability that the anchor belongs to the class (outputs of classification block). loc_dets is the set of detection locations of bounding boxes - the offset of coordinates of bounding boxes to their respective anchors (outputs of regression block). Classification block and regression block can consist of 1 direct conv layer or a stack of several conv layers. SSD7: The simple example of object detector SSD7 consists of: * Feature extractor is a stack of 8 conv layers. After each conv layer, the size of output is reduced by two using MaxPooling2D . Detection blocks Each detection block for each layer includes two sub-blocks: regression block and classification block. However, each sub-block is just 1 conv layer for simplicity as below: We define the SSD7 including its self.extractor , self.detection_blocks . The below code is simplified to make it succide. class DetectionBlock(nn.Module): def __init__(self): self.reg_conv self.cls_conv def forward(self, input): ... return loc_dets, cls_dets class SSD7(nn.Module): def __init__(self): self.extractor = ... # conv1 -> conv8 self.det_blocks = [DetectionBlock()] * num_detection_layers def forward(self, input): # perform feature extraction detection_layers = self.extractor(input) # perform detections final_loc_dets, final_cls_dets = [], [] for i, detection_layer in enumerate(detection_layers): loc_dets, cls_dets = self.det_blocks[i](detection_layers[i]) final_loc_dets.append(loc_dets) final_cls_dets.append(cls_dets) return final_loc_dets, final_cls_dets The details of implementation can be found at file models/ssd7.py . FaceNet: A single-stage face detector using ResNet backbone. Feature extractor ResNet backbone : The last conv layer of conv2 , conv3 , conv4 , conv5 blocks in ResNet are selected as the detection layers. Extra conv blocks Two more conv6 , conv7 blocks are added to ResNet backbone, each block includes 2 conv3x3. The last conv layers of conv6 and conv7 are selected as detection layers. Detection blocks The classification branch consists of 4 conv layers of 3x3 before the classification layer. The regression branch consists of 4 conv layers of 3x3 before the regression layer.","title":"Build some popular model"},{"location":"torch-build-model/#how-to-build-a-single-stage-model-for-object-detection-and-face-detection","text":"In this part, I'm going to present how to build a single-stage object detector like SSD, SFD and so on.","title":"How to build a single-stage model for object detection and face detection"},{"location":"torch-build-model/#contents","text":"The general architecture of a single-stage object detector Building SSD7: The simple example of object detector Building S3FD: Single scale-invariant face detector FaceNet: A single-stage face detector using ResNet backbone","title":"Contents"},{"location":"torch-build-model/#1-the-general-architecture-of-a-single-stage-object-detector","text":"The general architecture for a single-state (single-shot or one-stage or one-shot so on) object detector usually consists of:","title":"1. The general architecture of a single-stage object detector"},{"location":"torch-build-model/#11-the-feature-extractor","text":"First, we build a base network as a feature extractor. Second, we extract some layers in the feature extractor as the detection layers. Detection layer i (of size ( Mi x Ni x Ki )) is a set of Ki feature maps of size ( Mi x Ni ). Ki can be considered as the number of channels of this layer. The detection layer i divides the image into Mi x Ni spatially equal cell. Each cell may have 1 or more than 1 anchors. For more details about encoding, please visit How to encoding ground truth boxes and labels to anchors . In the example above, let say, we extract 3 detection layers from layers i , j , k . Later, we pass these layers to detection blocks to get the detection of total_num_anchors generated by our model. K = num_anchors_per_cells total_num_anchors = K[i]*(Mi*Ni) + K[j]*(Mj*Nj) + K[k]*(Mk*Nk)","title":"1.1. The feature extractor"},{"location":"torch-build-model/#12-the-detection-block","text":"The outputs of detection blocks will tell us whether a anchor belongs to a certain object class or not. If yes, then where is the detected bounding box by computing the offsets of coordinates of the detected boxes with this anchor. The detection block can be divided into 2 sub-blocks: classification block, and regression block. The detection block at each layer take the corresponding detection layer as the input and return the set of loc_dets and cls_dets . cls_dets is the set of detection classification scores for each class - the probability that the anchor belongs to the class (outputs of classification block). loc_dets is the set of detection locations of bounding boxes - the offset of coordinates of bounding boxes to their respective anchors (outputs of regression block). Classification block and regression block can consist of 1 direct conv layer or a stack of several conv layers.","title":"1.2. The detection block"},{"location":"torch-build-model/#ssd7-the-simple-example-of-object-detector","text":"SSD7 consists of: * Feature extractor is a stack of 8 conv layers. After each conv layer, the size of output is reduced by two using MaxPooling2D . Detection blocks Each detection block for each layer includes two sub-blocks: regression block and classification block. However, each sub-block is just 1 conv layer for simplicity as below: We define the SSD7 including its self.extractor , self.detection_blocks . The below code is simplified to make it succide. class DetectionBlock(nn.Module): def __init__(self): self.reg_conv self.cls_conv def forward(self, input): ... return loc_dets, cls_dets class SSD7(nn.Module): def __init__(self): self.extractor = ... # conv1 -> conv8 self.det_blocks = [DetectionBlock()] * num_detection_layers def forward(self, input): # perform feature extraction detection_layers = self.extractor(input) # perform detections final_loc_dets, final_cls_dets = [], [] for i, detection_layer in enumerate(detection_layers): loc_dets, cls_dets = self.det_blocks[i](detection_layers[i]) final_loc_dets.append(loc_dets) final_cls_dets.append(cls_dets) return final_loc_dets, final_cls_dets The details of implementation can be found at file models/ssd7.py .","title":"SSD7: The simple example of object detector"},{"location":"torch-build-model/#facenet-a-single-stage-face-detector-using-resnet-backbone","text":"Feature extractor ResNet backbone : The last conv layer of conv2 , conv3 , conv4 , conv5 blocks in ResNet are selected as the detection layers. Extra conv blocks Two more conv6 , conv7 blocks are added to ResNet backbone, each block includes 2 conv3x3. The last conv layers of conv6 and conv7 are selected as detection layers. Detection blocks The classification branch consists of 4 conv layers of 3x3 before the classification layer. The regression branch consists of 4 conv layers of 3x3 before the regression layer.","title":"FaceNet: A single-stage face detector using ResNet backbone."},{"location":"torch-encoding/","text":"How to encode the ground truth bounding boxes and labels. Define the anchor in a cell. The feature maps sizes fm_sizes at each detection layer will decide how many cell we should divide the image into. For example, if we have a detection layer of size 4x4xK , (the height and width of feature maps are 4), the image will be divided into 16 cells (a grid of 4x4) as below. This rule is also applied for the other detection layers of different sizes. The aspect ratios aspect_ratios each detection layer will decide how many anchors in 1 cell. If we set aspect_ratios=[1, 2, 1/2] for this detection layer, then we will have 3 anchors as the figure (the dash color rectangles). For simplicity, I just draw 3 anchors at 1 cell for demonstration (the same is also applied for the other cell in the detection layer). Size of anchors The size of the anchors will be defined by referenced size anchor_sizes and aspect ratios in 1 cell aspect_ratios . In the two below examples, for simple demonstration, I just pick the detection layer 4x4xK with aspect_ratios=[1,] . The size of image is 320x320. In the first example, if we set the anchor_size = 80 , then the anchors of 2 consecutive cells will not overlap each other. * In the second example, if we set the anchor_size = 120 , then the anchors of 2 consective cell will overlap each other. For practical implementation, we usually set anchor_size > image_size/feature_map_size (the second example). Therefore, more anchors will be tiled for better matching with ground truth object. Anchor matching In the example below, for simplicity, I just take 1 cell of feature maps from the detection layer 4x4xK , and 1 ground truth face. The ground truth box of face is denoted by solid green rectangle . The other default anchors are denoted by dash color rectangles . Now, we need to match which anchors (dash rectangles) with the face (solid rectangle). Therefore, we compute the IoU of the face with all the default anchors (including all anchors from all detection layers). Then we have three techniques to match anchors with ground truth boxes. Note that: One anchor is only assigned with 1 ground truth box. However, one ground truth box can be assigned with more than 1 anchor. First strategy For each ground truth box, we need to find the best anchor which has the maximum IoU with the ground truth box. Second strategy For each ground truth box, we can find as many anchors as possible with the condition that the IoU between the anchor >= first_iou_threshold . Normally, we use both the first and second strategies in object detection. However, for very small object (like tiny faces), some ground truth faces can't match enough the number of anchors which could decrease the recall rate. So, the compensation strategy is adopted to increase the number of anchor matches for small object. Third strategy (compensation strategy which is used in face detection ) After the second strategy, if the number of matched anchors for some object is smaller than topN (usually 3 < topN < 10 for optimal use), we need to find more anchors for those objects. Those anchors must satisfy the following conditions: They were not assigned to any object before. One anchor is only assigned to 1 object. The object will be assigned to those anchors must have the maximum IoU threshold with those anchors. The IoU between the objects and the anchors must greater than the second_iou_threshold . Please read the source code at torch_object_detection/utils/box_coder.py to understand how to implement three strategies.","title":"How to encode the grouth truth boxes"},{"location":"torch-encoding/#how-to-encode-the-ground-truth-bounding-boxes-and-labels","text":"","title":"How to encode the ground truth bounding boxes and labels."},{"location":"torch-encoding/#define-the-anchor-in-a-cell","text":"The feature maps sizes fm_sizes at each detection layer will decide how many cell we should divide the image into. For example, if we have a detection layer of size 4x4xK , (the height and width of feature maps are 4), the image will be divided into 16 cells (a grid of 4x4) as below. This rule is also applied for the other detection layers of different sizes. The aspect ratios aspect_ratios each detection layer will decide how many anchors in 1 cell. If we set aspect_ratios=[1, 2, 1/2] for this detection layer, then we will have 3 anchors as the figure (the dash color rectangles). For simplicity, I just draw 3 anchors at 1 cell for demonstration (the same is also applied for the other cell in the detection layer).","title":"Define the anchor in a cell."},{"location":"torch-encoding/#size-of-anchors","text":"The size of the anchors will be defined by referenced size anchor_sizes and aspect ratios in 1 cell aspect_ratios . In the two below examples, for simple demonstration, I just pick the detection layer 4x4xK with aspect_ratios=[1,] . The size of image is 320x320. In the first example, if we set the anchor_size = 80 , then the anchors of 2 consecutive cells will not overlap each other. * In the second example, if we set the anchor_size = 120 , then the anchors of 2 consective cell will overlap each other. For practical implementation, we usually set anchor_size > image_size/feature_map_size (the second example). Therefore, more anchors will be tiled for better matching with ground truth object.","title":"Size of anchors"},{"location":"torch-encoding/#anchor-matching","text":"In the example below, for simplicity, I just take 1 cell of feature maps from the detection layer 4x4xK , and 1 ground truth face. The ground truth box of face is denoted by solid green rectangle . The other default anchors are denoted by dash color rectangles . Now, we need to match which anchors (dash rectangles) with the face (solid rectangle). Therefore, we compute the IoU of the face with all the default anchors (including all anchors from all detection layers). Then we have three techniques to match anchors with ground truth boxes. Note that: One anchor is only assigned with 1 ground truth box. However, one ground truth box can be assigned with more than 1 anchor. First strategy For each ground truth box, we need to find the best anchor which has the maximum IoU with the ground truth box. Second strategy For each ground truth box, we can find as many anchors as possible with the condition that the IoU between the anchor >= first_iou_threshold . Normally, we use both the first and second strategies in object detection. However, for very small object (like tiny faces), some ground truth faces can't match enough the number of anchors which could decrease the recall rate. So, the compensation strategy is adopted to increase the number of anchor matches for small object. Third strategy (compensation strategy which is used in face detection ) After the second strategy, if the number of matched anchors for some object is smaller than topN (usually 3 < topN < 10 for optimal use), we need to find more anchors for those objects. Those anchors must satisfy the following conditions: They were not assigned to any object before. One anchor is only assigned to 1 object. The object will be assigned to those anchors must have the maximum IoU threshold with those anchors. The IoU between the objects and the anchors must greater than the second_iou_threshold . Please read the source code at torch_object_detection/utils/box_coder.py to understand how to implement three strategies.","title":"Anchor matching"},{"location":"torch-environment-setup/","text":"Contents Quick start Install conda Create an environment from evironment file Create an environment from scratch Install conda in your machine Please visit the website and follow the tutorial to download and install the Anaconda on your machine. Quick start If you already know conda and get it installed in your machine, here is the quick start for setting up the required environment without reading the rest of the parts. 1 . Install the environment. The torch_environment.yml can be found in the source directory. conda env create --name pytorch_p36 -f torch_environment.yml 2 . And activate it for further use: source activate pytorch_p36 Creating an environment from an environment.yml file By default, environments are installed into the envs directory in your conda directory. 1 . Install the required environment from the torch_environment.yml file: conda env create -f torch_environment.yml conda export 2 . Activate the environment to use the torch_object_detection package. conda activate pytorch_p36 If you set up the environment using the above command, you don't need to follow the detailed below steps because it will install all required packages for the environment from the environment.yml file. Feel free to skip the below commands. Create an environment from scratch Use the terminal (or command line window) for the following steps. 1 . To create an environment of Pytorch with Python 3.6 . conda create --name pytorch_p36 python=3.6 2 . After this, activate this environment: conda activate pytorch_p36 3 . Install Pytorch and torchvision via the below command. conda install pytorch torchvision -c pytorch 4 . Install other packages. conda install matplotlib","title":"Environment setup"},{"location":"torch-environment-setup/#contents","text":"Quick start Install conda Create an environment from evironment file Create an environment from scratch","title":"Contents"},{"location":"torch-environment-setup/#install-conda-in-your-machine","text":"Please visit the website and follow the tutorial to download and install the Anaconda on your machine.","title":"Install conda in your machine"},{"location":"torch-environment-setup/#quick-start","text":"If you already know conda and get it installed in your machine, here is the quick start for setting up the required environment without reading the rest of the parts. 1 . Install the environment. The torch_environment.yml can be found in the source directory. conda env create --name pytorch_p36 -f torch_environment.yml 2 . And activate it for further use: source activate pytorch_p36","title":"Quick start"},{"location":"torch-environment-setup/#creating-an-environment-from-an-environmentyml-file","text":"By default, environments are installed into the envs directory in your conda directory. 1 . Install the required environment from the torch_environment.yml file: conda env create -f torch_environment.yml conda export 2 . Activate the environment to use the torch_object_detection package. conda activate pytorch_p36 If you set up the environment using the above command, you don't need to follow the detailed below steps because it will install all required packages for the environment from the environment.yml file. Feel free to skip the below commands.","title":"Creating an environment from an environment.yml file"},{"location":"torch-environment-setup/#create-an-environment-from-scratch","text":"Use the terminal (or command line window) for the following steps. 1 . To create an environment of Pytorch with Python 3.6 . conda create --name pytorch_p36 python=3.6 2 . After this, activate this environment: conda activate pytorch_p36 3 . Install Pytorch and torchvision via the below command. conda install pytorch torchvision -c pytorch 4 . Install other packages. conda install matplotlib","title":"Create an environment from scratch"},{"location":"torch-getting-started/","text":"Contents Environment setup Getting started in 30 seconds to Face Detection How to prepare the dataset for training and validation How to build a face detection model How to train the model How to do inference 1. Environment setup To use torch_object_detection package, you need to install the several dependencies. I strongly recommend using conda package with the below command in your terminal: Install some presiquite dependencies 1 . Create the environment and install some required packages from torch_environment.yml file: $ conda env create --name pytorch_p36 -f torch_environment.yml 2 . Activate the environment: $ source activate pytorch_p36 Now you are ready to use the torch_object_detection for Face Detection. Install torch_object_detection as a Python package $ pip install dist/torch_object_detection-1.0.1-py3-none-any.whl For more information about the environment setup, please visit Setting up environment . 2. Getting started: 30 seconds to Face Detection Launch Python interactive shell, (or ipython, or Jupyter notebook) and import torch_object_detection package: >> import torch >> import torch_object_detection >> import torch_object_detection.models >> import torch_object_detection.utils Create the Face Detection model and load the trained weights 1. Create the model We create FaceNet with 2 classes ( background and face ) with ResNet101 as the base network: >> net = torch_object_detection.models.FaceNet(num_classes=2, base=torch_object_detection.models.Base101()) 2. Load the trained weights >> net.load_state_dict(torch.load('path/to/trained_model.pkl')) Load the image and make detections 3.1. Load image and preprocess the image >> image = torch_object_detection.utils.load_image('path/to/img', height=320, width=480) >> tensor = torch_object_detection.utils.preprocess_image(image) 3.2. Make detections If we use only CPU, please set device='cpu' . If we use a GPU, please set device='cuda' or maybe more specifically device='cuda:0' to indicate which GPU. >> det_boxes, det_scores, det_labels = torch_object_detection.utils.detect(tensor, net, device='cpu', score_thresh=0.3, nms_thresh=0.3) 3.3. Draw the detected bounding boxes and scores and show the result >> torch_object_detection.utils.draw_detections(image, det_boxes, det_scores, det_labels) 4. The complete script for end-to-end The script can be found at inference.py file. # test_inference.py import torch from torch_object_detection.utils import BoxCoder, load_image, prepocess_image, draw_detections, detect from torch_object_detection.models import FaceNet, Base101 DEVICE = 'cuda' NUM_CLASSES = 2 MODEL_PATH = '' img_path = '' # 1. Create and load trained weights net = FaceNet(num_classes=NUM_CLASSES, base=Base101()) net.load_state_dict(torch.load(MODEL_PATH, map_location='cpu')) # first load to cpu, # and then move to DEVICE if needed. net = net.to(DEVICE) # 2. Load image, convert and normalize it. image = load_image(img_path=img_path, height=640, width=640) tensor = prepocess_image(image) # 3. Create the boxcoder for decoding the outputs # Feature map sizes at each detection layer fm_sizes = [[160, 160], [80, 80], [40, 40], [20, 20], [10, 10], [5, 5]] # The referenced size of anchors at each detection layer anchor_sizes = (16, 32, 64, 128, 256, 512), # The aspect ratios of the anchors at each detection layer aspect_ratios = [[1, ], [1, ], [1, ], [1, ], [1, ], [1, ]] box_coder = BoxCoder(img_size=(HEIGHT, WIDTH), fm_sizes=fm_sizes, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios) # If you don't want to create box_coder manually, just set `box_coder=None` # 4. Make detection det_boxes, det_scores, det_labels = detect(tensor, net, box_coder=None, device=DEVICE, score_thresh=0.3, nms_thresh=0.3) # 5. Show the detected boxes and confidence scores draw_detections(image, boxes=det_boxes, scores=det_scores, labels=det_labels) To run the script: 1. Activate the environment >> source activate pytorch_p36 2. Compile and run it >> python test_inference.py","title":"Getting started"},{"location":"torch-getting-started/#contents","text":"Environment setup Getting started in 30 seconds to Face Detection How to prepare the dataset for training and validation How to build a face detection model How to train the model How to do inference","title":"Contents"},{"location":"torch-getting-started/#1-environment-setup","text":"To use torch_object_detection package, you need to install the several dependencies. I strongly recommend using conda package with the below command in your terminal: Install some presiquite dependencies 1 . Create the environment and install some required packages from torch_environment.yml file: $ conda env create --name pytorch_p36 -f torch_environment.yml 2 . Activate the environment: $ source activate pytorch_p36 Now you are ready to use the torch_object_detection for Face Detection. Install torch_object_detection as a Python package $ pip install dist/torch_object_detection-1.0.1-py3-none-any.whl For more information about the environment setup, please visit Setting up environment .","title":"1. Environment setup"},{"location":"torch-getting-started/#2-getting-started-30-seconds-to-face-detection","text":"Launch Python interactive shell, (or ipython, or Jupyter notebook) and import torch_object_detection package: >> import torch >> import torch_object_detection >> import torch_object_detection.models >> import torch_object_detection.utils Create the Face Detection model and load the trained weights 1. Create the model We create FaceNet with 2 classes ( background and face ) with ResNet101 as the base network: >> net = torch_object_detection.models.FaceNet(num_classes=2, base=torch_object_detection.models.Base101()) 2. Load the trained weights >> net.load_state_dict(torch.load('path/to/trained_model.pkl')) Load the image and make detections 3.1. Load image and preprocess the image >> image = torch_object_detection.utils.load_image('path/to/img', height=320, width=480) >> tensor = torch_object_detection.utils.preprocess_image(image) 3.2. Make detections If we use only CPU, please set device='cpu' . If we use a GPU, please set device='cuda' or maybe more specifically device='cuda:0' to indicate which GPU. >> det_boxes, det_scores, det_labels = torch_object_detection.utils.detect(tensor, net, device='cpu', score_thresh=0.3, nms_thresh=0.3) 3.3. Draw the detected bounding boxes and scores and show the result >> torch_object_detection.utils.draw_detections(image, det_boxes, det_scores, det_labels)","title":"2. Getting started: 30 seconds to Face Detection"},{"location":"torch-getting-started/#4-the-complete-script-for-end-to-end","text":"The script can be found at inference.py file. # test_inference.py import torch from torch_object_detection.utils import BoxCoder, load_image, prepocess_image, draw_detections, detect from torch_object_detection.models import FaceNet, Base101 DEVICE = 'cuda' NUM_CLASSES = 2 MODEL_PATH = '' img_path = '' # 1. Create and load trained weights net = FaceNet(num_classes=NUM_CLASSES, base=Base101()) net.load_state_dict(torch.load(MODEL_PATH, map_location='cpu')) # first load to cpu, # and then move to DEVICE if needed. net = net.to(DEVICE) # 2. Load image, convert and normalize it. image = load_image(img_path=img_path, height=640, width=640) tensor = prepocess_image(image) # 3. Create the boxcoder for decoding the outputs # Feature map sizes at each detection layer fm_sizes = [[160, 160], [80, 80], [40, 40], [20, 20], [10, 10], [5, 5]] # The referenced size of anchors at each detection layer anchor_sizes = (16, 32, 64, 128, 256, 512), # The aspect ratios of the anchors at each detection layer aspect_ratios = [[1, ], [1, ], [1, ], [1, ], [1, ], [1, ]] box_coder = BoxCoder(img_size=(HEIGHT, WIDTH), fm_sizes=fm_sizes, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios) # If you don't want to create box_coder manually, just set `box_coder=None` # 4. Make detection det_boxes, det_scores, det_labels = detect(tensor, net, box_coder=None, device=DEVICE, score_thresh=0.3, nms_thresh=0.3) # 5. Show the detected boxes and confidence scores draw_detections(image, boxes=det_boxes, scores=det_scores, labels=det_labels) To run the script: 1. Activate the environment >> source activate pytorch_p36 2. Compile and run it >> python test_inference.py","title":"4. The complete script for end-to-end"},{"location":"torch-inference/","text":"Inference for object detection 1. Build and load the model import torch import torch_object_detection # Build the model net = torch_object_detection.models.FaceNet(num_classes=2, base=torch_object_detection.models.Base101()) # Load the trained weights net.load_state_dict(torch.load('path/to/trained_model.pkl')) 2. Create a BoxCoder for decoding the detection results. If we make detections on in-variant size images (The image size is the same for all images), it is strongly recommended to define the BoxCoder for decoding the detection results. It is because we only need to define a BoxCoder once and use it many times (which saves a lot of time when creating a BoxCoder ). If the input images' size is variant, we can skip creating a BoxCoder . Insider, detect method, it already supports creating a BoxCoder by itself. However, this will make detection time longer. In the first case, we need to provide values to three arguments: fm_sizes : The feature map sizes of each detection layer obtained when we pass an image of size (Height, Width). These values can be different from the training procedure. aspect_ratios : The aspect ratios of the anchors at each detection layer. These values must be the same as the configuration setting during training. anchor_sizes : The referenced size of anchors at each detection layer. These values must be the same as the configuration setting during training. So, basically, we only need to determine the fm_sizes for different input image size (Height, Width). In case you don't know exactly how to compute it, please use the trick code below: # The simulated tensor - the same size as our input image. input_tensor = torch.randn(1, 3, Height, Width) input_tensor = input_tensor.to(DEVIDE) loc_dets, cls_dets, fm_sizes = net(input_tensor, fm_sizes_return=True) print(fm_sizes) fm_sizes = [[160, 160], [80, 80], [40, 40], [20, 20], [10, 10], [5, 5]] # The referenced size of anchors at each detection layer anchor_sizes = (16, 32, 64, 128, 256, 512), # The aspect ratios of the anchors at each detection layer aspect_ratios = [[1, ], [1, ], [1, ], [1, ], [1, ], [1, ]] box_coder = BoxCoder(img_size=(HEIGHT, WIDTH), fm_sizes=fm_sizes, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios) 3. Load the image and make detections # Load image and preprocess the image. image = torch_object_detection.utils.load_image('path/to/img', height=640, width=640) tensor = torch_object_detection.utils.preprocess_image(image) # Make detections # If you don't create the the box_coder, just leave `box_coder=None`. det_boxes, det_scores = torch_object_detection.utils.detect(tensor, net, box_coder=box_coder, device='cpu', score_thresh=0.3, nms_thresh=0.3) 4. Draw the detected bounding boxes and scores and show the result. torch_object_detection.utils.draw_detections(img, det_boxes, det_scores) Other detect methods For making detections of image with different scales..., I make several variants of detect formats. The source code can be found at torch_object_detection/utils/detect_utils.py : detect : The method that we use above. flip_detect : The detection method for detecting the horizontal flipped of the input image. resize_detect : The detection method for detecting the image of different size. Resize the one of the side of image to specific value and keep the aspect ratio unchanged. scale_detect : The detection method for detecting the image of different scale. For example, if we have an image of size (300, 500), we can use this method to detect for scaled image of (600, 1000) if scale = 2 . All the detection methods except detect require the original image (PIL Image) as the input while detect uses tensor of the image as the input. More information and details of arguments for those methods, please visit source code.","title":"Do inference from the trained model"},{"location":"torch-inference/#inference-for-object-detection","text":"","title":"Inference for object detection"},{"location":"torch-inference/#1-build-and-load-the-model","text":"import torch import torch_object_detection # Build the model net = torch_object_detection.models.FaceNet(num_classes=2, base=torch_object_detection.models.Base101()) # Load the trained weights net.load_state_dict(torch.load('path/to/trained_model.pkl'))","title":"1. Build and load the model"},{"location":"torch-inference/#2-create-a-boxcoder-for-decoding-the-detection-results","text":"If we make detections on in-variant size images (The image size is the same for all images), it is strongly recommended to define the BoxCoder for decoding the detection results. It is because we only need to define a BoxCoder once and use it many times (which saves a lot of time when creating a BoxCoder ). If the input images' size is variant, we can skip creating a BoxCoder . Insider, detect method, it already supports creating a BoxCoder by itself. However, this will make detection time longer. In the first case, we need to provide values to three arguments: fm_sizes : The feature map sizes of each detection layer obtained when we pass an image of size (Height, Width). These values can be different from the training procedure. aspect_ratios : The aspect ratios of the anchors at each detection layer. These values must be the same as the configuration setting during training. anchor_sizes : The referenced size of anchors at each detection layer. These values must be the same as the configuration setting during training. So, basically, we only need to determine the fm_sizes for different input image size (Height, Width). In case you don't know exactly how to compute it, please use the trick code below: # The simulated tensor - the same size as our input image. input_tensor = torch.randn(1, 3, Height, Width) input_tensor = input_tensor.to(DEVIDE) loc_dets, cls_dets, fm_sizes = net(input_tensor, fm_sizes_return=True) print(fm_sizes) fm_sizes = [[160, 160], [80, 80], [40, 40], [20, 20], [10, 10], [5, 5]] # The referenced size of anchors at each detection layer anchor_sizes = (16, 32, 64, 128, 256, 512), # The aspect ratios of the anchors at each detection layer aspect_ratios = [[1, ], [1, ], [1, ], [1, ], [1, ], [1, ]] box_coder = BoxCoder(img_size=(HEIGHT, WIDTH), fm_sizes=fm_sizes, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios)","title":"2. Create a BoxCoder for decoding the detection results."},{"location":"torch-inference/#3-load-the-image-and-make-detections","text":"# Load image and preprocess the image. image = torch_object_detection.utils.load_image('path/to/img', height=640, width=640) tensor = torch_object_detection.utils.preprocess_image(image) # Make detections # If you don't create the the box_coder, just leave `box_coder=None`. det_boxes, det_scores = torch_object_detection.utils.detect(tensor, net, box_coder=box_coder, device='cpu', score_thresh=0.3, nms_thresh=0.3)","title":"3. Load the image and make detections"},{"location":"torch-inference/#4-draw-the-detected-bounding-boxes-and-scores-and-show-the-result","text":"torch_object_detection.utils.draw_detections(img, det_boxes, det_scores)","title":"4. Draw the detected bounding boxes and scores and show the result."},{"location":"torch-inference/#other-detect-methods","text":"For making detections of image with different scales..., I make several variants of detect formats. The source code can be found at torch_object_detection/utils/detect_utils.py : detect : The method that we use above. flip_detect : The detection method for detecting the horizontal flipped of the input image. resize_detect : The detection method for detecting the image of different size. Resize the one of the side of image to specific value and keep the aspect ratio unchanged. scale_detect : The detection method for detecting the image of different scale. For example, if we have an image of size (300, 500), we can use this method to detect for scaled image of (600, 1000) if scale = 2 . All the detection methods except detect require the original image (PIL Image) as the input while detect uses tensor of the image as the input. More information and details of arguments for those methods, please visit source code.","title":"Other detect methods"},{"location":"torch-prepare-dataset/","text":"Prepare the dataset for training Annotation file format Dataset format relative/path/to/image1.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... relative/path/to/image2.jpg xmin2 ymin2 xmax2 ymax2 label2 xmin2 ymin2 xmax2 ymax2 label2 ... . . . The relative path to the image here is the path from the data_root to the image. For examples, data_root |____train_datasets | |____train_image1.jpg | |____train_image2.jpg | |____train_image3.jpg | |____train_image4.jpg | |____train_image5.jpg | |____val_datasets |____val_image1.jpg |____val_image2.jpg |____val_image3.jpg |____val_image4.jpg |____val_image5.jpg If we set: data_root = 'data_root/train_datasets' Then the annotations file train_annotations.txt for training dataset should be: train_images1.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... train_images2.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... train_images3.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... train_images4.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... train_images5.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... Meanwhile, if we set: val_data_root = 'data_root/val_datasets' Then the annotations file val_annotations.txt for validation dataset should be: val_images1.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... val_images2.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... val_images3.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... val_images4.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... val_images5.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... For each image, after the relative path to image file, we list all the possible bounding boxes of objects in the image. The a set of values xmini ymini xmaxi ymaxi labeli represents for a bounding box i. We denote (xmin, ymin), (xmax, ymax) are the coordinates of top-left, and bottom-right points of the bounding box while, the label is the class label of the object start from 0 . We only need to label the bounding boxes of objects (not the background). If we have only 1 class face , then the class label for face is 0 . For example, if the image have k faces then: relative/path/to/image.jpg xmin1 ymin1 xmax1 ymax1 label1 ... xmink ymink xmaxk ymaxk labelk Prepare the DataLoader for datasets In this section, I'm going to explain how to create an ODDataset ( ODDataset is acutally a wrapped class of torch.utils.data.Dataset ). Then we will create DataLoader for training and validation datasets (which is similar to DataGenerator in Keras): If we don't want to do validation during training, so we can skip all related below things for validation dataset. 1. Set some configuration parameters First, we set the values for the following parameters for making the data generator in torch , DataGenerator : BATCH_SIZE = 16 NUM_CLASSES = 2 WIDTH = 640 HEIGHT = 640 RGB_MEAN = (0.485, 0.456, 0.406) RGB_STD = (0.229, 0.224, 0.225) Explain : BATCH_SIZE : The size of batch during training and validation. NUM_CLASSES : Total number of classes in our datasets including the background class. For examples, we set NUM_CLASSES for face detection because we have two classes: face and background (non-face). WIDTH : The width of image that we resize to. We must resize the image of different size into the same size in order to take the advantage of batch training for GPUs. HEIGHT :The height of image that we resize to. RGB_MEAN: The mean values in red, green, blue channels. After converting to tensors using ToTensor() , the pixel values of the images will be converted from 0-255 to 0.0-1.0 . The we subtract these values to RGB_MEAN in each channel of the image. RGB_STD: The standard deviation values in red, green, blue channels. Note: Normally, we use any pretrained models, we use the above values of RGB_MEAN , and RGB_STD to normalize the values of inputs imagse. It's the common practice in Pytorch. Next, we create a box_coder for encoding the ground truth bounding boxes and labels of the datasets into the corresponding formats (anchors) for training and validation. 2. Create the BoxCoder We need to create for encoding the bounding boxes and labels of objects in images with the following parameters. box_coder = BoxCoder(img_size=(640, 640), offset=(0.5, 0.5), box_sizes=(16, 32, 64, 128, 256, 512), aspect_ratios=((1,), (1,), (1,), (1,), (1,), (1,)), fm_sizes=((160, 160), (80, 80), (40, 40), (20, 20), (10, 10), (5, 5)), ) img_size . The size of input image. (height, width) offset . The offset values to normalize the bounding boxes to each cell. It shouldn't be changed. aspect_ratios . This is a set of aspect ratios in each detection layer. For example, the first value aspect_ratios[0] = (1,) is the set of aspect ratios of anchors in a cell for the first detection layer. Here there is 1 aspect ratio value 1 which means there is only 1 anchor in a cell and anchor width / anchor height = 1 . fm_sizes . This is a set of feature maps sizes in each detection layer. 3. Create ODDatasets for training and validation datasets DataLoader is similar to DataGenerator in Keras which generates a batch of data in CPU meanwhile GPUs are doing heavy computations. 3.1. Define transformations First, we define the transformations applied for our training datasets including: Color jittering: randomly adjust the brightness , contrast , hue . Spatial transformations: randomly crop the several square patches from an image and select one of them (I follow SFD paper), randomly flipping, and Resize to fixed size (640, 640) for training and validation. Encoding the ground truth bounding boxes and labels: we need to pass to box_coder . We don't need to apply spatial transformations to validation dataset. # 2.1 Create train_loader # Initiate some augmentations perform on training data train_transform = Augmentation(brightnesss=32 / 255., contrast=0.5, saturation=0.5, hue=0.1, random_crop=RandomCrop(), resize=Resize(size=(HEIGHT, WIDTH)), random_flip=RandomFlip(), box_coder=box_coder, rgb_mean=RGB_MEAN, rgb_std=RGB_STD, ) val_transform = Augmentation(brightnesss=0, contrast=0, saturation=0, hue=0, resize=Resize(size=(HEIGHT, WIDTH)), box_coder=box_coder, rgb_mean=RGB_MEAN, rgb_std=RGB_STD, ) 3.2. Create ODDatasets ODDataset is actually a class that I wrap up torch.utils.data.Dataset which is needed for create the torch.utils.data.DataLoader . # 2. Create the training and validation data loaders # 2.0.1 The dir that contain the images train_images_dir = '/home/ubuntu/Datasets/WiderFace/WIDER_train/images' val_images_dir = '/home/ubuntu/Datasets/WiderFace/WIDER_val/images' # 2.0.2 The dir that contain the annotation files. train_annotations_file = './examples/annotations/widerface/train_annotations.txt' val_annotations_file = './examples/annotations/widerface/val_annotations.txt' train_datasets = ODDataset(images_dir=train_images_dir, anno_file=train_annotations_file, transform=train_transform) 4. Create DataLoaders for training and validation datasets Finally, we need to create DataLoader for training and validation dataset. # Get a train data loader (which is similar to DataGenerator in Keras) train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=BATCH_SIZE, shuffle=True, num_workers=8) val_datasets = ODDataset(images_dir=val_images_dir, anno_file=val_annotations_file, transform=val_transform) # Get a val data loader val_loader = torch.utils.data.DataLoader(val_datasets, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)","title":"Prepare the dataset"},{"location":"torch-prepare-dataset/#prepare-the-dataset-for-training","text":"","title":"Prepare the dataset for training"},{"location":"torch-prepare-dataset/#annotation-file-format","text":"Dataset format relative/path/to/image1.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... relative/path/to/image2.jpg xmin2 ymin2 xmax2 ymax2 label2 xmin2 ymin2 xmax2 ymax2 label2 ... . . . The relative path to the image here is the path from the data_root to the image. For examples, data_root |____train_datasets | |____train_image1.jpg | |____train_image2.jpg | |____train_image3.jpg | |____train_image4.jpg | |____train_image5.jpg | |____val_datasets |____val_image1.jpg |____val_image2.jpg |____val_image3.jpg |____val_image4.jpg |____val_image5.jpg If we set: data_root = 'data_root/train_datasets' Then the annotations file train_annotations.txt for training dataset should be: train_images1.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... train_images2.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... train_images3.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... train_images4.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... train_images5.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... Meanwhile, if we set: val_data_root = 'data_root/val_datasets' Then the annotations file val_annotations.txt for validation dataset should be: val_images1.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... val_images2.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... val_images3.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... val_images4.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... val_images5.jpg xmin1 ymin1 xmax1 ymax1 label1 xmin2 ymin2 xmax2 ymax2 label2 ... For each image, after the relative path to image file, we list all the possible bounding boxes of objects in the image. The a set of values xmini ymini xmaxi ymaxi labeli represents for a bounding box i. We denote (xmin, ymin), (xmax, ymax) are the coordinates of top-left, and bottom-right points of the bounding box while, the label is the class label of the object start from 0 . We only need to label the bounding boxes of objects (not the background). If we have only 1 class face , then the class label for face is 0 . For example, if the image have k faces then: relative/path/to/image.jpg xmin1 ymin1 xmax1 ymax1 label1 ... xmink ymink xmaxk ymaxk labelk","title":"Annotation file format"},{"location":"torch-prepare-dataset/#prepare-the-dataloader-for-datasets","text":"In this section, I'm going to explain how to create an ODDataset ( ODDataset is acutally a wrapped class of torch.utils.data.Dataset ). Then we will create DataLoader for training and validation datasets (which is similar to DataGenerator in Keras): If we don't want to do validation during training, so we can skip all related below things for validation dataset.","title":"Prepare the DataLoader for datasets"},{"location":"torch-prepare-dataset/#1-set-some-configuration-parameters","text":"First, we set the values for the following parameters for making the data generator in torch , DataGenerator : BATCH_SIZE = 16 NUM_CLASSES = 2 WIDTH = 640 HEIGHT = 640 RGB_MEAN = (0.485, 0.456, 0.406) RGB_STD = (0.229, 0.224, 0.225) Explain : BATCH_SIZE : The size of batch during training and validation. NUM_CLASSES : Total number of classes in our datasets including the background class. For examples, we set NUM_CLASSES for face detection because we have two classes: face and background (non-face). WIDTH : The width of image that we resize to. We must resize the image of different size into the same size in order to take the advantage of batch training for GPUs. HEIGHT :The height of image that we resize to. RGB_MEAN: The mean values in red, green, blue channels. After converting to tensors using ToTensor() , the pixel values of the images will be converted from 0-255 to 0.0-1.0 . The we subtract these values to RGB_MEAN in each channel of the image. RGB_STD: The standard deviation values in red, green, blue channels. Note: Normally, we use any pretrained models, we use the above values of RGB_MEAN , and RGB_STD to normalize the values of inputs imagse. It's the common practice in Pytorch. Next, we create a box_coder for encoding the ground truth bounding boxes and labels of the datasets into the corresponding formats (anchors) for training and validation.","title":"1. Set some configuration parameters"},{"location":"torch-prepare-dataset/#2-create-the-boxcoder","text":"We need to create for encoding the bounding boxes and labels of objects in images with the following parameters. box_coder = BoxCoder(img_size=(640, 640), offset=(0.5, 0.5), box_sizes=(16, 32, 64, 128, 256, 512), aspect_ratios=((1,), (1,), (1,), (1,), (1,), (1,)), fm_sizes=((160, 160), (80, 80), (40, 40), (20, 20), (10, 10), (5, 5)), ) img_size . The size of input image. (height, width) offset . The offset values to normalize the bounding boxes to each cell. It shouldn't be changed. aspect_ratios . This is a set of aspect ratios in each detection layer. For example, the first value aspect_ratios[0] = (1,) is the set of aspect ratios of anchors in a cell for the first detection layer. Here there is 1 aspect ratio value 1 which means there is only 1 anchor in a cell and anchor width / anchor height = 1 . fm_sizes . This is a set of feature maps sizes in each detection layer.","title":"2. Create the BoxCoder"},{"location":"torch-prepare-dataset/#3-create-oddatasets-for-training-and-validation-datasets","text":"DataLoader is similar to DataGenerator in Keras which generates a batch of data in CPU meanwhile GPUs are doing heavy computations.","title":"3. Create ODDatasets for training and validation datasets"},{"location":"torch-prepare-dataset/#31-define-transformations","text":"First, we define the transformations applied for our training datasets including: Color jittering: randomly adjust the brightness , contrast , hue . Spatial transformations: randomly crop the several square patches from an image and select one of them (I follow SFD paper), randomly flipping, and Resize to fixed size (640, 640) for training and validation. Encoding the ground truth bounding boxes and labels: we need to pass to box_coder . We don't need to apply spatial transformations to validation dataset. # 2.1 Create train_loader # Initiate some augmentations perform on training data train_transform = Augmentation(brightnesss=32 / 255., contrast=0.5, saturation=0.5, hue=0.1, random_crop=RandomCrop(), resize=Resize(size=(HEIGHT, WIDTH)), random_flip=RandomFlip(), box_coder=box_coder, rgb_mean=RGB_MEAN, rgb_std=RGB_STD, ) val_transform = Augmentation(brightnesss=0, contrast=0, saturation=0, hue=0, resize=Resize(size=(HEIGHT, WIDTH)), box_coder=box_coder, rgb_mean=RGB_MEAN, rgb_std=RGB_STD, )","title":"3.1. Define transformations"},{"location":"torch-prepare-dataset/#32-create-oddatasets","text":"ODDataset is actually a class that I wrap up torch.utils.data.Dataset which is needed for create the torch.utils.data.DataLoader . # 2. Create the training and validation data loaders # 2.0.1 The dir that contain the images train_images_dir = '/home/ubuntu/Datasets/WiderFace/WIDER_train/images' val_images_dir = '/home/ubuntu/Datasets/WiderFace/WIDER_val/images' # 2.0.2 The dir that contain the annotation files. train_annotations_file = './examples/annotations/widerface/train_annotations.txt' val_annotations_file = './examples/annotations/widerface/val_annotations.txt' train_datasets = ODDataset(images_dir=train_images_dir, anno_file=train_annotations_file, transform=train_transform)","title":"3.2. Create ODDatasets"},{"location":"torch-prepare-dataset/#4-create-dataloaders-for-training-and-validation-datasets","text":"Finally, we need to create DataLoader for training and validation dataset. # Get a train data loader (which is similar to DataGenerator in Keras) train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=BATCH_SIZE, shuffle=True, num_workers=8) val_datasets = ODDataset(images_dir=val_images_dir, anno_file=val_annotations_file, transform=val_transform) # Get a val data loader val_loader = torch.utils.data.DataLoader(val_datasets, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)","title":"4. Create DataLoaders for training and validation datasets"},{"location":"torch-train-model/","text":"Train a model for object detection To perform training the model, you should visit the documentation for part 1 , and part 2 to get the idea how to prepare the dataset and how to build a object detection model. For more details of the training code, please view train.py as an example. 1. Prepare the DataLoader for training and validation dataset Please visit Prepare the training dataset and How to encode bounding boxes and labels for dataset . train_loader = ... val_loader = ... # optionally 2. Create the model Please visit Build an object detection model to understand how to build your own model. DEVICE = 'cuda' # Initiate a model net = ... # your model # Move it to 'GPUs' net = net.to(DEVICE) # If you have two GPUs, you can train it in parallel. net = torch.nn.DataParallel(net) 3. Define the optimizer and loss # The loss criterion = SSDLoss(num_classes=NUM_CLASSES, alpha=4.0) # The optimizer optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4) scheduler = torch.optim.MultiStepLR(optimizer, milestones=[50, 100, 180], gamma=0.1) ''' We use MultiStepLR for schedule the learning rate of optimizer during the training Args: optimizer (Optimizer): Wrapped optimizer. milestones (list): List of epoch indices. Must be increasing. gamma (float): Multiplicative factor of learning rate decay. Default: 0.1. last_epoch (int): The index of last epoch. Default: -1. Example: >>> # Assuming optimizer uses lr = 0.05 for all groups >>> # lr = 0.05 if epoch < 30 >>> # lr = 0.005 if 30 <= epoch < 80 >>> # lr = 0.0005 if epoch >= 80 >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) ''' 4. Define training and validation steps. 1 . Training step # Define training step def train(epoch): # change the mode of the network to `train` net.train() train_loss = 0 for batch_idx, (inputs, loc_targets, cls_targets) in enumerate(train_loader): # Move the data to `DEVICE` (`cuda`). inputs = inputs.to(DEVICE) loc_targets = loc_targets.to(device=DEVICE) cls_targets = cls_targets.to(device=DEVICE) # Remember that we need to zero out the gradients before doing forward pass # because gradients will be aggregated from previous epochs. optimizer.zero_grad() # Do forward pass loc_preds, cls_preds = net(inputs) # Compute loss loss = criterion(loc_preds, loc_targets, cls_preds, cls_targets) # Do backward pass loss.backward() optimizer.step() # Some auxilaury steps train_loss += loss.item() train_loss /= (batch_idx + 1) # Save the current model weights. ... # Update the best model weights if any. ... return train_loss 2 . Validation step # Define eval step def eval(epoch): # Change the mode of network to `eval` net.eval() val_loss = 0 # We don't need to calculate the gradients for parameters with torch.no_grad(): for batch_idx, (inputs, loc_targets, cls_targets) in enumerate(val_loader): # Move the data to `DEVICE` (`cuda`). inputs = inputs.to(device=DEVICE) loc_targets = loc_targets.to(device=DEVICE) cls_targets = cls_targets.to(device=DEVICE) # Doing forward pass loc_preds, cls_preds = net(inputs) # Compute the loss loss = criterion(loc_preds, loc_targets, cls_preds, cls_targets) val_loss += loss.item() # Compare with the previous best validation loss and save checkpoint if any ... return val_loss 5. Start training and validation. for epoch in range(EPOCHS): # Update the learning rate along the epoch scheduler.step() # Perform training in epoch `epoch_th` train_loss = train(epoch) # Perform evaluation in epoch `epoch_th` val_loss = eval(epoch) print('train_loss: %.4f, val_loss: %.4f' % (train_loss, val_loss))","title":"Train the model"},{"location":"torch-train-model/#train-a-model-for-object-detection","text":"To perform training the model, you should visit the documentation for part 1 , and part 2 to get the idea how to prepare the dataset and how to build a object detection model. For more details of the training code, please view train.py as an example.","title":"Train a model for object detection"},{"location":"torch-train-model/#1-prepare-the-dataloader-for-training-and-validation-dataset","text":"Please visit Prepare the training dataset and How to encode bounding boxes and labels for dataset . train_loader = ... val_loader = ... # optionally","title":"1. Prepare the DataLoader for training and validation dataset"},{"location":"torch-train-model/#2-create-the-model","text":"Please visit Build an object detection model to understand how to build your own model. DEVICE = 'cuda' # Initiate a model net = ... # your model # Move it to 'GPUs' net = net.to(DEVICE) # If you have two GPUs, you can train it in parallel. net = torch.nn.DataParallel(net)","title":"2. Create the model"},{"location":"torch-train-model/#3-define-the-optimizer-and-loss","text":"# The loss criterion = SSDLoss(num_classes=NUM_CLASSES, alpha=4.0) # The optimizer optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4) scheduler = torch.optim.MultiStepLR(optimizer, milestones=[50, 100, 180], gamma=0.1) ''' We use MultiStepLR for schedule the learning rate of optimizer during the training Args: optimizer (Optimizer): Wrapped optimizer. milestones (list): List of epoch indices. Must be increasing. gamma (float): Multiplicative factor of learning rate decay. Default: 0.1. last_epoch (int): The index of last epoch. Default: -1. Example: >>> # Assuming optimizer uses lr = 0.05 for all groups >>> # lr = 0.05 if epoch < 30 >>> # lr = 0.005 if 30 <= epoch < 80 >>> # lr = 0.0005 if epoch >= 80 >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1) >>> for epoch in range(100): >>> scheduler.step() >>> train(...) >>> validate(...) '''","title":"3. Define the optimizer and loss"},{"location":"torch-train-model/#4-define-training-and-validation-steps","text":"1 . Training step # Define training step def train(epoch): # change the mode of the network to `train` net.train() train_loss = 0 for batch_idx, (inputs, loc_targets, cls_targets) in enumerate(train_loader): # Move the data to `DEVICE` (`cuda`). inputs = inputs.to(DEVICE) loc_targets = loc_targets.to(device=DEVICE) cls_targets = cls_targets.to(device=DEVICE) # Remember that we need to zero out the gradients before doing forward pass # because gradients will be aggregated from previous epochs. optimizer.zero_grad() # Do forward pass loc_preds, cls_preds = net(inputs) # Compute loss loss = criterion(loc_preds, loc_targets, cls_preds, cls_targets) # Do backward pass loss.backward() optimizer.step() # Some auxilaury steps train_loss += loss.item() train_loss /= (batch_idx + 1) # Save the current model weights. ... # Update the best model weights if any. ... return train_loss 2 . Validation step # Define eval step def eval(epoch): # Change the mode of network to `eval` net.eval() val_loss = 0 # We don't need to calculate the gradients for parameters with torch.no_grad(): for batch_idx, (inputs, loc_targets, cls_targets) in enumerate(val_loader): # Move the data to `DEVICE` (`cuda`). inputs = inputs.to(device=DEVICE) loc_targets = loc_targets.to(device=DEVICE) cls_targets = cls_targets.to(device=DEVICE) # Doing forward pass loc_preds, cls_preds = net(inputs) # Compute the loss loss = criterion(loc_preds, loc_targets, cls_preds, cls_targets) val_loss += loss.item() # Compare with the previous best validation loss and save checkpoint if any ... return val_loss","title":"4. Define training and validation steps."},{"location":"torch-train-model/#5-start-training-and-validation","text":"for epoch in range(EPOCHS): # Update the learning rate along the epoch scheduler.step() # Perform training in epoch `epoch_th` train_loss = train(epoch) # Perform evaluation in epoch `epoch_th` val_loss = eval(epoch) print('train_loss: %.4f, val_loss: %.4f' % (train_loss, val_loss))","title":"5. Start training and validation."},{"location":"torch-truncated-model/","text":"In this post, I'm going to present how to reuse the trained model on few detection layers. As we built and trained the FaceNet (using ResNet101 via Base101) with 6 detection layers. The first detection layers will detect the faces of approximately (16x16) in 160x160 cells if the input image size is 640x640 (the feature maps size of this detection layer). The second detection layers will detect the faces of approximately (32x32) in 80x80 cells if the input image size is 640x640 (the feature maps size of this detection layer). In practical use, we rarely need to detect the very tiny faces like this. The purpose of two layers is mainly for achieving high accuracy in WiderFace dataset. Removing the first detection layer or both layers will reduces a lot of time on making detections via forward pass. There are 3 main things we need to care when removing these layers. 1. Build the truncated models. A. Removing these detection layers is simply \"don't create DetectionBlock for these layers\" . When we define FaceNet model, I tried to make it flexible as possible. As we see in the code, the number of detection blocks depends on the length of the list num_anchors_per_cells . * num_anchors_per_cells : list of values representing for the number of anchors per cell in each detection layer (because we have only square anchor - then the values are 1 for all layers). class FaceNet(): def __init__(self, num_classes=2, num_anchors_per_cells=[1, 1, 1, 1, 1, 1], base=Base101(pretrained=True)): # ... self.num_det_layers = len(num_anchors_per_cells) for i, num_anchor_per_cell in enumerate(self.num_anchors_per_cells): self.det_blocks += [DetectionBlock(num_anchor_per_cell, self.num_classes)] If we remove the first detection layer, simply set num_anchors_per_cells=[1, 1, 1, 1, 1] . Then, there will be 5 DetectionBlock created. B. We also need to determine the the input detection layer for each DetectionBlock we created. The feature maps of 6 detection layers we extract from the self.extractor are [conv2, conv3, conv4, conv5, conv6, conv7] . If we remove the first detection layer, so we need to pass [conv3, conv4, conv5, conv6, conv7] to the self.det_blocks . class FaceNet(): def __init__(self): ... def forward(self, input): # Get [conv2, conv3, conv4, conv5, conv6, conv7] ftmaps = self.extractor(input) # If we remove the first detection layers, we only need to keep # [conv3, conv4, conv5, conv6, conv7]. start_idx = (len(ftmaps) - self.num_det_layers) ftmaps = ftmaps[start_idx : ] # Then, make the detections for i in range(self.num_det_layers): res_dets = self.det_blocks[i](ftmaps[i]) 2. Get the weights for these truncated models. We trained the FaceNet (ResNet101 backbone) model with 6 detection layers and save the weights to the pickle file. Essentially, the pickle file is just a dictionary ( OrderedDict ) with key : value is weight_name : weight_value . The DetectionBlock contains several conv layers, and the weights of conv layers would start with the prefix which is the name of its DetectionBlock . We define DetectionBlock in a list with the attribute self.det_blocks : class FaceNet(): def __init__(self, ...): ... for i, num_anchor_per_cell in enumerate(self.num_anchors_per_cells): self.det_blocks += [DetectionBlock(num_anchor_per_cell, self.num_classes)] Therefore, the weights of all conv layers in the first DetectionBlock would start with the prefix: det_blocks.0. . The second DetectionBlock starts with det_blocks.1 , and so on. If we remove the first detection layer, we need to: * Remove the first DetectionBlock from the weight dictionary. Remove all keys and weights which start with det_blocks.0. Change the key name of the other detection blocks. 'det_blocks.0' - remove 'det_blocks.1' -> change to 'det_blocks.0' 'det_blocks.2' -> change to 'det_blocks.1' 'det_blocks.3' -> change to 'det_blocks.2' 'det_blocks.4' -> change to 'det_blocks.3' 'det_blocks.5' -> change to 'det_blocks.4' If we remove the first and second layers: 'det_blocks.0' - remove 'det_blocks.1' - remove 'det_blocks.2' -> change to 'det_blocks.0' 'det_blocks.3' -> change to 'det_blocks.1' 'det_blocks.4' -> change to 'det_blocks.2' 'det_blocks.5' -> change to 'det_blocks.3' Finally, we save the weights dictionary to a new file. The implementation of the code is located in '/utils/model_utils.py'. 3. Make detections with the truncated models. The example below is for the truncated model of 5 detection layers (remove the first detection layer). # 1. Build model net = FaceNet(num_classes=2, num_anchors_per_cells=[1, 1, 1, 1, 1], # 5 base=torch_object_detection.models.Base101(pretrained=False) ) # 2. Load truncated weights net.load_state_dict(torch.load(truncated_model_path)) # 3. Make detections and suppose we have an image -> convert to a tensor loc_dets, cls_dets, fm_sizes = net(tensor, fm_sizes_return=True) Now, after doing forward pass to the model, we need to decode loc_dets, cls_dets, fm_sizes into the boxes, labels, and confidence scores. It means that we need to define the BoxCoder for it. If we remove the first layers, we need to redefine the values of anchor_sizes , aspect_ratios , and fm_sizes . The example below is for the input image of (640x640) # Load and image and preprocess it to tensor image = torch_object_detection.utils.load_image('./images/2.jpg', height=640, width=640) tensor = torch_object_detection.utils.preprocess_image(image) # Define some argument values for BoxCoder img_size = (640, 640) anchor_sizes = [32, 64, 128, 256, 512] aspect_ratios = [[1,],[1,], [1,], [1,], [1,]] fm_sizes = [[80, 80], [40, 40], [20, 20], [10, 10], [5, 5]] box_coder = torch_object_detection.utils.BoxCoder(img_size=img_size, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios, fm_sizes=fm_sizes) The fm_sizes can be changed if we change the input image size. For example, if we get the image of (320, 480) but we don't know what is the values of fm_sizes , so there is 1 way to do it: # Load and image and preprocess it to tensor image = torch_object_detection.utils.load_image('./images/2.jpg', height=640, width=640) tensor = torch_object_detection.utils.preprocess_image(image) # Do forward pass, we get the fm_sizes if `fm_sizes_return=True` loc_dets, cls_dets, fm_sizes = net(tensor, fm_sizes_return=True) In detect method, I already implement it for you, but you must define anchor_sizes , aspect_ratio for it while fm_sizes=None . # Load and image and preprocess it to tensor image = torch_object_detection.utils.load_image('./images/2.jpg', height=480, width=320) tensor = torch_object_detection.utils.preprocess_image(image) # Define some argument values for BoxCoder img_size = (image.size[1], image.size[0]) anchor_sizes = [32, 64, 128, 256, 512] aspect_ratios = [[1,],[1,], [1,], [1,], [1,]] box_coder = torch_object_detection.utils.BoxCoder(img_size=img_size, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios, fm_sizes=None) 4. Complete exmple: # Build model net = FaceNet(num_classes=2, num_anchors_per_cells=[1, 1, 1, 1, 1], # 5 base=torch_object_detection.models.Base101(pretrained=False) ) # Load truncated weights net.load_state_dict(torch.load(truncated_model_path)) # Load and image and preprocess it to tensor image = torch_object_detection.utils.load_image('./images/2.jpg', height=480, width=320) tensor = torch_object_detection.utils.preprocess_image(image) # Define some argument values for BoxCoder img_size = (image.size[1], image.size[0]) anchor_sizes = [32, 64, 128, 256, 512] aspect_ratios = [[1,],[1,], [1,], [1,], [1,]] box_coder = torch_object_detection.utils.BoxCoder(img_size=img_size, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios, fm_sizes=None) # Make detection with `detect` method det_boxes, det_scores, det_labels = torch_object_detection.utils.detect(tensor, net, box_coder=box_coder, device='cpu', score_thresh=0.3, nms_thresh=0.3) # Draw the detections drawn_image = torch_object_detection.utils.draw_detections(image, det_boxes, det_scores, det_labels) The scripts of examples can be found at examples/find_faces_with_original_model.py examples/find_faces_with_notop1_model.py examples/find_faces_with_notop12_model.py","title":"Truncate the first detection layers from models"},{"location":"torch-truncated-model/#1-build-the-truncated-models","text":"A. Removing these detection layers is simply \"don't create DetectionBlock for these layers\" . When we define FaceNet model, I tried to make it flexible as possible. As we see in the code, the number of detection blocks depends on the length of the list num_anchors_per_cells . * num_anchors_per_cells : list of values representing for the number of anchors per cell in each detection layer (because we have only square anchor - then the values are 1 for all layers). class FaceNet(): def __init__(self, num_classes=2, num_anchors_per_cells=[1, 1, 1, 1, 1, 1], base=Base101(pretrained=True)): # ... self.num_det_layers = len(num_anchors_per_cells) for i, num_anchor_per_cell in enumerate(self.num_anchors_per_cells): self.det_blocks += [DetectionBlock(num_anchor_per_cell, self.num_classes)] If we remove the first detection layer, simply set num_anchors_per_cells=[1, 1, 1, 1, 1] . Then, there will be 5 DetectionBlock created. B. We also need to determine the the input detection layer for each DetectionBlock we created. The feature maps of 6 detection layers we extract from the self.extractor are [conv2, conv3, conv4, conv5, conv6, conv7] . If we remove the first detection layer, so we need to pass [conv3, conv4, conv5, conv6, conv7] to the self.det_blocks . class FaceNet(): def __init__(self): ... def forward(self, input): # Get [conv2, conv3, conv4, conv5, conv6, conv7] ftmaps = self.extractor(input) # If we remove the first detection layers, we only need to keep # [conv3, conv4, conv5, conv6, conv7]. start_idx = (len(ftmaps) - self.num_det_layers) ftmaps = ftmaps[start_idx : ] # Then, make the detections for i in range(self.num_det_layers): res_dets = self.det_blocks[i](ftmaps[i])","title":"1. Build the truncated models."},{"location":"torch-truncated-model/#2-get-the-weights-for-these-truncated-models","text":"We trained the FaceNet (ResNet101 backbone) model with 6 detection layers and save the weights to the pickle file. Essentially, the pickle file is just a dictionary ( OrderedDict ) with key : value is weight_name : weight_value . The DetectionBlock contains several conv layers, and the weights of conv layers would start with the prefix which is the name of its DetectionBlock . We define DetectionBlock in a list with the attribute self.det_blocks : class FaceNet(): def __init__(self, ...): ... for i, num_anchor_per_cell in enumerate(self.num_anchors_per_cells): self.det_blocks += [DetectionBlock(num_anchor_per_cell, self.num_classes)] Therefore, the weights of all conv layers in the first DetectionBlock would start with the prefix: det_blocks.0. . The second DetectionBlock starts with det_blocks.1 , and so on. If we remove the first detection layer, we need to: * Remove the first DetectionBlock from the weight dictionary. Remove all keys and weights which start with det_blocks.0. Change the key name of the other detection blocks. 'det_blocks.0' - remove 'det_blocks.1' -> change to 'det_blocks.0' 'det_blocks.2' -> change to 'det_blocks.1' 'det_blocks.3' -> change to 'det_blocks.2' 'det_blocks.4' -> change to 'det_blocks.3' 'det_blocks.5' -> change to 'det_blocks.4' If we remove the first and second layers: 'det_blocks.0' - remove 'det_blocks.1' - remove 'det_blocks.2' -> change to 'det_blocks.0' 'det_blocks.3' -> change to 'det_blocks.1' 'det_blocks.4' -> change to 'det_blocks.2' 'det_blocks.5' -> change to 'det_blocks.3' Finally, we save the weights dictionary to a new file. The implementation of the code is located in '/utils/model_utils.py'.","title":"2. Get the weights for these truncated models."},{"location":"torch-truncated-model/#3-make-detections-with-the-truncated-models","text":"The example below is for the truncated model of 5 detection layers (remove the first detection layer). # 1. Build model net = FaceNet(num_classes=2, num_anchors_per_cells=[1, 1, 1, 1, 1], # 5 base=torch_object_detection.models.Base101(pretrained=False) ) # 2. Load truncated weights net.load_state_dict(torch.load(truncated_model_path)) # 3. Make detections and suppose we have an image -> convert to a tensor loc_dets, cls_dets, fm_sizes = net(tensor, fm_sizes_return=True) Now, after doing forward pass to the model, we need to decode loc_dets, cls_dets, fm_sizes into the boxes, labels, and confidence scores. It means that we need to define the BoxCoder for it. If we remove the first layers, we need to redefine the values of anchor_sizes , aspect_ratios , and fm_sizes . The example below is for the input image of (640x640) # Load and image and preprocess it to tensor image = torch_object_detection.utils.load_image('./images/2.jpg', height=640, width=640) tensor = torch_object_detection.utils.preprocess_image(image) # Define some argument values for BoxCoder img_size = (640, 640) anchor_sizes = [32, 64, 128, 256, 512] aspect_ratios = [[1,],[1,], [1,], [1,], [1,]] fm_sizes = [[80, 80], [40, 40], [20, 20], [10, 10], [5, 5]] box_coder = torch_object_detection.utils.BoxCoder(img_size=img_size, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios, fm_sizes=fm_sizes) The fm_sizes can be changed if we change the input image size. For example, if we get the image of (320, 480) but we don't know what is the values of fm_sizes , so there is 1 way to do it: # Load and image and preprocess it to tensor image = torch_object_detection.utils.load_image('./images/2.jpg', height=640, width=640) tensor = torch_object_detection.utils.preprocess_image(image) # Do forward pass, we get the fm_sizes if `fm_sizes_return=True` loc_dets, cls_dets, fm_sizes = net(tensor, fm_sizes_return=True) In detect method, I already implement it for you, but you must define anchor_sizes , aspect_ratio for it while fm_sizes=None . # Load and image and preprocess it to tensor image = torch_object_detection.utils.load_image('./images/2.jpg', height=480, width=320) tensor = torch_object_detection.utils.preprocess_image(image) # Define some argument values for BoxCoder img_size = (image.size[1], image.size[0]) anchor_sizes = [32, 64, 128, 256, 512] aspect_ratios = [[1,],[1,], [1,], [1,], [1,]] box_coder = torch_object_detection.utils.BoxCoder(img_size=img_size, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios, fm_sizes=None)","title":"3. Make detections with the truncated models."},{"location":"torch-truncated-model/#4-complete-exmple","text":"# Build model net = FaceNet(num_classes=2, num_anchors_per_cells=[1, 1, 1, 1, 1], # 5 base=torch_object_detection.models.Base101(pretrained=False) ) # Load truncated weights net.load_state_dict(torch.load(truncated_model_path)) # Load and image and preprocess it to tensor image = torch_object_detection.utils.load_image('./images/2.jpg', height=480, width=320) tensor = torch_object_detection.utils.preprocess_image(image) # Define some argument values for BoxCoder img_size = (image.size[1], image.size[0]) anchor_sizes = [32, 64, 128, 256, 512] aspect_ratios = [[1,],[1,], [1,], [1,], [1,]] box_coder = torch_object_detection.utils.BoxCoder(img_size=img_size, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios, fm_sizes=None) # Make detection with `detect` method det_boxes, det_scores, det_labels = torch_object_detection.utils.detect(tensor, net, box_coder=box_coder, device='cpu', score_thresh=0.3, nms_thresh=0.3) # Draw the detections drawn_image = torch_object_detection.utils.draw_detections(image, det_boxes, det_scores, det_labels) The scripts of examples can be found at examples/find_faces_with_original_model.py examples/find_faces_with_notop1_model.py examples/find_faces_with_notop12_model.py","title":"4. Complete exmple:"}]}